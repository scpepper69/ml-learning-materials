{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow_Keras_ImageClassificatioin_02_multi_label.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scpepper69/ml-learning-materials/blob/master/TensorFlow_Keras_ImageClassificatioin_03_multi_label.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYOnZhT30bGu",
        "colab_type": "text"
      },
      "source": [
        "# AI・機械学習 勉強会 #2\n",
        "## - オリジナル画像による画像分類モデルの構築 -\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRUBscIZP3DA",
        "colab_type": "text"
      },
      "source": [
        "## 目次\n",
        "\n",
        "  \n",
        "2.1.   概要\n",
        "\n",
        "2.2.   実装プロセス\n",
        "\n",
        "1.   画像データの収集\n",
        "2.   環境準備\n",
        "3.   学習に向けたデータの準備\n",
        "4.   モデル構築\n",
        "5.   モデルの学習\n",
        "6.   モデルによる予測\n",
        "7.    特徴の可視化\n",
        "8.   モデルのファイル出力\n",
        "9.   TensorBoardでの確認\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNaQV4x5qxii",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "##2.1 概要\n",
        "「TensorFlow_Keras_ImageClassification_01」をベースとして、自分で収集した画像ファイルを用いて画像分類モデルを構築します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2n0HLjd0bG3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## 2.2 実装プロセス"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVNVVU2o0bG5",
        "colab_type": "text"
      },
      "source": [
        "画像データの準備は本ノートブック上では行えませんので、各自のPCにて実施します。\n",
        "\n",
        "MNISTやCIFAR10などの公開されているデータセットではなく、自ら収集したオリジナルデータセットを用いて学習を行います。\n",
        "\n",
        "オリジナルのデータセットの作成およびそれを用いた学習の参考としてください。\n",
        "\n",
        "データ準備以降のステップについては、「TensorFlow_Keras_ImageClassification_01」と基本的には同じです。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juCkIfgekUSE",
        "colab_type": "text"
      },
      "source": [
        "###2.2.1 画像データの収集\n",
        "\n",
        "まずは、画像分類を行いたい画像を集めましょう。Webからスクレイピングするのも良し、自分で写真を撮って集めるのも良いです。\n",
        "\n",
        "Deep Learningにおいては、データの量が重要だと良く言われます。\n",
        "\n",
        "ただ、大量のデータといっても、品質が伴っていないと意味がありません。\n",
        "\n",
        "たとえば、判断不能なデータをラベリングし、学習させてしまっては、間違いを教えていることと同義になってしまいます。\n",
        "\n",
        "また、データは大量に用意できても、バリエーションに乏しければ、モデルの汎化性能は高くなりません。\n",
        "\n",
        "どのような画像が使えるのか、使えないのか、を知ることも精度の高いモデル構築に必要な知識です。\n",
        "\n",
        "かなり地味な作業となりますが、是非トライしてみてください。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDMe26ba_Bfs",
        "colab_type": "text"
      },
      "source": [
        "スクレイピングの方法はいくらでもありますが、Python使いであれば、下記が使いやすいので紹介しておきます。\n",
        "\n",
        "[Google Image Download](https://google-images-download.readthedocs.io/en/latest/index.html)\n",
        "\n",
        "こちらは[GitHub](https://github.com/hardikvasa/google-images-download)でソースも公開されています。\n",
        "\n",
        "Googleの画像検索から、指定したキーワードの結果を取得してくれます。\n",
        "\n",
        "本ノートブックで使うサンプルもこちらを用いて収集しました。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhfqR_vAk_GO",
        "colab_type": "text"
      },
      "source": [
        "本ノートブックでは、4クラス分類、64 x 64 のカラーの各クラス40枚(合計120枚)のサンプル画像をもとにソースコードを記載しています。\n",
        "\n",
        "各自準備したデータに応じて実装内容を調整してください。\n",
        "\n",
        "※サンプルデータでは、すべてのファイルを64 x 64に調整済みですが、コーディングにてreshapeする形でも問題ありません。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czcjbvDBl82U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 分類クラス数\n",
        "num_classes = 4\n",
        "\n",
        "# クラス毎の画像ファイル数\n",
        "num_images = 40\n",
        "\n",
        "# 画像のサイズ\n",
        "height, width, color = 64, 64, 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfinbX-8rfFr",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.2 環境準備\n",
        "\n",
        "モデル構造は、前回のノートブックと同様のシンプルなCNN、VGG16、RESNETv1/v2を用意しています。\n",
        "\n",
        "自力でデータ収集するとなると、それほど多くの画像ファイルは期待できないと思います。\n",
        "\n",
        "少量のデータの場合に、モデル構造によってどのような違いが出るのか比較するもの良いかと思います。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b5TKT0SrZVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# モデル構造を指定 (CNN, VGG16, RESNET1 or RESNET2)\n",
        "model_opt=\"RESNET2\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZaJgfxvyLAy",
        "colab_type": "text"
      },
      "source": [
        "Google Colabratoryは、ランタイムが初期化されるとデータも失われます。\n",
        "\n",
        "学習した中のチェックポイントが学習済みモデルを再利用できるよう、Google Driveをマウントし、ここに出力できるようしておきましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf6gYTI1xt0n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "8e7f03f6-1b43-4a58-af51-ba644d8752c9"
      },
      "source": [
        "import os, shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "gdrive_base='/content/drive/My Drive/Colab Notebooks/'\n",
        "\n",
        "# TensorBorad用ログ\n",
        "log_dir=gdrive_base+'ImageClassification/logs/'\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "# チェックポイントおよび学習済みモデルファイル\n",
        "model_dir=gdrive_base+'ImageClassification/model/'\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "    "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFKVp80t0bG8",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.3 学習に向けたデータの準備\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94oYsnmpjFNI",
        "colab_type": "text"
      },
      "source": [
        "収集した画像データをアップロードし、学習に使えるデータに変換していきます。\n",
        "\n",
        "全データを格納するための空のテンソルを準備します。\n",
        "\n",
        "テンソルは以下の５要素になります。\n",
        "\n",
        "- クラス番号(0～)\n",
        "- クラスごとのファイル番号(0～)\n",
        "- 画像のHeight\n",
        "- 画像のWidth\n",
        "- 画像のRBG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0t_rwbYO9kT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from glob import glob\n",
        "\n",
        "# 空のテンソルを用意\n",
        "ary = np.zeros([num_classes, num_images, height, width, color], dtype=np.int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zplsWRSw_xAV",
        "colab_type": "text"
      },
      "source": [
        "画像ファイルをGoogle Colabにアップロードし、１枚ずつ読み込み、テンソルに格納していきます。\n",
        "\n",
        "サンプルデータでは、ファイル名にてクラスを判別し、テンソルの１要素目(=クラス)を指定し、データを格納させています。\n",
        "\n",
        "Numpyには、テンソルデータを保存させておく機能があります。\n",
        "\n",
        "savez_compressed関数を使用し、作成したテンソルデータを再利用可能なようにファイル出力しておきましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gERS_nJSQVRR",
        "colab_type": "code",
        "outputId": "70aed6dc-b882-42c6-ee3d-56971f1346c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "# 学習データのアップロード\n",
        "# ここではサンプルデータをGitHubから取得していますが、適宜zip形式などでGoogle Colabにアップロードしてください。\n",
        "!wget -nc https://raw.githubusercontent.com/scpepper69/ml-learning-materials/master/sample/gface64x64.zip\n",
        "!unzip -oq gface64x64.zip\n",
        "\n",
        "dir_name='gface64x64'\n",
        "\n",
        "c0=0 # rx-178:mk2\n",
        "c1=0 # msz-006:Z\n",
        "c2=0 # rx-93:Nu\n",
        "c3=0 # ms-06:Zaku\n",
        "\n",
        "# 画像を順次読み込み、テンソルデータに変換\n",
        "for file in glob(dir_name + '/*.jpg'):\n",
        "    img = cv2.imread(file,cv2.IMREAD_COLOR)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    if 'rx-178' in file:\n",
        "        ary[0, c0] = img\n",
        "        c0 += 1\n",
        "    elif 'msz-006' in file:\n",
        "        ary[1, c1] = img\n",
        "        c1 += 1\n",
        "    elif 'rx-93' in file:\n",
        "        ary[2, c2] = img\n",
        "        c2 += 1\n",
        "    elif 'ms-06' in file:\n",
        "        ary[3, c3] = img\n",
        "        c3 += 1\n",
        "\n",
        "np.savez_compressed('gface_images.npz', ary)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-15 13:10:00--  https://raw.githubusercontent.com/scpepper69/ml-learning-materials/master/sample/gface64x64.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 977316 (954K) [application/zip]\n",
            "Saving to: ‘gface64x64.zip’\n",
            "\n",
            "\rgface64x64.zip        0%[                    ]       0  --.-KB/s               \rgface64x64.zip      100%[===================>] 954.41K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2019-10-15 13:10:01 (12.1 MB/s) - ‘gface64x64.zip’ saved [977316/977316]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-f4760e862d05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mc0\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m'msz-006'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mc1\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m'rx-93'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 40 is out of bounds for axis 1 with size 40"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySt5HvoHBVAn",
        "colab_type": "text"
      },
      "source": [
        "ここまでで、収集データを１つのテンソルに纏めることができました。\n",
        "\n",
        "次に、このテンソルをもとに、画像データ用テンソルと、ラベル用テンソルを作成します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGkHthXETusA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#保存したnpzファイルはnp.loadにて読み込むことができます\n",
        "#ary = np.load(\"gface_images.npz\")['arr_0']\n",
        "\n",
        "# 画像データのテンソルをソートし、ラベル用テンソルを用意\n",
        "X_train = np.zeros([num_classes * num_images, height, width, color], dtype=np.int)\n",
        "for i in range(num_classes):\n",
        "    for j in range(num_images):\n",
        "        X_train[(i * num_images) + j] = ary[i][j]\n",
        "\n",
        "# X_trainはクラス番号でソートされて格納されているので、下記だけでラベルデータが生成できる\n",
        "Y_train = np.repeat(np.arange(num_classes), num_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SnbE2e-DBZH",
        "colab_type": "text"
      },
      "source": [
        "Deep Learningには、学習データと検証データの２種類のデータが必要です。\n",
        "\n",
        "sklearnには、データを指定の割合で分割してくれる関数があります。これを利用して学習データと検証データに分割します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70olgK22Cw8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 検証データの割合を指定\n",
        "validate_rate=0.2\n",
        "\n",
        "# 学習データと検証データに分割\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=validate_rate)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2-51a7GmiIm",
        "colab_type": "text"
      },
      "source": [
        "今回はもう一つ、ラベルを追加し、1つのインプット画像から、2つアウトプット(ラベル)を出力するネットワークとしてみます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nztymPcumerE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "\n",
        "# 全体のラベルから複製して構成\n",
        "def add_label(y):\n",
        "    y2 = copy.deepcopy(y)\n",
        "    for i in range(len(y)):\n",
        "        # Gundam\n",
        "        if y2[i] in [0,1,2]:\n",
        "            y2[i] = 0\n",
        "        # Zeon\n",
        "        else:\n",
        "            y2[i] = 1\n",
        "    return y2\n",
        "\n",
        "num_classes2 = 2\n",
        "y_train2 = add_label(y_train)\n",
        "y_test2 = add_label(y_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3pSTYqA0bHI",
        "colab_type": "text"
      },
      "source": [
        "学習用画像データと画像に対応したラベルを表示してみます。\n",
        "\n",
        "画像表示には、matplotlibライブラリを用います。これはPythonにてグラフ表示によく使われるライブラリなので、使用方法は覚えておくと良いです。\n",
        "\n",
        "参考：https://matplotlib.org/api/pyplot_api.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-XaTkOc0bHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#jupyter notebook用マジックコマンド\n",
        "%matplotlib inline \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(9, 15))\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=0.5, hspace=0.05, wspace=0.05)\n",
        "\n",
        "# 各MNIST画像の上に（タイトルとして）対応するラベルを表示\n",
        "for i in range(10):\n",
        "    ax = fig.add_subplot(1, 10, i + 1, xticks=[], yticks=[])\n",
        "    ax.set_title(str(y_train[i]))\n",
        "    ax.imshow(x_train[i], cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZCxZxgl0bHX",
        "colab_type": "text"
      },
      "source": [
        "ラベルデータをone-hot表現に変換します。\n",
        "\n",
        "keras.utils.to_categorical関数を使いましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBvnPZCU0bHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# ラベルデータをone-hot表現へ変換\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# 追加ラベルも同じく変換\n",
        "y_train2 = to_categorical(y_train2, num_classes2)\n",
        "y_test2 = to_categorical(y_test2, num_classes2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfE9xtc-0Mgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 画像データの型を変換\n",
        "x_train = x_train.reshape(-1, height, width, color).astype(np.float32)\n",
        "x_test = x_test.reshape(-1, height, width, color).astype(np.float32)\n",
        "input_shape = (height, width, color)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUUkN6qj0bHi",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.4 モデル構築\n",
        "\n",
        "これから、学習モデルを構築します。\n",
        "\n",
        "モデルの構築は、モデルの「容器」としてSquential()を実施したのち、その中に順にレイヤーを追加していく流れになります。\n",
        "\n",
        "* [Sequential](https://keras.io/ja/models/sequential/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rIGYTTK0bHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# モデル構築用ライブラリをインポート\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
        "\n",
        "# CNNモデル用ライブラリ\n",
        "from tensorflow.python.keras.layers import Conv2D, Convolution2D, MaxPooling2D, Dropout, BatchNormalization, GlobalAveragePooling2D,AveragePooling2D,Input\n",
        "from tensorflow.python.keras import initializers\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# モデルの「容器」を作成\n",
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWgvceiHbKnb",
        "colab_type": "text"
      },
      "source": [
        "今回は、シンプルなCNNモデルの他、VGG16、RESNETの関数を準備してあります。\n",
        "\n",
        "- VGG16\n",
        "\n",
        "    Oxford大学の研究グループが提案し2014年のILSVRで2位を獲得したモデルで、畳み込みが13層、全結合層が3層の合計16層からなるニューラルネットワークです。\n",
        "    Kerasでは関数として組み込まれており、容易に実装することが可能になっています。\n",
        "    \n",
        "- ResNet\n",
        "\n",
        "    ResNetは2015年にMicrosoftより発表された152層からなるニューラルネットワークです。今まで20層ほどで作られていたCNNを特別なユニットを挟むことで深くすることを可能にしています。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt0Xe1505Quo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VGG16モデル\n",
        "def cnn_vgg16():\n",
        "\n",
        "    input_tensor = Input(shape=input_shape)\n",
        "    vgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=input_tensor)    \n",
        "\n",
        "    x = vgg16.output\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # original labels, 0 to 9, as output1\n",
        "    output1 = Dense(num_classes, activation='softmax', name='output1')(x)\n",
        "    # second labels, 0 or 1, as output2\n",
        "    output2 = Dense(num_classes2, activation='softmax', name='output2')(x)\n",
        "    \n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=input_tensor, outputs=[output1, output2])\n",
        "\n",
        "    #fix weights VGG16 layers\n",
        "    for layer in vgg16.layers:\n",
        "        layer.trainable = False\n",
        "        \n",
        "    return model    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLLXeRM3f7Fi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CNNモデル w/Batch Normalization\n",
        "def cnn_w_batchnorm():\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = inputs\n",
        "\n",
        "    x = Conv2D(32, (3, 3), kernel_initializer=initializers.TruncatedNormal(stddev=0.1), bias_initializer=initializers.Zeros())(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(32, (3, 3), kernel_initializer=initializers.TruncatedNormal(stddev=0.1), bias_initializer=initializers.Zeros())(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "               \n",
        "    x = Conv2D(64, (3, 3), kernel_initializer=initializers.TruncatedNormal(stddev=0.1), bias_initializer=initializers.Zeros())(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(64, (3, 3), kernel_initializer=initializers.TruncatedNormal(stddev=0.1), bias_initializer=initializers.Zeros())(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # original labels, 0 to 9, as output1\n",
        "    output1 = Dense(num_classes, activation='softmax', name='output1')(x)\n",
        "    # second labels, 0 or 1, as output2\n",
        "    output2 = Dense(num_classes2, activation='softmax', name='output2')(x)\n",
        "    \n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=[output1, output2])\n",
        "    return model    \n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGWk7WWk5Ya3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ResNetモデル from Keras Documentation\n",
        "\n",
        "# Model parameter\n",
        "# ----------------------------------------------------------------------------\n",
        "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
        "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
        "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
        "# ----------------------------------------------------------------------------\n",
        "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
        "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
        "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
        "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
        "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
        "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
        "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
        "# ---------------------------------------------------------------------------\n",
        "n = 3\n",
        "\n",
        "# Model version\n",
        "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
        "version = 1\n",
        "\n",
        "# Computed depth from supplied model parameter n\n",
        "#if version == 1:\n",
        "#    depth = n * 6 + 2\n",
        "#elif version == 2:\n",
        "#    depth = n * 9 + 2\n",
        "\n",
        "# ResNetモデル\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet_v1(input_shape, depth=20, num_classes=num_classes):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "#    y = Flatten()(x)\n",
        "#    outputs = Dense(num_classes,\n",
        "#                    activation='softmax',\n",
        "#                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "#    x = Dropout(0.5)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # original labels, 0 to 9, as output1\n",
        "    output1 = Dense(num_classes, activation='softmax', name='output1')(x)\n",
        "    # second labels, 0 or 1, as output2\n",
        "    output2 = Dense(num_classes2, activation='softmax', name='output2')(x)\n",
        "    \n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=[output1, output2])\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def resnet_v2(input_shape, depth=29, num_classes=num_classes):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "#    y = Flatten()(x)\n",
        "#    outputs = Dense(num_classes,\n",
        "#                    activation='softmax',\n",
        "#                    kernel_initializer='he_normal')(y)\n",
        "    \n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "#    x = Dropout(0.5)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # original labels, 0 to 9, as output1\n",
        "    output1 = Dense(num_classes, activation='softmax', name='output1')(x)\n",
        "    # second labels, 0 or 1, as output2\n",
        "    output2 = Dense(num_classes2, activation='softmax', name='output2')(x)\n",
        "    \n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=[output1, output2])\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DNVsEupdMjn",
        "colab_type": "text"
      },
      "source": [
        "それでは、任意のモデルを選び、構築してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq7BxPyz0bH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model構築\n",
        "if model_opt==\"VGG16\":\n",
        "    model = cnn_vgg16()\n",
        "elif model_opt==\"RESNET1\":\n",
        "    model = resnet_v1(input_shape=input_shape)\n",
        "elif model_opt==\"RESNET2\":\n",
        "    model = resnet_v2(input_shape=input_shape)\n",
        "else:\n",
        "    model=cnn_w_batchnorm()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5Dz7x4Scrf8",
        "colab_type": "text"
      },
      "source": [
        "学習の開始にあたり、最後にcompile関数でコンパイルを行います。\n",
        "\n",
        "compile関数でも、学習にあたって以下のパラメータを指定する必要があります。\n",
        "\n",
        "* optimizer（最適化手法）\n",
        "* loss（損失関数）\n",
        "* metrics（評価関数（任意））\n",
        "\n",
        "以下サンプルコードでは、現在Kerasで利用できるoptimizerを並べてみました。こちらもそれぞれどのような結果になるか比較してみるといいでしょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI4UDcU60bH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import losses\n",
        "# モデルの学習方法について指定しておく\n",
        "model.compile(loss={'output1': 'categorical_crossentropy','output2': 'categorical_crossentropy'}, optimizer=optimizers.SGD(lr=0.001, momentum=0.9), metrics=['accuracy'])\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=0.001), metrics=['accuracy'])\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=0.001), metrics=['accuracy'])\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adagrad(lr=0.001), metrics=['accuracy'])\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adadelta(lr=0.001), metrics=['accuracy'])\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adamax(lr=0.001), metrics=['accuracy'])\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=optimizers.Nadam(lr=0.001), metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZwzQE-FYxdR",
        "colab_type": "text"
      },
      "source": [
        "Optimizerアルゴリズムの動作イメージ(作者：Alec Radford)\n",
        "\n",
        "\n",
        "<img src=\"http://sebastianruder.com/content/images/2016/01/contours_evaluation_optimizers.gif\" width=\"400\">\n",
        "<img src=\"http://sebastianruder.com/content/images/2016/01/saddle_point_evaluation_optimizers.gif\" width=\"400\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LYqpAQPO04I",
        "colab_type": "text"
      },
      "source": [
        "早速学習開始と行きたいところですが、学習の途中結果を記録する(Checkpoint)ことと、TensorBoard向けのログ情報を取得できるようにしておきましょう。\n",
        "\n",
        "モデル構造や結果等の視覚化することで、何がどのように動いているのか、理解の手助けとなります。\n",
        "\n",
        "このjupyter notebook上でも可視化ロジックを組み込んでいますが、通常のモデル開発においては、TensorBoardを用いるほうがスマートでしょう。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weO_9YIVJ4dA",
        "colab_type": "text"
      },
      "source": [
        "TensorBoardでも参照できるよう、設定を組み込んでおきます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s80FRVRL0bH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TensorBoardでの可視化のため、出力先の設定\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "tb_cb = callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1,write_images=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJVjFnA8h5LT",
        "colab_type": "text"
      },
      "source": [
        "チェックポイントを生成しておくことで、中断した学習の再開や、チェックポイントを利用した静的モデルの出力を行うことができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk3ohoKJhd9P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# チェックポイント出力先\n",
        "RUN = RUN + 1 if 'RUN' in locals() else 1\n",
        "checkpoint_path = model_dir + f'run{RUN}/' + model_opt + \"_cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# チェックポイントコールバックを作る\n",
        "cp_cb = callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1, period=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMWlqNgvPBvp",
        "colab_type": "text"
      },
      "source": [
        "学習の実行の前に、モデルのサマリ情報を確認してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "135NNq6iO-b2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# モデルのサマリ情報の表示\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQ9BsS2Xh3Zv",
        "colab_type": "text"
      },
      "source": [
        "モデル構造を図示します。今回はKerasに備わっているplot_modelというツールを用います。\n",
        "\n",
        "非常に大きなモデル構造の場合、jupyter notebook上では見づらいので、ダウンロードしてローカルで確認することをお勧めします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "139AbglEh1aP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
        "from IPython.display import Image\n",
        "plot_model(model, to_file=f\"{model_dir}/model.png\", show_shapes=True)\n",
        "Image(f\"{model_dir}/model.png\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUmiS87vdonR",
        "colab_type": "text"
      },
      "source": [
        "学習途上のチェックポイントから再開させることもできます。\n",
        "\n",
        "load_weights関数にてチェックポイントファイルから重みをロードします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAQ_PWeidls-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# チェックポイントから学習済みパラメータを復元\n",
        "#model.load_weights(f'{model_dir}run1/{model_structure}_{data_set}_cp-0010.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_yBnQiN0bID",
        "colab_type": "text"
      },
      "source": [
        "### 1.2.4 モデルの学習\n",
        "\n",
        "ようやく学習に入ります。今回は少ない画像データを水増しして学習させる方法を使います。\n",
        "\n",
        "水増しとは聞こえが悪いですが、画像データに若干の加工を行い、データのバリエーションを増やすことが目的です。\n",
        "\n",
        "この場合、学習処理にはfit_generator関数を使います。\n",
        "\n",
        "Kerasに実装されているImageDataGenerator関数を用いる他、自分でgeneratorを実装することもできます。\n",
        "\n",
        "ImageDataGeneratorでは以下のようなパラメータを使用することができます。\n",
        "\n",
        "- rotation_range：画像を指定のレンジの幅で傾ける\n",
        "- zoom_range：画像の指定のレンジの幅で拡大する\n",
        "- height_shift_range：画像を指定のレンジの幅で縦にスライドする\n",
        "- width_shift_range：画像を指定のレンジの幅で横にスライドする\n",
        "\n",
        "今回は、自作のgenerator関数を用意しました。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI775x1593fK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing import image\n",
        "from random import shuffle\n",
        "from scipy import ndimage\n",
        "\n",
        "def generator(x, y1, y2, train):\n",
        "    batch_size = 16\n",
        "\n",
        "    while True:\n",
        "        if train:\n",
        "            keys = list(range(len(x)))\n",
        "            shuffle(keys)\n",
        "        else:\n",
        "            keys = list(range(len(y1)))\n",
        "            shuffle(keys)\n",
        "        inputs = []\n",
        "        label1 = []\n",
        "        label2 = []\n",
        "\n",
        "        for key in keys:\n",
        "            img = x[key]\n",
        "            if train:\n",
        "                # 画像の回転\n",
        "                rotate_rate = np.random.normal(0,0.5)*10\n",
        "                img = ndimage.rotate(x[key], rotate_rate)\n",
        "                img = cv2.resize(img,(height,width))\n",
        "                # 画像のぼかし\n",
        "                if np.random.randint(0,2):\n",
        "                    filter_rate = np.random.randint(0,6)\n",
        "                    img = ndimage.gaussian_filter(img, sigma=filter_rate)\n",
        "\n",
        "            inputs.append(img)\n",
        "            label1.append(y1[key])\n",
        "            label2.append(y2[key])\n",
        "\n",
        "            if len(inputs) == batch_size:\n",
        "                tmp_inputs = np.array(inputs)\n",
        "                tmp_label1 = np.array(label1)\n",
        "                tmp_label2 = np.array(label2)\n",
        "                inputs = []\n",
        "                label1 = []\n",
        "                label2 = []\n",
        "                yield tmp_inputs, {'output1': tmp_label1, 'output2': tmp_label2}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "_IQUEb790bIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# epoch数を指定\n",
        "epochs=10\n",
        "\n",
        "# batchサイズを指定\n",
        "batch_size=500\n",
        "\n",
        "# 学習の実行(fit)\n",
        "#result = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,callbacks=[tb_cb, cp_cb], validation_data=(x_test, y_test))\n",
        "\n",
        "    \n",
        "# 学習の実行 (fit_generator)\n",
        "result = model.fit_generator(generator(x_train, y_train, y_train2, True),\n",
        "                             steps_per_epoch=x_train.shape[0], \n",
        "                             epochs=epochs, \n",
        "                             validation_data=generator(x_test, y_test, y_test2, False), \n",
        "                             validation_steps=2, \n",
        "                             verbose=1,\n",
        "                             callbacks=[tb_cb])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCLIozIF0bIJ",
        "colab_type": "text"
      },
      "source": [
        "学習結果の評価については、evaluate関数にて得ることができます。\n",
        "\n",
        "主な引数は次の通りです。\n",
        "\n",
        "* x：評価に使用する入力データ\n",
        "* y：評価に使用する出力データ\n",
        "* batch_size：1回の評価を行うにあたって用いるサンプル数\n",
        "* verbose：評価のログを出力するか（0:しない、1：する(デフォルト)）\n",
        "\n",
        "基本的には、損失(Loss)は低ければ低いほうが、評価(Accuracy)は高ければ高いほうが良いです。\n",
        "\n",
        "Accuracyはモデルの精度そのもの、Lossは学習が効率よく行われているかを示す指標で、Lossが高いまま収束していかない＝効率が良くなく、モデル構造やパラメータ改善の余地あり、という感覚でよいかと思います。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys0GtCR60bIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#score = model.evaluate(x_test, y_test, verbose=0)\n",
        "score = model.evaluate(x_test, {'output1': y_test,'output2': y_test2}, verbose=0)\n",
        "print('Label#1 Test loss:', score[1])\n",
        "print('Label#1 Test accuracy:', score[3])\n",
        "\n",
        "print('Label#2 Test loss:', score[2])\n",
        "print('Label#2 Test accuracy:', score[4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvQIQuhhWdOo",
        "colab_type": "text"
      },
      "source": [
        "epochごとのAccuracyおよびLossの遷移をグラフ化してみましょう。\n",
        "\n",
        "fit関数のreturnから、データを取得することができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB8isRg_hfGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result.history.keys() # ヒストリデータのラベルを見てみる\n",
        "\n",
        "plt.plot(range(1, epochs+1), result.history['output1_acc'], label=\"training#1\")\n",
        "plt.plot(range(1, epochs+1), result.history['output2_acc'], label=\"training#2\")\n",
        "plt.plot(range(1, epochs+1), result.history['val_output1_acc'], label=\"validation#1\")\n",
        "plt.plot(range(1, epochs+1), result.history['val_output2_acc'], label=\"validation#2\")\n",
        "plt.title('Accuracy History')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.xlim([1,epochs])\n",
        "plt.ylim([0,1])\n",
        "plt.show()\n",
        "plt.savefig(model_opt+'_'+'acc.png')\n",
        "\n",
        "plt.plot(range(1, epochs+1), result.history['output1_loss'], label=\"training#1\")\n",
        "plt.plot(range(1, epochs+1), result.history['output2_loss'], label=\"training#2\")\n",
        "plt.plot(range(1, epochs+1), result.history['val_output1_loss'], label=\"validation#1\")\n",
        "plt.plot(range(1, epochs+1), result.history['val_output2_loss'], label=\"validation#2\")\n",
        "plt.title('Loss History')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.xlim([1,epochs])\n",
        "plt.ylim([0,10])\n",
        "plt.show()\n",
        "plt.savefig(model_opt+'_'+'loss.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq7fmArk0bIN",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.6 モデルによる予測\n",
        "\n",
        "精度だけ見ても面白くないので、検証用データの予測結果を見てみましょう。\n",
        "\n",
        "Sequential.predict関数によって予測を行うことができます。\n",
        "\n",
        "主な引数は次の通りです。\n",
        "\n",
        "* x_test：予測に使用する入力データ\n",
        "* batch_size：まとめて1度に予測を行うサンプル数\n",
        "* verbose：評価のログを出力するか（0:しない(デフォルト)、1：する）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7CJSTL50bIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "classes = model.predict(x_test, batch_size=128, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx85EWpfW7Nz",
        "colab_type": "text"
      },
      "source": [
        "上記コマンドで検証用データの予測を実施しています。\n",
        "\n",
        "どのような結果が得られているかイメージしやすくするために、画像を1枚抽出して、その結果を実際に見てみます。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sTY0plvOe-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# データセットの推論結果と元画像を確認\n",
        "# test_numを0～9999で指定してください\n",
        "%matplotlib inline\n",
        "test_num=1\n",
        "test_img = np.squeeze(x_test[test_num])\n",
        "\n",
        "#labels = np.array([0,1,2,3,4,5,6,7,8,9])\n",
        "labels1 = np.array([\n",
        "        'rx-178',\n",
        "        'msz-006',\n",
        "        'rx-93',\n",
        "        'ms-06'])\n",
        "\n",
        "labels2 = np.array([\n",
        "        'EFF',\n",
        "        'Zeon'])\n",
        "\n",
        "print(\"ラベル#1の確からしさ(%)：\"+str(np.round(classes[0][test_num],decimals=2)*100))\n",
        "print(\"推論結果：\"+str(labels1[classes[0][test_num].argmax()]))\n",
        "\n",
        "print(\"ラベル#2の確からしさ(%)：\"+str(np.round(classes[1][test_num],decimals=2)*100))\n",
        "print(\"推論結果：\"+str(labels2[classes[1][test_num].argmax()]))\n",
        "\n",
        "plt.imshow(test_img.astype(np.int),'gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ4aUVfFVYfE",
        "colab_type": "text"
      },
      "source": [
        "予測精度の最終確認として、Confusion Matrixを表示してみます。\n",
        "\n",
        "X軸とY軸が一致する箇所に集中していれば(要するに左上から右下に斜めに赤くなっている)、精度の良いモデルと言えるでしょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIOawSjjUl0y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# ラベル#1のマトリックス\n",
        "cmatrix = confusion_matrix(np.argmax(y_test, 1), np.argmax(classes[0], 1))\n",
        "cmatrix_plt = pd.DataFrame(cmatrix, index=labels1, columns=labels1)\n",
        "\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(cmatrix_plt, annot=True, cmap=\"Reds\", fmt=\"d\")\n",
        "plt.show()\n",
        "\n",
        "# ラベル#2のマトリックス\n",
        "cmatrix = confusion_matrix(np.argmax(y_test2, 1), np.argmax(classes[1], 1))\n",
        "cmatrix_plt = pd.DataFrame(cmatrix, index=labels2, columns=labels2)\n",
        "\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(cmatrix_plt, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs86zSLrtyBv",
        "colab_type": "text"
      },
      "source": [
        "###2.2.7 特徴の可視化\n",
        "\n",
        "[GradCam](http://gradcam.cloudcv.org/)を用いて、特徴部位のヒートマップによる可視化を行ってみます。\n",
        "\n",
        "具体的には、CNNモデルが画像のどの場所を見て答えを導いているのか、ヒートマップを用いて可視化します。\n",
        "\n",
        "ソースコードは[ココ](https://github.com/jacobgil/keras-grad-cam)をベースとして、今回のモデルに適合させています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITmUwk2mtzIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.python.keras.layers.core import Lambda\n",
        "from tensorflow.python.framework import ops\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "def target_category_loss(x, category_index, nb_classes):\n",
        "    return tf.multiply(x, K.one_hot([category_index], nb_classes))\n",
        "\n",
        "def target_category_loss_output_shape(input_shape):\n",
        "    return input_shape\n",
        "\n",
        "def normalize(x):\n",
        "    # utility function to normalize a tensor by its L2 norm\n",
        "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
        "\n",
        "def load_image(path):\n",
        "    #img_path = sys.argv[1]\n",
        "    img_path = path\n",
        "    img = image.load_img(img_path, target_size=(height, width))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    return x\n",
        "\n",
        "def register_gradient():\n",
        "    if \"GuidedBackProp\" not in ops._gradient_registry._registry:\n",
        "        @ops.RegisterGradient(\"GuidedBackProp\")\n",
        "        def _GuidedBackProp(op, grad):\n",
        "            dtype = op.inputs[0].dtype\n",
        "            return grad * tf.cast(grad > 0., dtype) * \\\n",
        "                tf.cast(op.inputs[0] > 0., dtype)\n",
        "\n",
        "def compile_saliency_function(model, activation_layer='block5_conv3'):\n",
        "    input_img = model.input\n",
        "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
        "    layer_output = layer_dict[activation_layer].output\n",
        "    max_output = K.max(layer_output, axis=3)\n",
        "    saliency = K.gradients(K.sum(max_output), input_img)[0]\n",
        "    return K.function([input_img, K.learning_phase()], [saliency])\n",
        "\n",
        "def modify_backprop(model, name):\n",
        "    g = tf.get_default_graph()\n",
        "    with g.gradient_override_map({'Relu': name}):\n",
        "\n",
        "        # get layers that have an activation\n",
        "        layer_dict = [layer for layer in model.layers[1:]\n",
        "                      if hasattr(layer, 'activation')]\n",
        "\n",
        "        # replace relu activation\n",
        "        for layer in layer_dict:\n",
        "            if layer.activation == keras.activations.relu:\n",
        "                layer.activation = tf.nn.relu\n",
        "\n",
        "        # re-instanciate a new model\n",
        "        new_model = VGG16(weights='imagenet')\n",
        "    return new_model\n",
        "\n",
        "def deprocess_image(x):\n",
        "    '''\n",
        "    Same normalization as in:\n",
        "    https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n",
        "    '''\n",
        "    if np.ndim(x) > 3:\n",
        "        x = np.squeeze(x)\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    if K.image_data_format() == 'th':\n",
        "        x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def grad_cam(input_model, image, category_index, layer_name):\n",
        "    nb_classes = 4\n",
        "    target_layer = lambda x: target_category_loss(x, category_index, nb_classes)\n",
        "\n",
        "    x = input_model.layers[-2].output\n",
        "    x = Lambda(target_layer, output_shape=target_category_loss_output_shape)(x)\n",
        "    model = Model(input_model.layers[0].input, x)\n",
        "\n",
        "    loss = K.sum(model.layers[-1].output)\n",
        "\n",
        "    print(layer_name)\n",
        "    for l in model.layers:\n",
        "        if l.name == layer_name:\n",
        "            print(l.name)\n",
        "            tmp_layer = l\n",
        "    \n",
        "    print(tmp_layer)\n",
        "            \n",
        "            \n",
        "    #conv_output = [l for l in model.layers[0].layers if l.name is layer_name][0].output\n",
        "#    conv_output = [l for l in model.layers if l.name is layer_name][0].output\n",
        "    conv_output = tmp_layer.output\n",
        "#    print(conv_output)\n",
        "\n",
        "    grads = normalize(K.gradients(loss, conv_output)[0])\n",
        "    gradient_function = K.function([model.layers[0].input], [conv_output, grads])\n",
        "\n",
        "    output, grads_val = gradient_function([image])\n",
        "    output, grads_val = output[0, :], grads_val[0, :, :, :]\n",
        "\n",
        "    weights = np.mean(grads_val, axis = (0, 1))\n",
        "    cam = np.ones(output.shape[0 : 2], dtype = np.float32)\n",
        "\n",
        "    for i, w in enumerate(weights):\n",
        "        cam += w * output[:, :, i]\n",
        "\n",
        "    cam = cv2.resize(cam, (height, width))\n",
        "    cam = np.maximum(cam, 0)\n",
        "    heatmap = cam / np.max(cam)\n",
        "\n",
        "    #Return to BGR [0..255] from the preprocessed image\n",
        "    image = image[0, :]\n",
        "    image -= np.min(image)\n",
        "    image = np.minimum(image, 255)\n",
        "\n",
        "    cam = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n",
        "    cam = np.float32(cam) + np.float32(image)\n",
        "    cam = 255 * cam / np.max(cam)\n",
        "    return np.uint8(cam), heatmap\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2_tP5divPMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# テスト対象イメージを準備\n",
        "preprocessed_input = np.zeros([1, height, width, color], dtype=np.int)\n",
        "preprocessed_input[0] = test_img\n",
        "\n",
        "predictions = model.predict(preprocessed_input)[0]\n",
        "predicted_class = np.argmax(predictions)\n",
        "\n",
        "# 畳み込み最終層の指定\n",
        "if model_opt==\"VGG16\":\n",
        "    cnn_out_layer = \"block5_conv3\"\n",
        "elif model_opt==\"RESNET1\":\n",
        "    cnn_out_layer = \"add_8\"\n",
        "elif model_opt==\"RESNET2\":\n",
        "    cnn_out_layer = \"add_8\"\n",
        "else:\n",
        "    cnn_out_layer = \"conv2d_3\"\n",
        "\n",
        "# Grad Cam\n",
        "cam, heatmap = grad_cam(model, preprocessed_input, predicted_class, cnn_out_layer)\n",
        "plt.imshow(cam.astype(np.int),'gray')\n",
        "plt.show()\n",
        "\n",
        "# Guided Grad Cam\n",
        "register_gradient()\n",
        "guided_model = modify_backprop(model, 'GuidedBackProp')\n",
        "saliency_fn = compile_saliency_function(guided_model)\n",
        "saliency = saliency_fn([preprocessed_input, 0])\n",
        "gradcam = saliency[0] * heatmap[..., np.newaxis]\n",
        "plt.imshow(deprocess_image(gradcam).astype(np.int),'gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqbPWg1mfbA3",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.8 モデルのファイル出力\n",
        "\n",
        "学習させたモデルを出力し、静的学習済みモデルとして外部で活用することもできます。\n",
        "\n",
        "ここでは、Keras形式に加えて、TensorFlowのSaved Model形式も試してみます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tcWm1vmlVvA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Keras形式でモデルを出力\n",
        "output_keras_name = f\"{model_dir}02_{model_opt}_{epochs}_frozen_graph.h5\"\n",
        "model.save(output_keras_name, include_optimizer=False)\n",
        "\n",
        "# TensorFlow Saved Model形式でモデルを出力\n",
        "from tensorflow.contrib import saved_model\n",
        "\n",
        "out_tf_saved_model = f\"{model_dir}02_{model_opt}_{epochs}_saved_models\"\n",
        "\n",
        "if os.path.exists(out_tf_saved_model):\n",
        "    shutil.rmtree(out_tf_saved_model)\n",
        "#saved_model_path = saved_model.save_keras_model(model, out_tf_saved_model)\n",
        "saved_model_path = saved_model.save_keras_model(model, \"./saved_model\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU493_lgMncX",
        "colab_type": "text"
      },
      "source": [
        "###2.2.9 TensorBoardでの確認\n",
        "\n",
        "TensorBoardを用いてモデル構造を確認してみましょう。\n",
        "\n",
        "Google Colabでは、直接HTTPアクセスはできないので、ngrokを用いて参照させます。\n",
        "\n",
        "下記コード実行後し、出力されるURLにアクセスしてみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cehjp0h3Mwot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Web参照のため、ngrokを利用\n",
        "if not os.path.exists('./ngrok'):\n",
        "    !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "    !unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "# TensorBoardおよびngrokの起動\n",
        "import subprocess\n",
        "cmd = f'tensorboard --logdir=\"{log_dir}\" --host 0.0.0.0 --port 6006 &'\n",
        "proc_tb = subprocess.call(cmd, shell=True)\n",
        "\n",
        "cmd = \"./ngrok http 6006 &\"\n",
        "proc_ng = subprocess.call(cmd, shell=True)\n",
        "\n",
        "# TensorBoard URL\n",
        "!curl -s http://localhost:4040/api/tunnels | python -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nwohZfUM6Mx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TensorBoardプロセスの停止\n",
        "#!ps -ef | grep tensorboard | grep -v grep | awk '{print \"kill -9\",$2}'| sh\n",
        "\n",
        "# ngrokプロセスの停止\n",
        "#!ps -ef | grep ngrok | grep -v grep | awk '{print \"kill -9\",$2}'| sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMRzOAUDWSob",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}