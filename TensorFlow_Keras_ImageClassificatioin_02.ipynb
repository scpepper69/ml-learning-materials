{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow_Keras_ImageClassificatioin_02.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scpepper69/ml-learning-materials/blob/master/TensorFlow_Keras_ImageClassificatioin_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYOnZhT30bGu",
        "colab_type": "text"
      },
      "source": [
        "# AI・機械学習 勉強会 #2\n",
        "## - オリジナル画像による画像分類モデルの構築 -\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRUBscIZP3DA",
        "colab_type": "text"
      },
      "source": [
        "## 目次\n",
        "\n",
        "  \n",
        "2.1.   概要\n",
        "\n",
        "2.2.   実装プロセス\n",
        "\n",
        "1.   画像データの収集\n",
        "2.   環境準備\n",
        "3.   学習に向けたデータの準備\n",
        "4.   モデル構築\n",
        "5.   モデルの学習\n",
        "6.   モデルによる予測\n",
        "7.   モデルのファイル出力\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNaQV4x5qxii",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "##2.1 概要\n",
        "「TensorFlow_Keras_ImageClassification_01」をベースとして、自分で収集した画像ファイルを用いて画像分類モデルを構築します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2n0HLjd0bG3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## 2.2 実装プロセス"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVNVVU2o0bG5",
        "colab_type": "text"
      },
      "source": [
        "画像データの準備は本ノートブック上では行えませんので、各自のPCにて実施します。\n",
        "\n",
        "各自のPCにて集めた画像(jpg形式を想定)データを用いて、学習を行います。\n",
        "\n",
        "データ準備以降のステップについては、「TensorFlow_Keras_ImageClassification_01」と基本的には同じです。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juCkIfgekUSE",
        "colab_type": "text"
      },
      "source": [
        "###2.2.1 画像データの収集\n",
        "\n",
        "まずは、画像分類を行いたい画像を集めましょう。Webからスクレイピングするのも良し、自分で写真を撮って集めるのも良いです。\n",
        "\n",
        "Deep Learningにおいては、データの量が重要だと良く言われます。\n",
        "\n",
        "ただ、大量のデータといっても、品質が伴っていないと意味がありません。\n",
        "\n",
        "たとえば、判断不能なデータをラベリングし、学習させてしまっては、間違いを教えていることと同義になってしまいます。\n",
        "\n",
        "また、データは大量に用意できても、バリエーションに乏しければ、モデルの汎化性能は高くなりません。\n",
        "\n",
        "どのような画像が使えるのか、使えないのか、を知ることも精度の高いモデル構築に必要な知識です。\n",
        "\n",
        "かなり地味な作業となりますが、是非トライしてみてください。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDMe26ba_Bfs",
        "colab_type": "text"
      },
      "source": [
        "スクレイピングの方法はいくらでもありますが、Python使いであれば、下記が使いやすいので紹介しておきます。\n",
        "\n",
        "[Google Image Download](https://google-images-download.readthedocs.io/en/latest/index.html)\n",
        "\n",
        "こちらは[GitHub](https://github.com/hardikvasa/google-images-download)でソースも公開されています。\n",
        "\n",
        "Googleの画像検索から、指定したキーワードの結果を取得してくれます。\n",
        "\n",
        "本ノートブックで使うサンプルもこちらを用いて収集しました。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhfqR_vAk_GO",
        "colab_type": "text"
      },
      "source": [
        "本ノートブックでは、３クラス分類、64 x 64 のカラーの各クラス40枚(合計120枚)のサンプル画像をもとにソースコードを記載しています。\n",
        "\n",
        "各自準備したデータに応じて実装内容を調整してください。\n",
        "\n",
        "※サンプルデータでは、すべてのファイルを64 x 64に調整済みですが、コーディングにてreshapeする形でも問題ありません。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czcjbvDBl82U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 分類クラス数\n",
        "num_classes = 3\n",
        "\n",
        "# クラス毎の画像ファイル数\n",
        "num_images = 40\n",
        "\n",
        "# 画像のサイズ\n",
        "height, width, color = 64, 64, 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfinbX-8rfFr",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.2 環境準備\n",
        "\n",
        "モデル構造は、前回のノートブックと同様のシンプルなCNN、VGG16、RESNETv1/v2を用意しています。\n",
        "\n",
        "自力でデータ収集するとなると、それほど多くの画像ファイルは期待できないと思います。\n",
        "\n",
        "少量のデータの場合に、モデル構造によってどのような違いが出るのか比較するもの良いかと思います。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b5TKT0SrZVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# モデル構造を指定 (CNN, VGG16, RESNET1 or RESNET2)\n",
        "model_opt=\"RESNET2\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZaJgfxvyLAy",
        "colab_type": "text"
      },
      "source": [
        "Google Colabratoryは、ランタイムが初期化されるとデータも失われます。\n",
        "\n",
        "学習した中のチェックポイントが学習済みモデルを再利用できるよう、Google Driveをマウントし、ここに出力できるようしておきましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf6gYTI1xt0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "gdrive_base='/content/drive/My Drive/Colab Notebooks/'\n",
        "\n",
        "# TensorBorad用ログ\n",
        "log_dir=gdrive_base+'ImageClassification/logs/'\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "# チェックポイントおよび学習済みモデルファイル\n",
        "model_dir=gdrive_base+'ImageClassification/model/'\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFKVp80t0bG8",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.3 学習に向けたデータの準備\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94oYsnmpjFNI",
        "colab_type": "text"
      },
      "source": [
        "収集した画像データをアップロードし、学習に使えるデータに変換していきます。\n",
        "\n",
        "全データを格納するための空のテンソルを準備します。\n",
        "\n",
        "テンソルは以下の５要素になります。\n",
        "\n",
        "- クラス番号(0～)\n",
        "- クラスごとのファイル番号(0～)\n",
        "- 画像のHeight\n",
        "- 画像のWidth\n",
        "- 画像のRBG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0t_rwbYO9kT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from glob import glob\n",
        "\n",
        "# 空のテンソルを用意\n",
        "ary = np.zeros([num_classes, num_images, height, width, color], dtype=np.int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zplsWRSw_xAV",
        "colab_type": "text"
      },
      "source": [
        "画像ファイルをGoogle Colabにアップロードし、１枚ずつ読み込み、テンソルに格納していきます。\n",
        "\n",
        "サンプルデータでは、ファイル名にてクラスを判別し、テンソルの１要素目(=クラス)を指定し、データを格納させています。\n",
        "\n",
        "Numpyには、テンソルデータを保存させておく機能があります。\n",
        "\n",
        "savez_compressed関数を使用し、作成したテンソルデータを再利用可能なようにファイル出力しておきましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gERS_nJSQVRR",
        "colab_type": "code",
        "outputId": "aaef55f1-e758-423f-d0ca-f5b9015552b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# 学習データのアップロード\n",
        "# ここではサンプルデータをGitHubから取得していますが、適宜zip形式などでGoogle Colabにアップロードしてください。\n",
        "!wget -nc https://raw.githubusercontent.com/scpepper69/ml-learning-materials/master/sample/gface_images.zip\n",
        "!unzip -oq gface_images.zip\n",
        "\n",
        "dir_name='gface_images'\n",
        "\n",
        "c0=0 # rx-178:mk2\n",
        "c1=0 # msz-006:Z\n",
        "c2=0 # rx-93:Nu\n",
        "\n",
        "# 画像を順次読み込み、テンソルデータに変換\n",
        "for file in glob(dir_name + '/*.jpg'):\n",
        "    img = cv2.imread(file,cv2.IMREAD_COLOR)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    if 'rx-178' in file:\n",
        "        ary[0, c0] = img\n",
        "        c0 += 1\n",
        "    elif 'msz-006' in file:\n",
        "        ary[1, c1] = img\n",
        "        c1 += 1\n",
        "    elif 'rx-93' in file:\n",
        "        ary[2, c2] = img\n",
        "        c2 += 1\n",
        "\n",
        "np.savez_compressed('gface_images.npz', ary)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-12 06:58:08--  https://raw.githubusercontent.com/scpepper69/ml-learning-materials/master/sample/gface_images.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 432372 (422K) [application/zip]\n",
            "Saving to: ‘gface_images.zip’\n",
            "\n",
            "\rgface_images.zip      0%[                    ]       0  --.-KB/s               \rgface_images.zip    100%[===================>] 422.24K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2019-07-12 06:58:08 (8.26 MB/s) - ‘gface_images.zip’ saved [432372/432372]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySt5HvoHBVAn",
        "colab_type": "text"
      },
      "source": [
        "ここまでで、収集データを１つのテンソルに纏めることができました。\n",
        "\n",
        "次に、このテンソルをもとに、画像データ用テンソルと、ラベル用テンソルを作成します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGkHthXETusA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#保存したnpzファイルはnp.loadにて読み込むことができます\n",
        "#ary = np.load(\"gface_images.npz\")['arr_0']\n",
        "\n",
        "# 画像データのテンソルをソートし、ラベル用テンソルを用意\n",
        "X_train = np.zeros([num_classes * num_images, height, width, color], dtype=np.int)\n",
        "for i in range(num_classes):\n",
        "    for j in range(num_images):\n",
        "        X_train[(i * num_images) + j] = ary[i][j]\n",
        "\n",
        "# X_trainはクラス番号でソートされて格納されているので、下記だけでラベルデータが生成できる\n",
        "Y_train = np.repeat(np.arange(num_classes), num_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SnbE2e-DBZH",
        "colab_type": "text"
      },
      "source": [
        "Deep Learningには、学習データと検証データの２種類のデータが必要です。\n",
        "\n",
        "sklearnには、データを指定の割合で分割してくれる関数があります。これを利用して学習データと検証データに分割します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70olgK22Cw8g",
        "colab_type": "code",
        "outputId": "a636d6fd-596e-4505-cb30-5562b6f65f0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 検証データの割合を指定\n",
        "validate_rate=0.2\n",
        "\n",
        "# 学習データと検証データに分割\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=validate_rate)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(96, 64, 64, 3)\n",
            "(24, 64, 64, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3pSTYqA0bHI",
        "colab_type": "text"
      },
      "source": [
        "学習用画像データと画像に対応したラベルを表示してみます。\n",
        "\n",
        "画像表示には、matplotlibライブラリを用います。これはPythonにてグラフ表示によく使われるライブラリなので、使用方法は覚えておくと良いです。\n",
        "\n",
        "参考：https://matplotlib.org/api/pyplot_api.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-XaTkOc0bHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#jupyter notebook用マジックコマンド\n",
        "%matplotlib inline \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(9, 15))\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=0.5, hspace=0.05, wspace=0.05)\n",
        "\n",
        "# 各MNIST画像の上に（タイトルとして）対応するラベルを表示\n",
        "for i in range(10):\n",
        "    ax = fig.add_subplot(1, 10, i + 1, xticks=[], yticks=[])\n",
        "    ax.set_title(str(y_train[i]))\n",
        "    ax.imshow(x_train[i], cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZCxZxgl0bHX",
        "colab_type": "text"
      },
      "source": [
        "ラベルデータをone-hot表現に変換します。\n",
        "\n",
        "keras.utils.to_categorical関数を使いましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBvnPZCU0bHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# ラベルデータをone-hot表現へ変換\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfE9xtc-0Mgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 画像データの型を変換\n",
        "x_train = x_train.reshape(-1, height, width, color).astype(np.float32)\n",
        "x_test = x_test.reshape(-1, height, width, color).astype(np.float32)\n",
        "input_shape = (height, width, color)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUUkN6qj0bHi",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.4 モデル構築\n",
        "\n",
        "これから、学習モデルを構築します。\n",
        "\n",
        "モデルの構築は、モデルの「容器」としてSquential()を実施したのち、その中に順にレイヤーを追加していく流れになります。\n",
        "\n",
        "* [Sequential](https://keras.io/ja/models/sequential/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rIGYTTK0bHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# モデル構築用ライブラリをインポート\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
        "\n",
        "# CNNモデル用ライブラリ\n",
        "from tensorflow.python.keras.layers import Conv2D, Convolution2D, MaxPooling2D, Dropout, BatchNormalization, GlobalAveragePooling2D,AveragePooling2D,Input\n",
        "from tensorflow.python.keras import initializers\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# モデルの「容器」を作成\n",
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWgvceiHbKnb",
        "colab_type": "text"
      },
      "source": [
        "今回は、シンプルなCNNモデルの他、VGG16、RESNETの関数を準備してあります。\n",
        "\n",
        "- VGG16\n",
        "\n",
        "    Oxford大学の研究グループが提案し2014年のILSVRで2位を獲得したモデルで、畳み込みが13層、全結合層が3層の合計16層からなるニューラルネットワークです。\n",
        "    Kerasでは関数として組み込まれており、容易に実装することが可能になっています。\n",
        "    \n",
        "- ResNet\n",
        "\n",
        "    ResNetは2015年にMicrosoftより発表された152層からなるニューラルネットワークです。今まで20層ほどで作られていたCNNを特別なユニットを挟むことで深くすることを可能にしています。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt0Xe1505Quo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VGG16モデル\n",
        "def cnn_vgg16():\n",
        "    input_tensor = Input(shape=input_shape)\n",
        "    vgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=input_tensor)    \n",
        "    \n",
        "    add_model = Sequential()\n",
        "    add_model.add(Flatten())\n",
        "    add_model.add(Dense(256, kernel_initializer=initializers.TruncatedNormal(stddev=0.1), bias_initializer=initializers.Zeros()))\n",
        "    add_model.add(Activation('relu'))\n",
        "    add_model.add(BatchNormalization())\n",
        "    add_model.add(Dense(units=num_classes))\n",
        "    add_model.add(Activation('softmax'))\n",
        "    \n",
        "    vgg16_model = Model(inputs=vgg16.input, outputs=add_model(vgg16.output))\n",
        "\n",
        "    #fix weights VGG16 layers\n",
        "    for layer in vgg16.layers:\n",
        "        layer.trainable = False\n",
        "        \n",
        "    return vgg16_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLLXeRM3f7Fi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CNNモデル\n",
        "def cnn_model():\n",
        "    model.add(Conv2D(32, (3, 3), kernel_initializer=initializers.TruncatedNormal(stddev=0.1), bias_initializer=initializers.Zeros(), input_shape=input_shape))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(64, (3, 3), kernel_initializer=initializers.TruncatedNormal(stddev=0.1), bias_initializer=initializers.Zeros()))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, kernel_initializer=initializers.TruncatedNormal(stddev=0.1), bias_initializer=initializers.Zeros()))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(units=num_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    return model\n",
        "\n",
        "# CNNモデル w/Dropout\n",
        "def cnn_w_dropout():\n",
        "    model.add(Conv2D(32, (3, 3), kernel_initializer=initializers.TruncatedNormal(stddev=0.1), bias_initializer=initializers.Zeros(), input_shape=input_shape))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(64, (3, 3), kernel_initializer=initializers.TruncatedNormal(stddev=0.1), bias_initializer=initializers.Zeros()))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, kernel_initializer=initializers.TruncatedNormal(stddev=0.1), bias_initializer=initializers.Zeros()))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(units=num_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    return model\n",
        "    \n",
        "# CNNモデル w/Batch Normalization\n",
        "def cnn_w_batchnorm():\n",
        "    model.add(Conv2D(32, (3, 3), kernel_initializer=initializers.TruncatedNormal(stddev=0.1), bias_initializer=initializers.Zeros(), input_shape=input_shape))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(32, (3, 3), kernel_initializer=initializers.TruncatedNormal(stddev=0.1), bias_initializer=initializers.Zeros()))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), kernel_initializer=initializers.TruncatedNormal(stddev=0.1), bias_initializer=initializers.Zeros(), input_shape=input_shape))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(64, (3, 3), kernel_initializer=initializers.TruncatedNormal(stddev=0.1), bias_initializer=initializers.Zeros()))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(256, kernel_initializer=initializers.TruncatedNormal(stddev=0.1), bias_initializer=initializers.Zeros()))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dense(units=num_classes))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    return model\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGWk7WWk5Ya3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ResNetモデル from Keras Documentation\n",
        "\n",
        "# Model parameter\n",
        "# ----------------------------------------------------------------------------\n",
        "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
        "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
        "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
        "# ----------------------------------------------------------------------------\n",
        "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
        "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
        "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
        "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
        "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
        "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
        "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
        "# ---------------------------------------------------------------------------\n",
        "n = 3\n",
        "\n",
        "# Model version\n",
        "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
        "version = 1\n",
        "\n",
        "# Computed depth from supplied model parameter n\n",
        "#if version == 1:\n",
        "#    depth = n * 6 + 2\n",
        "#elif version == 2:\n",
        "#    depth = n * 9 + 2\n",
        "\n",
        "# ResNetモデル\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet_v1(input_shape, depth=20, num_classes=num_classes):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "def resnet_v2(input_shape, depth=29, num_classes=num_classes):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "    y = Flatten()(x)\n",
        "    outputs = Dense(num_classes,\n",
        "                    activation='softmax',\n",
        "                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DNVsEupdMjn",
        "colab_type": "text"
      },
      "source": [
        "それでは、任意のモデルを選び、構築してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq7BxPyz0bH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model構築\n",
        "if model_opt==\"VGG16\":\n",
        "    model = cnn_vgg16()\n",
        "elif model_opt==\"RESNET1\":\n",
        "    model = resnet_v1(input_shape=input_shape)\n",
        "elif model_opt==\"RESNET2\":\n",
        "    model = resnet_v2(input_shape=input_shape)\n",
        "else:\n",
        "#    model=cnn_model()\n",
        "#    model=cnn_w_dropout()\n",
        "    model=cnn_w_batchnorm()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5Dz7x4Scrf8",
        "colab_type": "text"
      },
      "source": [
        "学習の開始にあたり、最後にcompile関数でコンパイルを行います。\n",
        "\n",
        "compile関数でも、学習にあたって以下のパラメータを指定する必要があります。\n",
        "\n",
        "* optimizer（最適化手法）\n",
        "* loss（損失関数）\n",
        "* metrics（評価関数（任意））\n",
        "\n",
        "以下サンプルコードでは、現在Kerasで利用できるoptimizerを並べてみました。こちらもそれぞれどのような結果になるか比較してみるといいでしょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI4UDcU60bH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import losses\n",
        "# モデルの学習方法について指定しておく\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.001, momentum=0.9), metrics=['accuracy'])\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=0.001), metrics=['accuracy'])\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=0.001), metrics=['accuracy'])\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adagrad(lr=0.001), metrics=['accuracy'])\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adadelta(lr=0.001), metrics=['accuracy'])\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adamax(lr=0.001), metrics=['accuracy'])\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=optimizers.Nadam(lr=0.001), metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZwzQE-FYxdR",
        "colab_type": "text"
      },
      "source": [
        "Optimizerアルゴリズムの動作イメージ(作者：Alec Radford)\n",
        "\n",
        "\n",
        "<img src=\"http://sebastianruder.com/content/images/2016/01/contours_evaluation_optimizers.gif\" width=\"400\">\n",
        "<img src=\"http://sebastianruder.com/content/images/2016/01/saddle_point_evaluation_optimizers.gif\" width=\"400\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LYqpAQPO04I",
        "colab_type": "text"
      },
      "source": [
        "早速学習開始と行きたいところですが、学習の途中結果を記録する(Checkpoint)ことと、TensorBoard向けのログ情報を取得できるようにしておきましょう。\n",
        "\n",
        "モデル構造や結果等の視覚化することで、何がどのように動いているのか、理解の手助けとなります。\n",
        "\n",
        "このjupyter notebook上でも可視化ロジックを組み込んでいますが、通常のモデル開発においては、TensorBoardを用いるほうがスマートでしょう。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weO_9YIVJ4dA",
        "colab_type": "text"
      },
      "source": [
        "TensorBoardでも参照できるよう、設定を組み込んでおきます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s80FRVRL0bH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TensorBoardでの可視化のため、出力先の設定\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "tb_cb = callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1,write_images=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJVjFnA8h5LT",
        "colab_type": "text"
      },
      "source": [
        "チェックポイントを生成しておくことで、中断した学習の再開や、チェックポイントを利用した静的モデルの出力を行うことができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk3ohoKJhd9P",
        "colab_type": "code",
        "outputId": "e7c6764b-794c-4d2f-c67c-055add378b67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# チェックポイント出力先\n",
        "RUN = RUN + 1 if 'RUN' in locals() else 1\n",
        "checkpoint_path = model_dir + f'run{RUN}/' + model_opt + \"_cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# チェックポイントコールバックを作る\n",
        "cp_cb = callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1, period=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0712 06:58:18.731832 140019297666944 callbacks.py:875] `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMWlqNgvPBvp",
        "colab_type": "text"
      },
      "source": [
        "学習の実行の前に、モデルのサマリ情報を確認してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "135NNq6iO-b2",
        "colab_type": "code",
        "outputId": "b5ee0b06-9a5f-41bb-9253-3e47ef98cc14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# モデルのサマリ情報の表示\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 64, 64, 16)   448         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 64, 64, 16)   64          conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 64, 64, 16)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 64, 64, 16)   272         activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 64, 64, 16)   64          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 64, 64, 16)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 64, 64, 16)   2320        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 64, 64, 16)   64          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 64, 64, 16)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 64, 64, 64)   1088        activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 64, 64, 64)   1088        activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 64, 64, 64)   0           conv2d_4[0][0]                   \n",
            "                                                                 conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 64, 64, 64)   256         add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 64, 64, 64)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 64, 64, 16)   1040        activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 64, 64, 16)   64          conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 64, 64, 16)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 64, 64, 16)   2320        activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 64, 64, 16)   64          conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 64, 64, 16)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 64, 64, 64)   1088        activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 64, 64, 64)   0           add[0][0]                        \n",
            "                                                                 conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 64, 64, 64)   256         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 64, 64, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 64, 64, 16)   1040        activation_6[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 64, 64, 16)   64          conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 64, 64, 16)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 64, 64, 16)   2320        activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 64, 64, 16)   64          conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 64, 64, 16)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 64, 64, 64)   1088        activation_8[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 64, 64, 64)   0           add_1[0][0]                      \n",
            "                                                                 conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 64, 64, 64)   256         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 64, 64, 64)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 32, 32, 64)   4160        activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 32, 32, 64)   256         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 32, 32, 64)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 32, 32, 64)   36928       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 32, 32, 64)   256         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 32, 32, 64)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 32, 32, 128)  8320        add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 32, 32, 128)  8320        activation_11[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 32, 32, 128)  0           conv2d_14[0][0]                  \n",
            "                                                                 conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 32, 32, 128)  512         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 32, 32, 128)  0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 32, 32, 64)   8256        activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 32, 32, 64)   256         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 32, 32, 64)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 32, 32, 64)   36928       activation_13[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 32, 32, 64)   256         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 32, 32, 64)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 32, 32, 128)  8320        activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 32, 32, 128)  0           add_3[0][0]                      \n",
            "                                                                 conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 32, 32, 128)  512         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 32, 32, 128)  0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 32, 32, 64)   8256        activation_15[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 32, 32, 64)   256         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 32, 32, 64)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 32, 32, 64)   36928       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 32, 32, 64)   256         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 32, 32, 64)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 32, 32, 128)  8320        activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 32, 32, 128)  0           add_4[0][0]                      \n",
            "                                                                 conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 32, 32, 128)  512         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 32, 32, 128)  0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 16, 16, 128)  16512       activation_18[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 16, 16, 128)  512         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 16, 16, 128)  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 16, 16, 128)  147584      activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 16, 16, 128)  512         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 16, 16, 128)  0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 16, 16, 256)  33024       add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 16, 16, 256)  33024       activation_20[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 16, 16, 256)  0           conv2d_24[0][0]                  \n",
            "                                                                 conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 16, 16, 256)  1024        add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 16, 16, 256)  0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 16, 16, 128)  32896       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 16, 16, 128)  512         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 16, 16, 128)  0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 16, 16, 128)  147584      activation_22[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 16, 16, 128)  512         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 16, 16, 128)  0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 16, 16, 256)  33024       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 16, 16, 256)  0           add_6[0][0]                      \n",
            "                                                                 conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 16, 16, 256)  1024        add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 16, 16, 256)  0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 16, 16, 128)  32896       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 16, 16, 128)  512         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 16, 16, 128)  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 16, 16, 128)  147584      activation_25[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 16, 16, 128)  512         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 16, 16, 128)  0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 16, 16, 256)  33024       activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 16, 16, 256)  0           add_7[0][0]                      \n",
            "                                                                 conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 16, 16, 256)  1024        add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 16, 16, 256)  0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d (AveragePooli (None, 2, 2, 256)    0           activation_27[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 1024)         0           average_pooling2d[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 3)            3075        flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 849,507\n",
            "Trainable params: 844,291\n",
            "Non-trainable params: 5,216\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUmiS87vdonR",
        "colab_type": "text"
      },
      "source": [
        "学習途上のチェックポイントから再開させることもできます。\n",
        "\n",
        "load_weights関数にてチェックポイントファイルから重みをロードします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAQ_PWeidls-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# チェックポイントから学習済みパラメータを復元\n",
        "#model.load_weights(f'{model_dir}run1/{model_structure}_{data_set}_cp-0010.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_yBnQiN0bID",
        "colab_type": "text"
      },
      "source": [
        "### 1.2.4 モデルの学習\n",
        "\n",
        "ようやく学習に入ります。今回は少ない画像データを水増しして学習させる方法を使います。\n",
        "\n",
        "この場合、学習処理にはfit_generator関数を使います。\n",
        "\n",
        "水増しというと聞こえが悪いですが、、、ImageDataGenerator関数を用いることで、画像データに若干の加工を行い、データのバリエーションを増やすことができます。\n",
        "\n",
        "サンプルでは、以下４点の設定を入れています。\n",
        "\n",
        "- rotation_range：画像を指定のレンジの幅で傾ける\n",
        "- zoom_range：画像の指定のレンジの幅で拡大する\n",
        "- height_shift_range：画像を指定のレンジの幅で縦にスライドする\n",
        "- width_shift_range：画像を指定のレンジの幅で横にスライドする\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "_IQUEb790bIE",
        "colab_type": "code",
        "outputId": "284b9b70-369f-4a9c-f918-ecccf75190e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "from keras.preprocessing import image\n",
        "\n",
        "# epoch数を指定\n",
        "epochs=20\n",
        "\n",
        "# batchサイズを指定\n",
        "batch_size=500\n",
        "\n",
        "# 学習の実行(fit)\n",
        "#result = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,callbacks=[tb_cb, cp_cb], validation_data=(x_test, y_test))\n",
        "\n",
        "# 画像データ生成器を作成する。\n",
        "params = {\n",
        "    'rotation_range': 20,\n",
        "    'zoom_range': 0.10,\n",
        "    'height_shift_range': 0.1,\n",
        "    'width_shift_range': 0.1\n",
        "}\n",
        "datagen = image.ImageDataGenerator(**params)\n",
        "datagen.fit(x_train)\n",
        "\n",
        "# 学習の実行 (fit_generator)\n",
        "result = model.fit_generator(datagen.flow(x_train, y_train, batch_size=16), steps_per_epoch=x_train.shape[0], epochs=epochs, validation_data=(x_test, y_test))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "96/96 [==============================] - 19s 200ms/step - loss: 1.4964 - acc: 0.6732 - val_loss: 2.7477 - val_acc: 0.2917\n",
            "Epoch 2/20\n",
            "96/96 [==============================] - 10s 107ms/step - loss: 1.1674 - acc: 0.8646 - val_loss: 1.2646 - val_acc: 0.6667\n",
            "Epoch 3/20\n",
            "96/96 [==============================] - 10s 107ms/step - loss: 1.0071 - acc: 0.8952 - val_loss: 1.0428 - val_acc: 0.8750\n",
            "Epoch 4/20\n",
            "96/96 [==============================] - 10s 107ms/step - loss: 0.9218 - acc: 0.9134 - val_loss: 0.9405 - val_acc: 0.9167\n",
            "Epoch 5/20\n",
            "96/96 [==============================] - 10s 106ms/step - loss: 0.8614 - acc: 0.9336 - val_loss: 0.8917 - val_acc: 0.9167\n",
            "Epoch 6/20\n",
            "96/96 [==============================] - 10s 107ms/step - loss: 0.8401 - acc: 0.9395 - val_loss: 0.8695 - val_acc: 0.9167\n",
            "Epoch 7/20\n",
            "96/96 [==============================] - 10s 106ms/step - loss: 0.7959 - acc: 0.9557 - val_loss: 0.8462 - val_acc: 0.9167\n",
            "Epoch 8/20\n",
            "96/96 [==============================] - 10s 107ms/step - loss: 0.7726 - acc: 0.9609 - val_loss: 0.8283 - val_acc: 0.9167\n",
            "Epoch 9/20\n",
            "96/96 [==============================] - 10s 107ms/step - loss: 0.7511 - acc: 0.9635 - val_loss: 0.8180 - val_acc: 0.8750\n",
            "Epoch 10/20\n",
            "96/96 [==============================] - 10s 107ms/step - loss: 0.7383 - acc: 0.9714 - val_loss: 0.7855 - val_acc: 0.9167\n",
            "Epoch 11/20\n",
            "96/96 [==============================] - 10s 107ms/step - loss: 0.7218 - acc: 0.9759 - val_loss: 0.7766 - val_acc: 0.9167\n",
            "Epoch 12/20\n",
            "96/96 [==============================] - 10s 107ms/step - loss: 0.7213 - acc: 0.9707 - val_loss: 0.7729 - val_acc: 0.9167\n",
            "Epoch 13/20\n",
            "96/96 [==============================] - 10s 107ms/step - loss: 0.6942 - acc: 0.9837 - val_loss: 0.7657 - val_acc: 0.9167\n",
            "Epoch 14/20\n",
            "96/96 [==============================] - 10s 107ms/step - loss: 0.6841 - acc: 0.9896 - val_loss: 0.7646 - val_acc: 0.9167\n",
            "Epoch 15/20\n",
            "96/96 [==============================] - 10s 107ms/step - loss: 0.6883 - acc: 0.9844 - val_loss: 0.7832 - val_acc: 0.9167\n",
            "Epoch 16/20\n",
            "96/96 [==============================] - 10s 108ms/step - loss: 0.6729 - acc: 0.9896 - val_loss: 0.7686 - val_acc: 0.9167\n",
            "Epoch 17/20\n",
            "96/96 [==============================] - 10s 108ms/step - loss: 0.6684 - acc: 0.9928 - val_loss: 0.7572 - val_acc: 0.9167\n",
            "Epoch 18/20\n",
            "96/96 [==============================] - 10s 108ms/step - loss: 0.6610 - acc: 0.9922 - val_loss: 0.7532 - val_acc: 0.9167\n",
            "Epoch 19/20\n",
            "96/96 [==============================] - 10s 108ms/step - loss: 0.6563 - acc: 0.9941 - val_loss: 0.7460 - val_acc: 0.9167\n",
            "Epoch 20/20\n",
            "96/96 [==============================] - 10s 107ms/step - loss: 0.6543 - acc: 0.9941 - val_loss: 0.7667 - val_acc: 0.9583\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCLIozIF0bIJ",
        "colab_type": "text"
      },
      "source": [
        "学習結果の評価については、evaluate関数にて得ることができます。\n",
        "\n",
        "主な引数は次の通りです。\n",
        "\n",
        "* x：評価に使用する入力データ\n",
        "* y：評価に使用する出力データ\n",
        "* batch_size：1回の評価を行うにあたって用いるサンプル数\n",
        "* verbose：評価のログを出力するか（0:しない、1：する(デフォルト)）\n",
        "\n",
        "基本的には、損失(Loss)は低ければ低いほうが、評価(Accuracy)は高ければ高いほうが良いです。\n",
        "\n",
        "Accuracyはモデルの精度そのもの、Lossは学習が効率よく行われているかを示す指標で、Lossが高いまま収束していかない＝効率が良くなく、モデル構造やパラメータ改善の余地あり、という感覚でよいかと思います。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys0GtCR60bIK",
        "colab_type": "code",
        "outputId": "ec458f24-6c62-4f66-b73c-1087ee455b0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.7852859497070312\n",
            "Test accuracy: 0.9583333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvQIQuhhWdOo",
        "colab_type": "text"
      },
      "source": [
        "epochごとのAccuracyおよびLossの遷移をグラフ化してみましょう。\n",
        "\n",
        "fit関数のreturnから、データを取得することができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB8isRg_hfGT",
        "colab_type": "code",
        "outputId": "b68054c9-6759-4f51-fc2b-dd10fc5a6e17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 644
        }
      },
      "source": [
        "result.history.keys() # ヒストリデータのラベルを見てみる\n",
        "print(epochs)\n",
        "print(result.history['acc'])\n",
        "plt.plot(range(1, epochs+1), result.history['acc'], label=\"training\")\n",
        "plt.plot(range(1, epochs+1), result.history['val_acc'], label=\"validation\")\n",
        "plt.title('Accuracy History')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.xlim([1,epochs])\n",
        "plt.ylim([0,1])\n",
        "plt.show()\n",
        "plt.savefig(model_opt+'_'+'acc.png')\n",
        "\n",
        "plt.plot(range(1, epochs+1), result.history['loss'], label=\"training\")\n",
        "plt.plot(range(1, epochs+1), result.history['val_loss'], label=\"validation\")\n",
        "plt.title('Loss History')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.xlim([1,epochs])\n",
        "plt.ylim([0,20])\n",
        "plt.show()\n",
        "plt.savefig(model_opt+'_'+'loss.png')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n",
            "[0.67317706, 0.8645833, 0.8951823, 0.91341144, 0.93359375, 0.9394531, 0.9557292, 0.9609375, 0.9635417, 0.9713542, 0.97591144, 0.9707031, 0.98372394, 0.9895833, 0.984375, 0.9895833, 0.99283856, 0.9921875, 0.9941406, 0.9941406]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEWCAYAAACaBstRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl4HNWZ7/Hvq1ZrsyV5kXcZbMBg\nGzDexuwJEJKAh0BCWANhmWS44YEEMpnJMDe5JDDJ3GRgMglZLyQwhkAIEBbnXghbHJZgM5aMMV5Y\nDHiR5EW2ZUuytpb03j+qZLeFJLelbrVa+n2ep5/uqjp96lW53W+fc6pOmbsjIiKSiKx0ByAiIplD\nSUNERBKmpCEiIglT0hARkYQpaYiISMKUNEREJGFKGiIDkJnVm9kR6Y5DpDMlDRlQzOwvZlZjZrnp\njiVVzMzN7KhO675rZr/tWHb34e7+wUHqOcPMKlIVp0hXlDRkwDCzKcDpgAPn9/O+s/tzfwPBUPyb\npe+UNGQguQpYBvwXcHX8BjPLN7P/MLONZrbHzF41s/xw22lm9pqZ7TazzWZ2Tbj+L2b25bg6rjGz\nV+OW3cxuMLP3gPfCdT8J66g1s3IzOz2ufMTM/qeZvW9mdeH2yWb2czP7j07xLjazr/f2QMS3Rsxs\noZmtDfdZaWb/aGbDgGeAiWFXVr2ZTTSzXDP7sZlVhY8fd7TaOlomZvbPZrYVuM/MVpvZZ+L2GzWz\nHWY2p7exy+CmpCEDyVXAg+Hj02Y2Lm7bncA84BRgFPBNoN3MDif48vwpMAaYDaw8hH1+FjgRmBku\nLw/rGAU8BDxqZnnhtn8ALgcWAkXA3wENwCLgcjPLAjCzEuDs8P3J8Bvgf7h7IXAc8Gd33wucC1SF\nXVnD3b0K+BZwUvg3nAAsAL4dV9f48G87HLgOuB+4Mm77QmCLu7+RpNhlsHF3PfRI+wM4DYgBJeHy\n28DXw9dZQCNwQhfv+xfgiW7q/Avw5bjla4BX45YdOOsgcdV07Bd4B7igm3LrgE+Gr28Enu6hTgdq\ngd1xjybgt53KHBW+3gT8D6CoUz1nABWd1r0PLIxb/jSwIa58C5AXt30iUNdRN/AY8M10fx70GLgP\ntTRkoLgaeM7dd4TLD7G/i6oEyCP4QuxscjfrE7U5fiHs+lkXdoHtBorD/R9sX4vY/4v9SuCBg+x3\nrruP6HgAP+ih7OcJWgAbzewlMzu5h7ITgY1xyxvDdR2q3b2pY8GD1slfgc+b2QiC1suDB4ldhjAN\nhEnahWMTlwCRsK8dIBcYYWYnAG8R/BI/Eniz09s3E3TBdGUvUBC3PL6LMvumeQ7HL74JfAJY4+7t\nZlYDWNy+jgRWd1HPb4HVYbwzgCe7iemQufty4AIzixK0Yh4hSGBdTVFdRdD1tCZcPixct6+6Lt6z\nCPgywffBUnevTFLoMgippSEDwWeBNoJxhdnhYwbwCnCVu7cD9wI/Cgd7I2Z2cjjA+yBwtpldYmbZ\nZjbazGaH9a4ELjSzgnBQ+UsHiaMQaAWqgWwzu5Vg7KLDr4F/NbNpFphlZqMB3L2CYDzkAeAP7t7Y\n14MCYGY5ZnaFmRW7e4ygW6s93LwNGG1mxXFv+R3wbTMbE46t3EqQ0HryJDAXuIlgjEOkW0oaMhBc\nDdzn7pvcfWvHA/gZcEV4aug/ErQ4lgO7gB8CWe6+iaDr5hvh+pUEA8AA/0nQh7+N4Nf0wbpdngX+\nBLxL0K3TxIHdVz8i+JX/HMGX92+A/Ljti4DjOXjX1KH6IrDBzGqBrwBXALj72wRJ4oPwzLGJwPeA\nMmAVwfFaEa7rVpjg/gBMBR5PcuwyyJi7bsIkkgxm9jGCX/WHe4b9xwpbVUe7+5UHLSxDmsY0RJIg\nHG+4Cfh1BiaMUQRdd19Mdywy8KWse8rM7jWz7WbW1aAhYZ/wXWa23sxWmdncVMUikkpmNoPgtNkJ\nwI/THM4hMbO/J+iCe8bdX053PDLwpax7Kmyq1wP3u/txXWxfCHyVoD/6ROAn7n5iSoIREZGkSFlL\nI/zVsquHIhcQJBR392UEp1dOSFU8IiLSd+kc05jEgWemVITrtnQuaGbXEUx5wLBhw+ZNnz69XwIU\nEelv7uA47R687vrSmkO3auUbO9x9TF/ryYiBcHe/G7gbYP78+V5WVpbmiESkta2dPY0xYm1OUX42\n+dEIZnbwN6ZAW7tT2xijrqmVxlgbDS3Bc1OsjcaWdhpaWoPXHcuxVppawuVYO41h+caWYLmtvZ0s\nM8yMLIOs8PnAZcM6XmfxkfIGtLY7sbZ2Ym3ttLS209K2fzkWLre0thEL17e2p/AcipXnbTx4oYNL\nZ9KoJLiqtUNpuE5E+lFbu1PXFKOmIcbuhhZ2N8TY3dhCzd4Yuxvj1+1/XdPQQl1T6wH1RCNGUV6U\novzwkZdNcfi6OD9KUV74nJ/daTko29LWTm1jK3saY9Q2xdjTED43xg5c3xijtjF4rmsK1tc3t3bz\n13UtJzuL/GiEgpwI+dEIedEI+TkRhuVmM3p4hOwso907fu0HzwcuO+3twbq2difW5l2Wz45kkRMx\nopEsCnKyiUayyM3OIhqui2ZnkRPJIid+XSR+XRaRLDD6nowv/WGfqwDSmzQWAzea2cMEA+F73P0j\nXVMi0ne1TTHWb69n/fZ63g+fP9yxl10NLexpjNHd+TBmUJQXZWRBlBEFOYwalsORY4ZTnB9lZEEO\nIwqiRCNZH/kyrw2/zCtrGtkTruvrr+hhOZH9SSY/SunIgn1JqCMBFeZlU5CTTUHO/kTQOTHkRyNE\nstLTIkqnS5NUT8qShpn9jmBWzRIL7i72HSAK4O6/Ap4mOHNqPcH00temKhaRdGpvd+pbWqlvaiXW\n1k5hXvDlFo0k9zwUd6e6vvmAxLC+up73ttWzva55X7mcSBZTS4ZxzPhCxhTmMiI/SAgjCoJEUBw+\njwi/nJPxBevuNMbaemxJ5EazwpZK1y2R7CQfL+mdlCUNd7/8INsduCFV+xdJBnenrrmVHXXN+7pB\n6ptaqWtqpa65lbqmGPVNrdQ3d7+uu66TYTmR8Atxf7dNUacvy+Kuunnyo9TsbdnXctifHOqojesy\nGpYT4aixwzltWgnTxhZy1NjhHDV2OJNH5vf7F7CZhS2AbMYX5x38DTJgZcRAuEiytbU7O/c2U13X\nzPa6Zqprm6mub2Z7bVOwHK7fXtdEU6y9x7oKciIMz82mMC+b4XnBl/z4orxwXZThedkU5WUzPDdo\nXdQ17e++2d+dE6NydxPrttQFA7qH0Ec/elgOR44dznknTGRamBiOGjuc8UV5aRuYlsFLSUMygrvT\n0tZOU6yd5tY2mmPtNLe20xRro7k1XNfaTnPHcliuKdZObVOM7bVBAggSQzM797bQ1kUfe1FeNmMK\ncxlbmMecw0YwZnguY4tyGVOYG/aZR/cliMLcKMNyIyn51d4xON3dAHBhXpRp44Zz1JjhjByWk/T9\ni3RHSUNSrqcvwJ7Ojqlrao1LCj3/2u9JlkFJx5f/8FyOnVC8LxGMLcxlTGFe+JxLXjSSxL+89yJZ\nFo4zKCFIEmxcmrSqlDSkz9ydyt2NrK7cw6qKPaypqmV7XXOQFBLoaolk2Uf67ieOyKcwN5u8aITc\naBa52RHywufc7OC0xbxo+Dra/bq86NA9W0aEynL48/fh/ReTVqWShhwSd6dqTxNvVewJkkRl8Lxr\nbwsA2VnGtHGFTBqRz8wJRQecCXPAwG7B/gHfgpz0XRQmMihtfQuW/Bu88zTkj4JP3g633ZyUqpU0\npFvuztbaJlZ1JIjweWeYICJZxtHjCjl7xliOLx3B8ZOKmT6+cMB08YgMOdXvBMli7ZOQWwxnfhtO\n+grkFgJKGpIkTbE2asIrfTfvamB15R7eCh876vcniGljh3PW9LEcX1rM8ZOKmTGhSAlCZCDY+T68\n9EN461GIFsDH/glOvgHyRyZ9V0oag0hLa3swzUNjjJq9LZ2mfYixZ9/UEOG0EOF0EJ0HmbMMpo0t\n5ONHj2VWaTHHTSpm5oQi8nOUIEQGlN2b4OU74I0HIZIDJ98Ip94Mw0anbJdKGhkq1tbO6so9LPtg\nF8s+2Mkbm2oOuLCrs2gkPBsnnP5h8qgCZpXuvxJ4RH4OIwuijC3KY8aEQgpy9NEQGbBqt8Ar/wHl\n/xXM9bLg7+G0f4DCcSnftb4ZMkSsrZ23Kvew7IOdLPtgF+UbdrG3pQ2Ao8YO529nTWRicR4jhu1P\nDCMKovumhtBgs8ggUF8Nf/0xLP81tLfCnCuDrqji0n4LQUljgIq1tbOqoiNJ7KR8Yw0NYZKYNnY4\nF84t5aQjRrNg6ijGFOamOVoRSamGXbD0Z7DsV9DaCLMug49/E0ZN7fdQlDTSbdeHUPMhsTbnwx31\nrNtSx9tba3l3az3NbcFYQ+mIPM6eVsSMCUUcM76Q4vw2YHvwCJ8GpZzhMGk+ZKV5ojp3qFoBTXvS\nG4cMTZuXBwmjuRaO+zx8/BYYc3TawlHSSJO9W99j73Pfp+SDp8iinShwdPi4ACASPiCYA/j98DHU\njD8+OG3w6E8Hfbf97cOX4c/fg82v9/++RTpMPw/O/J8w7th0R6Kk0R/cnU27GijfWMP769dx3Pp7\nOLv5BSJk8eu2c3lnxOnMnDiCYycWhRfERdMd8sCwcz28fCf87tKgxXHWt+CIM/sneWxaFiSLDa9A\n4URYeGeQwET6W0EJlByV7ij2Me/u7isDVCbc7rUp1saqij2s2FRD+cYa3thUg9Vv44bsp/hC5M9k\nGbw14UKaTryJmcccQ7GSRPfaYrDyIXjp36G2Ag4/Fc78Fkw5NTX7q1wBS74P61+AYWPg9G/AvGsh\nqum8JbOZWbm7z+9zPUoafVe1u5HyjTWs2FTDio01rKmq3XeXstmjW7kp72lOr3mCiLfis68g6+P/\nBCMOS3PUGaa1GcoXwSt3Qv22oMVx1rehtM//BwJbV8Nf/je8/X+DC6JOvTk4jTFnWHLqF0kzJY00\naoq18eyarTy3dhsrNtawZU8TAHnRLE4oHcHcw0dy4vgsFmx9iILyuyHWALMuDc52GH1kWmPPeC0N\nUPYbePU/oWEnHH1O0Nc74YTe1Vf9bpAs1jwOuUXBxVEnXQ95RcmNWyTNlDT6mbuzcvNuHiuvYPGb\nVdQ1tTKuKJcFU0cz77AgUcyYUEQ0Vg+v/wpe+xk074FjPwdn/AuMOabfYx7Umuvg9f8Dr90VnNU0\n4/wgeYydkdj7d30YTLuw6veQnR/Mz3PyjVAwKrVxi6SJkkY/2V7bxONvVPJYeQXrt9eTF83i3OMm\ncPG84DqJrI4pt1v2wn/fE1x401gDxywMvsQ0eJpajbth2S9g6S+gpR6OvxjOuKX7Ft2einDahd9C\nVjb8zZfhtK/DsJL+jVuknylppFBzaxsvrtvOY+UVvPRuNW3tzrzDR3LxvFIWzppAUV7cwHWsCcrv\ng1d+BHu3w5GfCAZqS+elNEbppGEX/PUnQeujrQVmXw4f+yaMPDzYXrc1+Dcqvy+47mL+tcG0C0UT\n0hu3SD9R0kgyd2dNVS2PlVfw5MpKdjfEGFeUy4VzS7loXilHjhl+4Bva22DFInjpDqirgsNPCwZm\nDz856bHJIajbFox3lN0L3g5zr4KcAvjvXwfJZM4VwbQLOhFBhphkJY0hf53GzvpmnlxZxaNlm3l7\nax05kSw+eew4Lp5XyunTxnR/x7elP4Pnb4XSBfC5X8LUj6fn4jM5UOE4OPcHcMpXgzOtViwKErxO\nRBBJiiHZ0oi1tbPk7aD76c9vb6e13ZlVWszF80r5zAkTD35f5vZ2+OlcKJoI1/w/JYuBrLYquNaj\no5tKZIhSS6OX6ptbOe+uV9iws4GS4Tlce+oULpo3mWPGFyZeycZXoebD4KwoJYyBrWhiuiMQGVSG\nXNJ4fEUFG3Y2cMdFs/jsnElEI72YDG/FA8GtFGeen/wARUQGsDRPH9q/3J1Fr21gVmkxF80r7V3C\naKyBtU/BrIshmp/8IEVEBrAhlTT+un4n71fv5eqTp/T+hkSrHoW25uCsHBGRIWZIJY1FSzcwalgO\nfzurl+fmu8OK+2H8rN5PWyEiksGGTNLYvKuBF9dt47K/mUxeNHLwN3Rly0rY9pZaGSIyZA2ZpPHb\n1zcCcOVJfTj1csUDkJ0XTFUhIjIEDYmk0RRr4/fLN/OpmeOZOKKXg9ctDfDWozDzAsgfkdwARUQy\nxJBIGovfrGJ3Q4yrT5nS+0rWPhXco1ddUyIyhA36pNFxmu3R44Zz0hF9mPb6jQdg1BHBneNERIao\nQZ80VmwK7qR3VV9Os92xHjb+FeZ8UVeAi8iQNuiTxqLXNlKYl83n5kzqfSVvPAAWgdlfSF5gIiIZ\nKKVJw8zOMbN3zGy9md3SxfbDzGyJmb1hZqvMbGEy97+9tomn39rCxfMmMyy3lzOmtMVg5UNw9Keh\ncHwywxMRyTgpSxpmFgF+DpwLzAQuN7OZnYp9G3jE3ecAlwG/SGYMD/33JlrbnatO7sNptu89F9xc\nac4XkxeYiEiGSmVLYwGw3t0/cPcW4GHggk5lHCgKXxcDVcnaeUtrOw++vokzjhnDlJJhva9oxf0w\nfDxM+1SyQhMRyVipTBqTgM1xyxXhunjfBa40swrgaeCrXVVkZteZWZmZlVVXVye08z+t2Up1XTNX\nnzzlUOPer7YqaGnM/gJEhtyEwCIiH5HugfDLgf9y91JgIfCAmX0kJne/293nu/v8MWPGJFTx/a9t\n4PDRBXz86MTKd2nlQ8EtQ+dc2fs6REQGkVQmjUpgctxyabgu3peARwDcfSmQB5T0dcerK/dQtrGG\nL550OFnd3a71YNrbg7OmppyuW4SKiIRSmTSWA9PMbKqZ5RAMdC/uVGYT8AkAM5tBkDQS63/qwf1L\nN5AfjXDx/MkHLdutja9CzQYNgIuIxElZ0nD3VuBG4FlgHcFZUmvM7HYz67jl3TeAvzezN4HfAdd4\nH29aXrO3hadWVvG5uZMozo/2vqIV9+vufCIinaR0dNfdnyYY4I5fd2vc67VAUufl+H3ZZppb2/t2\nmm1jDaxdHMwzpbvziYjsk+6B8KRqa3ceWLqRk44YxfTxRQd/Q3f23Z1PXVMiIvEGVdJ4cd02Knc3\n9u0024678004QXfnExHpZFAljfuXbmRCcR6fnDmu95V03J1PA+AiIh8xaJLG+u11vLp+B1eedDjZ\nkT78WSvu1935RES6MWiSxv1LN5ITyeLSv+nDabYtDfDWYzDzs7o7n4hIFwZF0qhrivGH8grOO2EC\nJcNze1/RvrvzqWtKRKQrgyJpPL6ikr0tbX0bAIega0p35xMR6VbGJ432dmfR0g2cMHkEJ0zuQ5fS\njvWw6TXdnU9EpAcZnzT++v4OPqjeyzWn9OFiPoA37tfd+UREDiLjk8ai1zZQMjyHhcdP6H0lbTFY\n+TvdnU9E5CAyOmls3tXAi29v5/IFh5GbHel9Re8+G9ydb+5VyQtORGQQyuik8dtlG8ky4wsnHta3\nijruznfUJ5MTmIjIIJWxSaOxpY2Hl2/m08eOY0JxHyYVrK2C9c/r7nwiIgnI2KSx+M1K9jTG+n6a\n7coHdXc+EZEEZWTScHcWvbaR6eMLWTB1VO8ram+HN36ru/OJiCQoI5NG2cYa1m6p5epTpmB9uaZi\nwyvB3fk0AC4ikpCMTBqLXttAUV42F8ye2LeKVtwPecUw4zPJCUxEZJDLuKQRa3P+tHorl8yfTEFO\nHwauG3bBuj/C8Zfo7nwiIgnKuKSxa28zbe58sS+3cwV4q+PufOqaEhFJVAYmjRbOPGYsh48e1vtK\nDrg736zkBSciMshlXNJobXeuPmVK3yqpegO2rVYrQ0TkEGVc0sjJzuL0o0r6VknH3fmOuyg5QYmI\nDBEZlzQOH1VAVlYfTrNtbYbVf9Dd+UREeiHjkkZetA8TEwJsfSu4O9/0hckJSERkCMm4pNFnFcuD\n59K/SW8cIiIZaAgmjTIonAhFfbwwUERkCBp6SaOyDErnpzsKEZGMNLSSxt4dwVxTShoiIr0ytJJG\nRVnwPElJQ0SkN4ZW0qgsA4vAxNnpjkREJCMNraRRsRzGzYScPkxBIiIyhA2dpNHeDpUr1DUlItIH\nQydp7HwvuKhP12eIiPTa0EkaHYPgOnNKRKTXUpo0zOwcM3vHzNab2S3dlLnEzNaa2RozeyhlwVQs\nh9xiGD0tZbsQERns+nDru56ZWQT4OfBJoAJYbmaL3X1tXJlpwL8Ap7p7jZmNTVU8VJbBpLmQNXQa\nVyIiyZbKb9AFwHp3/8DdW4CHgQs6lfl74OfuXgPg7ttTEknLXti2Vl1TIiJ9lMqkMQnYHLdcEa6L\ndzRwtJn91cyWmdk5XVVkZteZWZmZlVVXVx96JFUrwdt05pSISB+lu68mG5gGnAFcDtxjZh+5yYW7\n3+3u8919/pgxYw59L5UaBBcRSYaDJg0z+6qZjexF3ZXA5Ljl0nBdvApgsbvH3P1D4F2CJJJcFWUw\ncgoM6+Md/0REhrhEWhrjCAaxHwnPhkr0tnnLgWlmNtXMcoDLgMWdyjxJ0MrAzEoIuqs+SLD+xFWU\nqWtKRCQJDpo03P3bBL/+fwNcA7xnZv9mZkce5H2twI3As8A64BF3X2Nmt5vZ+WGxZ4GdZrYWWAL8\nk7vv7PVf05XaKqir0kV9IiJJkNApt+7uZrYV2Aq0AiOBx8zseXf/Zg/vexp4utO6W+PrBf4hfKSG\nLuoTEUmagyYNM7sJuArYAfyaoDUQM7Ms4D2g26QxIFQsh0gOjD8+3ZGIiGS8RFoao4AL3X1j/Ep3\nbzez81ITVhJVlsP4WZCdm+5IREQyXiID4c8AuzoWzKzIzE4EcPd1qQosKdpaoeoNdU2JiCRJIknj\nl0B93HJ9uG7g274WYg06c0pEJEkSSRoWDlgDQbcUKZyzKql0UZ+ISFIlkjQ+MLOvmVk0fNxEKq6l\nSIWKcigYHVzYJyIifZZI0vgKcArB1dwVwInAdakMKmkqlgddUwlfjygiIj05aDdTOPPsZf0QS3I1\n7YEd78LxF6c7EhGRQSOR6zTygC8BxwJ5Hevd/e9SGFffVa4AHErnpTsSEZFBI5HuqQeA8cCngZcI\nJh6sS2VQSdFxJfjEuemNQ0RkEEkkaRzl7v8L2Ovui4C/JRjXGNgqy6DkGMj/yEzrIiLSS4kkjVj4\nvNvMjgOKgdTdljUZ3IOWhk61FRFJqkSut7g7vJ/GtwmmNh8O/K+URtVXNRugYQdM0niGiEgy9Zg0\nwkkJa8N7eL8MHNEvUfVVZXnwrOnQRUSSqsfuqfDq74E9i21XKsogWgBjZ6Y7EhGRQSWRMY0XzOwf\nzWyymY3qeKQ8sr6oWA4TZkMkM2Y7ERHJFIl8q14aPt8Qt84ZqF1Vrc2wdRWc+JV0RyIiMugkckX4\n1P4IJGm2roa2Fp05JSKSAolcEX5VV+vd/f7kh5MEFcuDZ02HLiKSdIl0T8WfgpQHfAJYAQzMpFFZ\nBoUToXhSuiMRERl0Eume+mr8spmNAB5OWUR9VVGm+aZERFIkkbOnOtsLDMxxjr07oOZDdU2JiKRI\nImMafyQ4WwqCJDMTeCSVQfWaLuoTEUmpRMY07ox73QpsdPeKFMXTNxVlYBGYODvdkYiIDEqJJI1N\nwBZ3bwIws3wzm+LuG1IaWW9ULA+uAs8Zlu5IREQGpUTGNB4F2uOW28J1A0t7e3DjJV2fISKSMokk\njWx3b+lYCF/npC6kXtq5Hpr3KGmIiKRQIkmj2szO71gwswuAHakLqZd0UZ+ISMolMqbxFeBBM/tZ\nuFwBdHmVeFpVlkFuEZQcne5IREQGrUQu7nsfOMnMhofL9SmPqjcqymDSXMjqzaUnIiKSiIN+w5rZ\nv5nZCHevd/d6MxtpZt/rj+AS1tIA29aoa0pEJMUS+Vl+rrvv7lgI7+K3MHUh9cKWleBtuqhPRCTF\nEkkaETPL7Vgws3wgt4fy/a+iLHjWmVMiIimVyED4g8CLZnYfYMA1wKJUBnXIKpbDiMNhWEm6IxER\nGdQSGQj/oZm9CZxNMAfVs8DhqQ7skFSWw2EnpzsKEZFBL9FTjbYRJIyLgbOAdYm8yczOMbN3zGy9\nmd3SQ7nPm5mb2aH3L9VWQW2luqZERPpBty0NMzsauDx87AB+D5i7n5lIxWYWAX4OfJLg2o7lZrbY\n3dd2KlcI3AS83qu/oGM8Q2dOiYikXE8tjbcJWhXnuftp7v5TgnmnErUAWO/uH4RTjzwMXNBFuX8F\nfgg0HULd+1WWQVYUxh/fq7eLiEjiekoaFwJbgCVmdo+ZfYJgIDxRk4DNccsV4bp9zGwuMNnd/19P\nFZnZdWZWZmZl1dXVB26sKIcJsyCadwihiYhIb3SbNNz9SXe/DJgOLAFuBsaa2S/N7FN93bGZZQE/\nAr5xsLLufre7z3f3+WPGjNm/oa0Vqlaoa0pEpJ8cdCDc3fe6+0Pu/hmgFHgD+OcE6q4EJsctl4br\nOhQCxwF/MbMNwEnA4kMaDK9eB7EGDYKLiPSTQ5qoyd1rwl/9n0ig+HJgmplNNbMc4DJgcVxde9y9\nxN2nuPsUYBlwvruXJRyQLuoTEelXKZvdz91bgRsJrutYBzzi7mvM7Pb4qdb7pKIMCkbDyKlJqU5E\nRHqWyBXhvebuTwNPd1p3azdlzzjkHVSWwaR5YIcyPi8iIr2VufOIN+2B6nc0SaGISD/K3KRRuQLw\noKUhIiL9IoOTRseV4EoaIiL9JXOTRkV5cGvX/BHpjkREZMjIzKThHg6C61RbEZH+lJlJY/dG2FsN\npeqaEhHpT5mZNPZd1Kczp0RE+lNmJo3KcsjOh7HHpjsSEZEhJTOTRsVymDgbIim9NlFERDrJvKTh\nDltWab4pEZE0yLyk0doIbc06c0pEJA0yL2m07A2e1dIQEel3GZg0GqBwAhRNOnhZERFJqsxLGrG9\nmtlWRCRNMi9ptDara0pEJE0yL2mALuoTEUmTDEwaBhNmpzsIEZEhKfOSRjQPcoenOwoRkSEp85KG\n7gcuIpI2mZc0snPTHYGIyJCVeUlDRETSRklDREQSpqQhIiIJU9IQEZGEKWmIiEjClDRERCRhShoi\nIpIwJQ0REUmYkoaIiCRMSUMFA4C+AAAPdElEQVRERBKmpCEiIglT0hARkYQpaYiISMKUNEREJGEp\nTRpmdo6ZvWNm683sli62/4OZrTWzVWb2opkdnsp4RESkb1KWNMwsAvwcOBeYCVxuZjM7FXsDmO/u\ns4DHgH9PVTwiItJ3qWxpLADWu/sH7t4CPAxcEF/A3Ze4e0O4uAwoTWE8IiLSR6lMGpOAzXHLFeG6\n7nwJeKarDWZ2nZmVmVlZdXV1EkMUEZFDMSAGws3sSmA+cEdX2939bnef7+7zx4wZ07/BiYjIPtkp\nrLsSmBy3XBquO4CZnQ18C/i4uzenMB4REemjVLY0lgPTzGyqmeUAlwGL4wuY2Rzg/wDnu/v2FMYi\nIiJJkLKk4e6twI3As8A64BF3X2Nmt5vZ+WGxO4DhwKNmttLMFndTnYiIDACp7J7C3Z8Gnu607ta4\n12encv8iIpJcKU0a/SUWi1FRUUFTU1O6QxkU8vLyKC0tJRqNpjsUERlgBkXSqKiooLCwkClTpmBm\n6Q4no7k7O3fupKKigqlTp6Y7HBEZYAbEKbd91dTUxOjRo5UwksDMGD16tFptItKlQZE0ACWMJNKx\nFJHuDJqkISIiqaekkQS7d+/mF7/4xSG/b+HChezevbvHMrfeeisvvPBCb0MTEUkqJY0k6C5ptLa2\n9vi+p59+mhEjRvRY5vbbb+fss3VmsogMDIPi7Kl4t/1xDWurapNa58yJRXznM8d2u/2WW27h/fff\nZ/bs2USjUfLy8hg5ciRvv/027777Lp/97GfZvHkzTU1N3HTTTVx33XUATJkyhbKyMurr6zn33HM5\n7bTTeO2115g0aRJPPfUU+fn5XHPNNZx33nlcdNFFTJkyhauvvpo//vGPxGIxHn30UaZPn051dTVf\n+MIXqKqq4uSTT+b555+nvLyckpKSpB4HERG1NJLgBz/4AUceeSQrV67kjjvuYMWKFfzkJz/h3Xff\nBeDee++lvLycsrIy7rrrLnbu3PmROt577z1uuOEG1qxZw4gRI/jDH/7Q5b5KSkpYsWIF119/PXfe\neScAt912G2eddRZr1qzhoosuYtOmTan7Y0VkSBt0LY2eWgT9ZcGCBQdc43DXXXfxxBNPALB582be\ne+89Ro8efcB7pk6dyuzZswGYN28eGzZs6LLuCy+8cF+Zxx9/HIBXX311X/3nnHMOI0eOTOrfIyLS\nYdAljYFg2LBh+17/5S9/4YUXXmDp0qUUFBRwxhlndHkNRG5u7r7XkUiExsbGLuvuKBeJRA46ZiIi\nkmzqnkqCwsJC6urquty2Z88eRo4cSUFBAW+//TbLli1L+v5PPfVUHnnkEQCee+45ampqkr4PERFQ\nSyMpRo8ezamnnspxxx1Hfn4+48aN27ftnHPO4Ve/+hUzZszgmGOO4aSTTkr6/r/zne9w+eWX88AD\nD3DyySczfvx4CgsLk74fERFz93THcEjmz5/vZWVlB6xbt24dM2bMSFNE6dfc3EwkEiE7O5ulS5dy\n/fXXs3Llyj7VOdSPqchgY2bl7j6/r/WopTEIbNq0iUsuuYT29nZycnK455570h2SiAxSShqDwLRp\n03jjjTfSHYaIDAEaCBcRkYQpaYiISMKUNEREJGFKGiIikjAljTQYPnw4AFVVVVx00UVdljnjjDPo\nfGpxZz/+8Y9paGjYt5zIVOsiIn2hpJFGEydO5LHHHuv1+zsnjUSmWhcR6YvBd8rtM7fA1reSW+f4\n4+HcH3S7+ZZbbmHy5MnccMMNAHz3u98lOzubJUuWUFNTQywW43vf+x4XXHDBAe/bsGED5513HqtX\nr6axsZFrr72WN998k+nTpx8w99T111/P8uXLaWxs5KKLLuK2227jrrvuoqqqijPPPJOSkhKWLFmy\nb6r1kpISfvSjH3HvvfcC8OUvf5mbb76ZDRs2dDsFu4hIItTSSIJLL71039xPAI888ghXX301Tzzx\nBCtWrGDJkiV84xvfoKer73/5y19SUFDAunXruO222ygvL9+37fvf/z5lZWWsWrWKl156iVWrVvG1\nr32NiRMnsmTJEpYsWXJAXeXl5dx33328/vrrLFu2jHvuuWffdRyJTsEuItKVwdfS6KFFkCpz5sxh\n+/btVFVVUV1dzciRIxk/fjxf//rXefnll8nKyqKyspJt27Yxfvz4Lut4+eWX+drXvgbArFmzmDVr\n1r5tjzzyCHfffTetra1s2bKFtWvXHrC9s1dffZXPfe5z+2bbvfDCC3nllVc4//zzE56CXUSkK4Mv\naaTJxRdfzGOPPcbWrVu59NJLefDBB6murqa8vJxoNMqUKVO6nBL9YD788EPuvPNOli9fzsiRI7nm\nmmt6VU+HRKdgFxHpirqnkuTSSy/l4Ycf5rHHHuPiiy9mz549jB07lmg0ypIlS9i4cWOP7//Yxz7G\nQw89BMDq1atZtWoVALW1tQwbNozi4mK2bdvGM888s+893U3Jfvrpp/Pkk0/S0NDA3r17eeKJJzj9\n9NOT+NeKyFCllkaSHHvssdTV1TFp0iQmTJjAFVdcwWc+8xmOP/545s+fz/Tp03t8//XXX8+1117L\njBkzmDFjBvPmzQPghBNOYM6cOUyfPp3Jkydz6qmn7nvPddddxznnnLNvbKPD3Llzueaaa1iwYAEQ\nDITPmTNHXVEi0meaGl26pGMqMrgka2p0dU+JiEjClDRERCRhgyZpZFo320CmYyki3RkUSSMvL4+d\nO3fqyy4J3J2dO3eSl5eX7lBEZAAaFGdPlZaWUlFRQXV1dbpDGRTy8vIoLS1NdxgiMgANiqQRjUaZ\nOnVqusMQERn0Uto9ZWbnmNk7ZrbezG7pYnuumf0+3P66mU1JZTwiItI3KUsaZhYBfg6cC8wELjez\nmZ2KfQmocfejgP8EfpiqeEREpO9S2dJYAKx39w/cvQV4GLigU5kLgEXh68eAT5iZpTAmERHpg1SO\naUwCNsctVwAndlfG3VvNbA8wGtgRX8jMrgOuCxfrzeydlET8USWdYxngFG9qKd7UUrypdUwyKsmI\ngXB3vxu4u7/3a2Zlybjsvr8o3tRSvKmleFPLzHq+f3SCUtk9VQlMjlsuDdd1WcbMsoFiYGcKYxIR\nkT5IZdJYDkwzs6lmlgNcBizuVGYxcHX4+iLgz64r9EREBqyUdU+FYxQ3As8CEeBed19jZrcDZe6+\nGPgN8ICZrQd2ESSWgaTfu8T6SPGmluJNLcWbWkmJN+OmRhcRkfQZFHNPiYhI/1DSEBGRhA3ppGFm\nk81siZmtNbM1ZnZTF2XOMLM9ZrYyfNyajljj4tlgZm+FsXzkFDoL3BVOzbLKzOamI84wlmPijttK\nM6s1s5s7lUn78TWze81su5mtjls3ysyeN7P3wueR3bz36rDMe2Z2dVdl+ineO8zs7fDf/AkzG9HN\ne3v8/PRjvN81s8q4f/eF3by3x6mI+jHe38fFusHMVnbz3nQc3y6/x1L2GXb3IfsAJgBzw9eFwLvA\nzE5lzgD+b7pjjYtnA1DSw/aFwDOAAScBr6c75jCuCLAVOHygHV/gY8BcYHXcun8Hbglf3wL8sIv3\njQI+CJ9Hhq9HpineTwHZ4esfdhVvIp+ffoz3u8A/JvCZeR84AsgB3uz8/7O/4u20/T+AWwfQ8e3y\neyxVn+Eh3dJw9y3uviJ8XQesI7hKPZNdANzvgWXACDObkO6ggE8A77v7xnQH0pm7v0xw9l68+Clu\nFgGf7eKtnwaed/dd7l4DPA+ck7JAQ13F6+7PuXtruLiM4LqoAaGb45uIRKYiSrqe4g2nOboE+F2q\n40hUD99jKfkMD+mkES+cYXcO8HoXm082szfN7BkzO7ZfA/soB54zs/JwepXOupq+ZSAkwsvo/j/a\nQDq+Hca5+5bw9VZgXBdlBuqx/juC1mZXDvb56U83ht1p93bTdTIQj+/pwDZ3f6+b7Wk9vp2+x1Ly\nGVbSAMxsOPAH4GZ3r+20eQVBl8oJwE+BJ/s7vk5Oc/e5BLMH32BmH0tzPAcVXtx5PvBoF5sH2vH9\nCA/a8RlxbrqZfQtoBR7spshA+fz8EjgSmA1sIejyyQSX03MrI23Ht6fvsWR+hod80jCzKMGBftDd\nH++83d1r3b0+fP00EDWzkn4OMz6eyvB5O/AEQRM+XiLTt/S3c4EV7r6t84aBdnzjbOvo1guft3dR\nZkAdazO7BjgPuCL8kviIBD4//cLdt7l7m7u3A/d0E8dAO77ZwIXA77srk67j2833WEo+w0M6aYT9\nk78B1rn7j7opMz4sh5ktIDhmaZkfy8yGmVlhx2uCwc/VnYotBq4Kz6I6CdgT10RNl25/nQ2k49tJ\n/BQ3VwNPdVHmWeBTZjYy7F75VLiu35nZOcA3gfPdvaGbMol8fvpFp3G2z3UTRyJTEfWns4G33b2i\nq43pOr49fI+l5jPcn6P8A+0BnEbQZFsFrAwfC4GvAF8Jy9wIrCE4c2MZcEoa4z0ijOPNMKZvhevj\n4zWCm1+9D7wFzE/zMR5GkASK49YNqONLkNC2ADGCPt0vEUzR/yLwHvACMCosOx/4ddx7/w5YHz6u\nTWO86wn6pjs+x78Ky04Enu7p85OmeB8IP5+rCL7cJnSON1xeSHA20PvpjDdc/18dn9u4sgPh+Hb3\nPZaSz7CmERERkYQN6e4pERE5NEoaIiKSMCUNERFJmJKGiIgkTElDREQSpqQhEjKzNjtwVt6kzapq\nZlPiZ00VyVQpu92rSAZqdPfZ6Q5CZCBTS0PkIMJ7JPx7eJ+E/zazo8L1U8zsz+Gkey+a2WHh+nEW\n3NPizfBxSlhVxMzuCe958JyZ5YflvxbeC2GVmT2cpj9TJCFKGiL75Xfqnro0btsedz8e+Bnw43Dd\nT4FF7j6LYILAu8L1dwEveTAJ41yCq4MBpgE/d/djgd3A58P1twBzwnq+kqo/TiQZdEW4SMjM6t19\neBfrNwBnufsH4cRwW919tJntIJj+Ihau3+LuJWZWDZS6e3NcHVMI7lswLVz+ZyDq7t8zsz8B9QQz\n/D7p4QSOIgORWhoiifFuXh+K5rjXbewfU/xbgvnC5gLLw9lURQYkJQ2RxFwa97w0fP0awcyrAFcA\nr4SvXwSuBzCziJkVd1epmWUBk919CfDPQDHwkdaOyEChXzQi++Wb2cq45T+5e8dptyPNbBVBa+Hy\ncN1XgfvM7J+AauDacP1NwN1m9iWCFsX1BLOmdiUC/DZMLAbc5e67k/YXiSSZxjREDiIc05jv7jvS\nHYtIuql7SkREEqaWhoiIJEwtDRERSZiShoiIJExJQ0REEqakISIiCVPSEBGRhP1/XmMuiew0MHQA\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEWCAYAAACjYXoKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxMzMMvv/7zLadhWgxgkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq7fmArk0bIN",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.6 モデルによる予測\n",
        "\n",
        "精度だけ見ても面白くないので、検証用データの予測結果を見てみましょう。\n",
        "\n",
        "Sequential.predict関数によって予測を行うことができます。\n",
        "\n",
        "主な引数は次の通りです。\n",
        "\n",
        "* x_test：予測に使用する入力データ\n",
        "* batch_size：まとめて1度に予測を行うサンプル数\n",
        "* verbose：評価のログを出力するか（0:しない(デフォルト)、1：する）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7CJSTL50bIN",
        "colab_type": "code",
        "outputId": "b9c9a127-13d9-4b75-e858-516792551efb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "classes = model.predict(x_test, batch_size=128, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r24/24 [==============================] - 0s 15ms/sample\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx85EWpfW7Nz",
        "colab_type": "text"
      },
      "source": [
        "上記コマンドで検証用データの予測を実施しています。\n",
        "\n",
        "どのような結果が得られているかイメージしやすくするために、画像を1枚抽出して、その結果を実際に見てみます。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sTY0plvOe-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# データセットの推論結果と元画像を確認\n",
        "# test_numを0～9999で指定してください\n",
        "%matplotlib inline\n",
        "test_num=9\n",
        "test_img = np.squeeze(x_test[test_num])\n",
        "\n",
        "labels = np.array([0,1,2,3,4,5,6,7,8,9])\n",
        "labels = np.array([\n",
        "        'rx-178',\n",
        "        'msz-006',\n",
        "        'rx-93'])\n",
        "\n",
        "print(\"クラスごとの確からしさ(%)：\"+str(np.round(classes[test_num],decimals=2)*100))\n",
        "\n",
        "print(\"推論結果：\"+str(labels[classes[test_num].argmax()]))\n",
        "plt.imshow(test_img.astype(np.int),'gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ4aUVfFVYfE",
        "colab_type": "text"
      },
      "source": [
        "予測精度の最終確認として、Confusion Matrixを表示してみます。\n",
        "\n",
        "X軸とY軸が一致する箇所に集中していれば(要するに左上から右下に斜めに赤くなっている)、精度の良いモデルと言えるでしょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIOawSjjUl0y",
        "colab_type": "code",
        "outputId": "c1ef3226-06e4-4c49-c323-c420953ef2f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        }
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#print(classification_report(np.argmax(y_test, 1), np.argmax(classes, 1)))\n",
        "#print(confusion_matrix(np.argmax(y_test, 1), np.argmax(classes, 1)))\n",
        "\n",
        "cmatrix = confusion_matrix(np.argmax(y_test, 1), np.argmax(classes, 1))\n",
        "cmatrix_plt = pd.DataFrame(cmatrix, index=labels, columns=labels)\n",
        "\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(cmatrix_plt, annot=True, cmap=\"Reds\", fmt=\"d\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGfCAYAAACNytIiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3X2YXWV57/Hvb5KgBBWsQkCSAsUX\nDhalFhBtFQQVUQEVPGK1IIoRX4ray1atLa14adVja2tROFFRrC/HlqoFBcWKFkQEIyCILxUUCG8B\nQaCoICT3+WPv4Jgms3cme8/KM/P95FoXa6+19tr3hJWZe+77edZKVSFJktSVia4DkCRJc5vJiCRJ\n6pTJiCRJ6pTJiCRJ6pTJiCRJ6pTJiCRJ6pTJiCRJ6pTJiCRJ6pTJiCRJ6tT8cX/Ax7faxlu8aqRe\nfP2Pug5Bkqa2cMvM5McdkweN7GftSXXHjMYOVkYkSVLHxl4ZkSRJ49V6ZaH1+CVJUuOsjEiS1LiJ\nzPgwj5EyGZEkqXGttzlaj1+SJDXOyogkSY2baLtLYzIiSVLrWm9ztB6/JElqnJURSZIa52waSZLU\nqdbbHK3HL0mSGmdlRJKkxjmbRpIkdar1Nkfr8UuSpMZZGZEkqXFxNo0kSepS622O1uOXJEmNszIi\nSVLjWp9NY2VEkqTGTYxwmUqSRyW5ZNJyR5LXrXXMvklun3TMcYPitzIiSZKGUlU/BHYHSDIPuA74\n7DoOPbeqnj3seU1GJElqXEfPptkfuLKqrt7YE9mmkSSpcTPVplnL4cCn1rPvCUm+k+TMJI8eJn5J\nkiQAkixNsnzSsnQdx2wGHAz86zpOcRGwQ1U9Fvgn4HODPtM2jSRJjRvlbJqqWgYsG3DYgcBFVbVy\nHe+/Y9L6GUk+kOShVfXT9Z3MZESSpMZ10OZ4Ietp0STZFlhZVZVkL3rh3TLVyUxGJEnS0JJsATwN\neMWkbccAVNVJwGHAK5PcC/wSOLyqaqpzmoxIktS4CWZuNk1V/Rx4yFrbTpq0fgJwwoac02REkqTG\neQdWSZKkjWBlRJKkxrVeWTAZkSSpca23aUxGJElq3EwOYB2H1is7kiSpcVZGJElqnG0aSZLUqdbb\nHK3HL0mSGmdlRJKkxtmmkSRJnXI2jSRJ0kawMiJJUuNs00iSpE41novYppEkSd2yMiJJUuNs00iS\npE45m0aSJGkjWBmRJKlxtmkkSVKnWm9ztB6/JElqnJURSZIa13iXxmREkqTWTaTtdMQ2jSRJ6pSV\nEUmSGtd2XcRkRJKk5rWejNimkSRJnbIyIklS41qvjJiMSJLUuDibRpIkafpMRjr2oIfvzDPPPfu+\n5X9fcyW7vHJp12Gpceecdz4HPOcwnnbw81h28ildh6NZwGtq05YRLl2wTdOxO664kjOetB8AmZjg\ned+/lBWfP6PjqNSyVatWcfw7381HTjyBRYu24bAXHcl++zyJh+/8O12HpkZ5TW36Wq8stB7/rLLt\nPk/mv39yFT9fcW3Xoahhl373cnZYspgli7dnswULeNYBT+crXzun67DUMK8pjZvJyCZkh0Ofw1X/\n9pmuw1DjVt50M9suWnTf60WLtmHlzTd3GJFa5zW16UtGt3RhymQkyR5Jvprk40mWJPlyktuTfCvJ\n781UkHPBxIIFLD7wAK753OldhyJJakxG+KcLgyojHwDeDXwB+Abwf6tqS+BN/X3rlGRpkuVJlp/9\nq1+OLNjZ7GFP259bv3MZd/nbhjbSom225saVK+97vXLlTSzaeusOI1LrvKY0boOSkQVVdWZVfQqo\nqjqV3spXgPuv701Vtayq9qiqPfbbbPMRhjt77Xjoc23RaCR2e/SuXHXNClZcdx2/uucevvCls9hv\n3yd1HZYa5jW16Zvts2nuSvJ0YEugkjynqj6XZB9g1fjDmxvmLVzIdk/Zhwte/4auQ9EsMH/+fI57\n459x9KuOZdXq1Rx6yEE8Yueduw5LDfOa2vS1fcszSFWtf2fyWHptmtXA64FXAkcC1wEvr6pvDPqA\nj2+1zfo/QJqGF1//o65DkKSpLdxyRvODzz9ku5H9rH32LTfMeG4zZWWkqr4DHDBp02v7iyRJ2kRM\nNF4aGTSb5tgki2cqGEmStOFm+2yatwEXJjk3yauSOHxakiSN1KBk5MfAYnpJye8D30vyxSRHJnng\n2KOTJEkDzfbZNFVVq4GzgLOSLAAOBF4IvAewUiJJUse6unPqqAxKRn7jy6uqe4DTgNOSLBxbVJIk\nac4YlIy8YH07quoXI45FkiRNQ+OFkYFTe/9rffuSPKCq7hx9SJIkaUNMNJ6ObMxTe783sigkSdKc\nNWVlJMmfrm8X8IDRhyNJkjZU23WRwZWRdwAPBh641vKAId4rSZJmQDK6pQuDBrBeBHyuqr699o4k\nR48nJEmSNJcMSkaOAm5Zz749RhyLJEmahtbbNINm0/xw7W1Jtq2qG6tq5fjCkiRJw+rqmTKjMp1x\nH2eMPApJkjRnDWrTrEvb6ZckSbPMROM/maeTjHxw5FFIkqRpazwXGa5Nk+Spa9ar6gP9bUeOKyhJ\nkjR3DDtm5LgkJybZIsmiJKcDB40zMEmSNJyMcOnCsMnIPsCVwCXA14FPVtVhY4tKkiQNLSP804Vh\nx4w8GNiLXkKyGNghSaqqxhaZJEkaSld3Th2VgZWRJAG+BXyxqp4B7Ak8DDhvzLFJkqQ5YGBlpKqq\nXwU5uf/6l8CxSZ489ugkSdJArT8sbtj4z02y5+QNVXXOGOKRJEkbaK4MYH08cH6SK5NcmuSyJJeO\nMzBJkrTpSbJVklOT/CDJ95M8Ya39SfK+JFf0c4bHDTrnsANYD5hWxJIkaewysyNY/5HeONLDkmwG\nLFxr/4HAI/rL44ET+/9dr6GSkaq6esNjlSRJM2GmUpEkWwJPBl4CUFW/An611mGHAB/rz7j9Zr+S\nsl1V3bC+87Y+5kWSJI1QkqVJlk9alk7avRNwM/CRJBcn+VCSLdY6xfbAikmvr+1vWy+TEUmSGjfK\nAaxVtayq9pi0LJv0UfOBxwEnVtXvAT8H3rSx8ZuMSJLUuCQjWwa4Fri2qi7ovz6VXnIy2XXAkkmv\nF/e3rZfJiCRJGkpV3QisSPKo/qb9ge+tddhpwBH9WTV7A7dPNV4Ehp9NI0mSNlETM3uDkD8BPtGf\nSfNj4KgkxwBU1UnAGcAzgSuAXwBHDTqhyYgkSY3LDGYjVXUJsMdam0+atL+AV2/IOW3TSJKkTlkZ\nkSSpca0/tddkRJKkxrWejNimkSRJnbIyIklS42b42TQjZzIiSVLjGs9FbNNIkqRuWRmRJKlxtmkk\nSVKnGs9FbNNIkqRuWRmRJKlxE42XRkxGJElqXOO5iG0aSZLULSsjkiQ1ztk0kiSpU2m8z9F4+JIk\nqXVWRiRJapxtGkmS1KnGcxHbNJIkqVtWRiRJapxtGkmS1KnGcxHbNJIkqVtWRiRJapzPppEkSZ1q\nPBexTSNJkrplZUSSpMY5m2aAF1//o3F/hOaYY7ZY0nUImkVOvOq8rkPQLJQddpvZz2s7F7FNI0mS\numWbRpKkxrVeGTEZkSSpcZloOxuxTSNJkjplZUSSpMbZppEkSZ1q/Q6stmkkSVKnrIxIktS4xgsj\nJiOSJLWu9Tuw2qaRJEmdsjIiSVLjGi+MmIxIktQ62zSSJEkbwcqIJEmNa7wwYjIiSVLrWm/TmIxI\nktS4ND7oovHwJUlS66yMSJLUONs0kiSpWxNtJyO2aSRJUqesjEiS1DrbNJIkqUutjxmxTSNJkjpl\nZUSSpNY1PoDVZESSpNbZppEkSZo+KyOSJDUutmkkSVKnbNNIkiRNn5URSZIaZ5tGkiR1yzaNJEnS\n9FkZkSSpdbZpJElSl3w2jSRJ0kYwGZEkqXUTGd0yhCTzklyc5PPr2PeSJDcnuaS/HD3ofLZpJElq\n3cy3aV4LfB940Hr2f7qqXjPsyayMSJKkoSVZDDwL+NCozmkyIklS4zIxumUI/wD8ObB6imMOTXJp\nklOTLBl0QpMRSZJal4xsSbI0yfJJy9Jff0yeDdxUVd+eIprTgR2r6jHAl4FTBoXvmBFJknSfqloG\nLFvP7j8ADk7yTOD+wIOSfLyqXjzp/bdMOv5DwLsHfaaVEUmSGpeJjGyZSlW9uaoWV9WOwOHA2ZMT\nEYAk2016eTC9ga5TsjIiSVLrOr7pWZLjgeVVdRpwbJKDgXuBW4GXDHq/yYgkSdpgVfU14Gv99eMm\nbX8z8OYNOZfJiCRJrfPZNJIkqUs+m0aSJGkjWBmRJKl1tmkkSVKnbNNIkiRNn5URSZIa1/oAVpMR\nSZJa1/iYEds0kiSpU1ZGJElqnG0aSZLULds0kiRJ02dlRJKk1tmmkSRJXYptGm2sc847nwOecxhP\nO/h5LDv5lK7D0Syw/+tezXHfvYC/uuybvOyTJzP/fvfrOiQ17C/+7v088fkv5aCXv77rUDRLmYx0\nbNWqVRz/znfzoRP+kS/826f5/Be/xBVX/rjrsNSwrR62HU859hX87R778Lbd9mZi3gR7Hn5o12Gp\nYc992lP44Dv+suswNJVkdEsHTEY6dul3L2eHJYtZsnh7NluwgGcd8HS+8rVzug5LjZuYP58Fm2/O\nxLx5LFi4kNuuv7HrkNSwPR+zK1s+8AFdh6GpTGR0SxfhT7UzyXOT/FZ/feskH0tyWZJPJ1k8MyHO\nbitvupltFy267/WiRduw8uabO4xIrbvt+hv4j/f8E++45nLedcOPuOv2O/j+l8/uOixJWq9BlZG3\nV9Wt/fUTgIuBA4EzgY+MMzBJ07Nwq614zCHP5C932o03PuyRbLbFQvZ60Qu6DkvSGCUZ2dKFQcnI\nvEnrD6+q91bVtVX1UWDr9b0pydIky5MsX3byR0cQ5uy1aJutuXHlyvter1x5E4u2Xu9frTTQLk/d\nl1t+cjV3/vQWVt97Lxd/5nR2fuLjuw5L0jjN5jYN8LUkxyfZvL/+XIAkTwFuX9+bqmpZVe1RVXss\nfelLRhftLLTbo3flqmtWsOK66/jVPffwhS+dxX77PqnrsNSwW6+5lp323pMFm28OwC7778MN3/9h\nx1FJ0voNus/Ia4C3AGu+k70+yc+B04E/Hmdgc8X8+fM57o1/xtGvOpZVq1dz6CEH8Yidd+46LDXs\nqguXc9Gp/85bLjqXVffey4qLL+Xry+yqavr+9B3v5VuXXs7Pbv9v9vmjpfzJH7+Aww7cv+uwNFnj\nNz1LVQ13YLIlML+qbtmgT/jF7cN9gDSkY7ZY0nUImkVOvOq8rkPQLJQddpvR7ODeYw8e2c/a+e87\nbcYzm4F3YO0nIc8Atu+/vg74UlXdNubYJEnSMBqvjAya2nsEcBGwL7CwvzwF+HZ/nyRJ0kYZVBl5\nC/D7a1dBkjwYuAD42LgCkyRJQ5po+x6mg5KRAOvqQ63u75MkSV1rvE0zKBl5O3BRkrOAFf1tvw08\nDXjbOAOTJElzw5R1nao6BdgD+E/g7v7yNWCP/o3PJElS1xp/UN7A2TRV9bMkX6U/mwa4rqp+Nt6w\nJEnS0GZzmybJ7sBJwJbAtfTGiSxOchvwqqq6aPwhSpKk2WxQZeSjwCuq6oLJG5PsTe9BeY8dU1yS\nJGlYs3w2zRZrJyIAVfXNJFuMKSZJkrQhZnObBjgzyRfo3U9kzWyaJcARwBfHGZgkSZobpkxGqurY\nJAcChzBpACvw/qo6Y9zBSZKkIczyyghVdSZw5prXSbatqhvHGpUkSRpe48nIdEa8WBGRJEkjM7Ay\nsg5tp1+SJM02s3w2zbp8cORRSJKk6ZsLbZokH+7fAI2q+kB/29+MMS5JkjRHDFvXOQA4JckRk7Yd\nPIZ4JEnShmr82TTDJiM3AU8Gnp/k/Unm49gRSZI2DXMkGUlV3V5VBwE303ty75Zji0qSJM0Zww5g\nPW3NSlX9TZJvA68fT0iSJGlDZI7Mpnl3komqWp3kkfRaNAeMMS5JkjSsuTCbBjgHuH+S7YGzgD+m\n99ReSZKkjbIhY0Z+ATwP+EBVPR/YbXxhSZKkoTU+gHXYNk2SPAF4EfCy/ra2G1SSJM0Wc6RN81rg\nTcBnquryJDsBZ48vLEmSNFcMWxn5BbAaeGGSF9MbwFpji0qSJA1vjsym+QTwBuC79JISSZK0qWi8\nTTNsMnJzVZ0+1kgkSdKcNGwy8tdJPgR8Bbh7zcaq+sxYopIkScObI5WRo4BdgAX8uk1TgMmIJEld\nmyPJyJ5V9aixRiJJkuakYZORbyTZtaq+N9ZoJEnShpsjs2n2Bi5J8hN6Y0YCVFU9ZmyRSZKk4cyR\nNs0zxhqFJEmas4ZKRqrq6nEHIkmSpmmOVEYkSdKmqvExI21HL0mSmmdlRJKk1tmmkSRJnWo8GbFN\nI0mShpLk/kkuTPKdJJcnees6jrlfkk8nuSLJBUl2HHRekxFJklqXjG6Z2t3AflX1WGB34BlJ9l7r\nmJcBP6uqhwPvBd416KQmI5IktW5iYnTLFKrnzv7LBf2l1jrsEOCU/vqpwP7J1FmOyYgkSbpPkqVJ\nlk9alq61f16SS4CbgC9X1QVrnWJ7YAVAVd0L3A48ZKrPdACrJEmtG+EA1qpaBiybYv8qYPckWwGf\nTfK7VfXdjflMKyOSJLVu5saM3KeqbgO+yv98ZMx1wJJeWJkPbAncMtW5TEYkSdJQkmzdr4iQZHPg\nacAP1jrsNODI/vphwNlVtfa4kt9gm0aSpNZlxmoL2wGnJJlHr6DxL1X1+STHA8ur6jTgw8A/J7kC\nuBU4fNBJTUYkSWrdxMzc9KyqLgV+bx3bj5u0fhfw/A05r20aSZLUKSsjkiS1bubaNGNhMiJJUut8\nNo0kSdL0WRmRJKl1A27jvqkzGZEkqXWNt2lMRiRJal3jA1jbjl6SJDXPyogkSa2zTSPNrJN+vqLr\nEDSLHLPFkq5D0Cx0Ut0xsx/Y+ADWtqOXJEnNszIiSVLrbNNIkqROOZtGkiRp+qyMSJLUugnbNJIk\nqUu2aSRJkqbPyogkSa1zNo0kSeqUbRpJkqTpszIiSVLrnE0jSZI61fiYEds0kiSpU1ZGJElqXeMD\nWE1GJElqXeNjRtpOpSRJUvOsjEiS1DrbNJIkqVPOppEkSZo+KyOSJLXONo0kSeqUs2kkSZKmz8qI\nJEmts00jSZI65WwaSZKk6bMyIklS6ybari2YjEiS1DrbNJIkSdNnZUSSpNY5m0aSJHXKNo0kSdL0\nWRmRJKl1zqaRJEmdsk0jSZI0fVZGJElqnbNpJElSp2zTSJIkTZ+VEUmSWmebRpIkdWrCNo0kSdK0\nWRmRJKl1tmkkSVKnnE0jSZI0fVZGJElqnW0aSZLUpdimkSRJmj4rI5Iktc42jSRJ6lTjyUjb0UuS\npOZZGZEkqXWN3w7eZESSpNY13qYxGZEkqXVO7ZUkSXNFkpOT3JTku+vZv2+S25Nc0l+OG3ROKyOS\nJLVuZts0HwVOAD42xTHnVtWzhz2hyYgkSa2bwTZNVZ2TZMdRntM2jSRJuk+SpUmWT1qWTuM0T0jy\nnSRnJnn0oIOtjEiS1LoRtmmqahmwbCNOcRGwQ1XdmeSZwOeAR0z1BisjkiS1biKjWzZSVd1RVXf2\n188AFiR56JThb/SnSpIk9SXZNv3HCCfZi16ucctU77FNswk457zzefv/+TtWr17N859zCEtfemTX\nIalxXlMatf1f92r+4OgjqCquv+x7nHLUK7n37ru7DktrzOBsmiSfAvYFHprkWuCvgQUAVXUScBjw\nyiT3Ar8EDq+qmuqcJiMdW7VqFce/89185MQTWLRoGw570ZHst8+TePjOv9N1aGqU15RGbauHbcdT\njn0Fb911L+656y5e/umPsufhh3L+KZ/sOjStMbOzaV44YP8J9Kb+Ds02Tccu/e7l7LBkMUsWb89m\nCxbwrAOezle+dk7XYalhXlMah4n581mw+eZMzJvHgoULue36G7sOSbPIwGQkyV5J9uyv75rkT/uj\nYzUCK2+6mW0XLbrv9aJF27Dy5ps7jEit85rSqN12/Q38x3v+iXdccznvuuFH3HX7HXz/y2d3HZYm\ny8Tolg5M+alJ/hp4H3Bikr+lV3bZAnhTkrdM8b775igvO/mjo4xXkjTDFm61FY855Jn85U678caH\nPZLNtljIXi96QddhabJkdEsHBo0ZOQzYHbgfcCOwuKruSPIe4ALg7et602/MUf7F7VMOWpnrFm2z\nNTeuXHnf65Urb2LR1lt3GJFa5zWlUdvlqftyy0+u5s6f9iZEXPyZ09n5iY/nwk98uuPINFsMqsfc\nW1WrquoXwJVVdQdAVf0SWD326OaA3R69K1dds4IV113Hr+65hy986Sz22/dJXYelhnlNadRuveZa\ndtp7TxZsvjkAu+y/Dzd8/4cdR6Xf0HibZlBl5FdJFvaTkd9fszHJlpiMjMT8+fM57o1/xtGvOpZV\nq1dz6CEH8Yidd+46LDXMa0qjdtWFy7no1H/nLRedy6p772XFxZfy9WUf6TosTTbR9nyUTDX1N8n9\nqup/TCTv30ltu6q6bOAn2KaRtAk7ZoslXYegWeikumNGB1+s/sH5I/tZO7HLE2Z84MiUlZE1iUiS\nrYHFwCrgx1X1U+Cn4w9PkiQNko4Gno7KlMlIkl3pzabZEfht4GJgmyT/Cby2qm4fe4SSJGlqHY31\nGJVB0Z8MvLqqHg78IfCDqtoJOA/48LiDkyRJs9+gZGTzqvohQFVdCOzWX/8g8OgxxyZJkoYxy+8z\ncmWSvwLOBp4HXAKQZAHeSl6SpE3DLG/TvBR4IPBm4C7gtf3tCwEfAypJkjbaoNk0twF/Pnlbkm2r\n6kbgm+MMTJIkDWk2z6ZZjzOAx406EEmSNE2N3/RsOtG3nX5JkqRNynQqIx8ceRSSJGn6Gm/TDFUZ\nSfLUNetV9YH+NgewSpK0KWj8QXnDfupxSU5MskWSRUlOBw4aZ2CSJGluGDYZ2Qe4kt59Rr4OfLKq\nDhtbVJIkaXiN3/Rs2GTkwcBe9BKSu4Ed0vpTeSRJmjUywmXmDUxG+knHt4AvVtUzgD2Bh9F7Po0k\nSdJGGTibpqoqSarq5P7rXwLHJnny2KOTJEmDNd6sGLZNc26SPSdvqKpzxhCPJEnaUI2PGRn2PiOP\nB16U5Grg5/SaSlVVjxlbZJIkaU4YNhk5YKxRSJKkjdB2m2aoZKSqrh53IJIkaZrmyJgRSZKksZjO\ns2kkSdKmpO3CiMmIJEntazsbsU0jSZI6ZWVEkqTWNT6A1WREkqTWNZ6M2KaRJEmdsjIiSVLz2q6M\nmIxIktQ62zSSJEnTZ2VEkqTmtV0ZMRmRJKl1tmkkSZKmz8qIJEmta7wyYjIiSVLz2k5GbNNIkqRO\nWRmRJKlxsU0jSZI6ZTIiSZK61XYy4pgRSZLUKSsjkiS1zjaNJEnqVOPJiG0aSZLUKSsjkiQ1r+3K\niMmIJEmts00jSZI0fVZGJElqXduFEZMRSZLa13Y2YptGkiR1ysqIJEmta3wAq8mIJEmtazwZsU0j\nSZI6ZWVEkqTmtV0ZMRmRJKl1tmkkSZKmz2REkqTWJaNbBn5UnpHkh0muSPKmdey/X5JP9/dfkGTH\nQec0GZEkqXkZ4TLFpyTzgPcDBwK7Ai9Msutah70M+FlVPRx4L/CuQdGbjEiSpGHtBVxRVT+uql8B\n/w84ZK1jDgFO6a+fCuyfTF1yMRmRJKl1M9em2R5YMen1tf1t6zymqu4FbgceMtVJxz+bZuGWbQ/x\nnUFJllbVsq7j0Ozg9TSck+qOrkNohtfUJmyEP2uTLAWWTtq0bNz/362MbFqWDj5EGprXk0bNa2oO\nqKplVbXHpGVyInIdsGTS68X9bazrmCTzgS2BW6b6TJMRSZI0rG8Bj0iyU5LNgMOB09Y65jTgyP76\nYcDZVVVTndSbnkmSpKFU1b1JXgN8CZgHnFxVlyc5HlheVacBHwb+OckVwK30EpYpZUCyohlkP1aj\n5PWkUfOa0riYjEiSpE45ZkSSJHXKZKRjSXZJcn6Su5O8YdL2RyW5ZNJyR5LX9fftnuSb/e3Lk+zV\n3VegFqTnff3bM1+a5HGT9h2Z5Ef95chJ2zdLsizJfyX5QZJDu4lem5IkD07y2f51dGGS3+1vv3//\n9XeSXJ7krV3HqnY4gHVM+nebS1WtHnDorcCxwHMmb6yqHwK79881j95Uqc/2d78beGtVnZnkmf3X\n+44ues1CBwKP6C+PB04EHp/kt4C/BvYACvh2ktOq6mfAW4CbquqRSSaA3+omdM2EDfie9RfAJVX1\n3CS70Ls1+P7A3cB+VXVnkgXA15OcWVXfHG/kmg2sjIxQkh37Dw/6GPBj4MokD00ykeTcJE9f+z1V\ndVNVfQu4Z4pT7w9cWVVXr3kb8KD++pbA9SP8MtSR/vXzgyQf7VcjPpHkqUnO61ct9kqyz6Rq2cVJ\nHpjk+EnbrkvykXWc/hDgY9XzTWCrJNsBBwBfrqpb+wnIl4Fn9N/zUuBvAapqdVX9dAb+GjSDpvM9\ni97zSM4GqKofADsmWdS/tu7sH7OgvzgoUUOxMjJ6jwCOrKojkhxN7zfQC4HvVdVZ0zzn4cCnJr1+\nHfClJO+hl1A+cWMC1ibl4cDz6SUC3wL+CPhD4GB6v5HOA15dVecleQBwV1UdBxyXZCvgXOCEdZx3\nfbdwXuf2/rkA3pZkX+BK4DVVtXIkX6U2JRv6Pes7wPOAc/st4h3o3fhqZb+K+2161/H7q+qCGfkK\n1DwrI6N39ZqyZFV9iF4F4xjgDVO+az36N5U5GPjXSZtfCby+qpYAr6c3p1uzw0+q6rJ+qfxy4Cv9\nmwVdBuwInAf8fZJjga36z31YU2L/OPD3VfXtEcQxn94PmG9U1eOA84H3jOC82vRs6Pesd9KrrF0C\n/AlwMbCq//5VVbU7vWtnrzXjSaRBTEZG7+drVpIspPePEuAB/W2vnlRSf9gQ5zsQuGit30iPBD7T\nX/9Xek9R1Oxw96T11ZNerwbmV9U7gaOBzYHz+j17gL8Brq2qj8A6r7P13cJ5fdtvAX7Bb15nj0Oz\n0QZ9z6qqO6rqqH7ScQSwNb0Wz32q6jbgq/y65SdNyWRkvN4FfAI4DvggQFW9v6p27y/DjPV4Ib/Z\nooHeGJF9+uv7AT8aUbzaxCVWJHIjAAABOElEQVTZuV85eRe9Ns4uSQ4CnkpvIDSwzuvsNOCI/qya\nvYHbq+oGendRfHp/hsSDgacDX+pXY07n1wOj9we+N1Nfpzoz8HtWkq36FVvoJcbnVNUdSbZe095L\nsjnwNOAHHXwNapBjRsYkyT7AnsAfVNWqJIcmOWrNb66TjtsWWE6vNLq6P3131/4/7i3o/YN+xVqn\nfznwj+k9gOgufHjVXPK6JE+hVym5HDgT+CK9sR8X9ro1nNYfRzLZGcAzgSvoVTyOAqiqW5O8jV5i\nA3B8Vd3aX38jvVs6/wNw85r3aHYa9nsW8L+AU5IUvWvwZf3t2/W3z6P3i+6/VNXnZyp+tc07sEqS\npE7ZppEkSZ0yGZEkSZ0yGZEkSZ0yGZEkSZ0yGZEkSZ0yGZEkSZ0yGZEkSZ0yGZEkSZ36/8pAMHVI\ndBqkAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqbPWg1mfbA3",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.7 モデルのファイル出力\n",
        "\n",
        "学習させたモデルを出力し、静的学習済みモデルとして外部で活用することもできます。\n",
        "\n",
        "ここでは、Keras形式に加えて、TensorFlowのSaved Model形式も試してみます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tcWm1vmlVvA",
        "colab_type": "code",
        "outputId": "740d25bf-40da-4f0d-81a0-3e9ced704da9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Keras形式でモデルを出力\n",
        "output_keras_name = f\"{model_dir}02_{model_opt}_{epochs}_frozen_graph.h5\"\n",
        "model.save(output_keras_name, include_optimizer=False)\n",
        "\n",
        "# TensorFlow Saved Model形式でモデルを出力\n",
        "from tensorflow.contrib import saved_model\n",
        "\n",
        "out_tf_saved_model = f\"{model_dir}02_{model_opt}_{epochs}_saved_models\"\n",
        "\n",
        "if os.path.exists(out_tf_saved_model):\n",
        "    shutil.rmtree(out_tf_saved_model)\n",
        "#saved_model_path = saved_model.save_keras_model(model, out_tf_saved_model)\n",
        "saved_model_path = saved_model.save_keras_model(model, \"./saved_modelxy\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0712 07:02:05.427572 140019297666944 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0712 07:02:05.429402 140019297666944 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0712 07:02:05.459376 140019297666944 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0712 07:02:13.526880 140019297666944 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:253: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "W0712 07:02:13.528868 140019297666944 export_utils.py:182] Export includes no default signature!\n",
            "W0712 07:02:18.477467 140019297666944 export_utils.py:182] Export includes no default signature!\n",
            "W0712 07:02:20.730558 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-1._updates\n",
            "W0712 07:02:20.732207 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-3._updates\n",
            "W0712 07:02:20.737928 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-5._updates\n",
            "W0712 07:02:20.739681 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-8._updates\n",
            "W0712 07:02:20.741731 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-10._updates\n",
            "W0712 07:02:20.743557 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-12._updates\n",
            "W0712 07:02:20.745907 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-14._updates\n",
            "W0712 07:02:20.747919 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-16._updates\n",
            "W0712 07:02:20.749972 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-18._updates\n",
            "W0712 07:02:20.751935 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-20._updates\n",
            "W0712 07:02:20.753929 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-22._updates\n",
            "W0712 07:02:20.756004 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-24._updates\n",
            "W0712 07:02:20.758124 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-27._updates\n",
            "W0712 07:02:20.760372 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-29._updates\n",
            "W0712 07:02:20.762562 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-31._updates\n",
            "W0712 07:02:20.765097 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-33._updates\n",
            "W0712 07:02:20.768019 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-35._updates\n",
            "W0712 07:02:20.769976 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-37._updates\n",
            "W0712 07:02:20.772171 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-39._updates\n",
            "W0712 07:02:20.773956 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-41._updates\n",
            "W0712 07:02:20.775869 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-43._updates\n",
            "W0712 07:02:20.777520 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-46._updates\n",
            "W0712 07:02:20.779489 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-48._updates\n",
            "W0712 07:02:20.781364 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-50._updates\n",
            "W0712 07:02:20.783636 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-52._updates\n",
            "W0712 07:02:20.785667 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-54._updates\n",
            "W0712 07:02:20.787399 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-56._updates\n",
            "W0712 07:02:20.789269 140019297666944 util.py:244] Unresolved object in checkpoint: (root).layer_with_weights-58._updates\n",
            "W0712 07:02:20.790977 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer.decay\n",
            "W0712 07:02:20.792953 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "W0712 07:02:20.794739 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer.momentum\n",
            "W0712 07:02:20.796727 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-0.kernel\n",
            "W0712 07:02:20.798446 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-0.bias\n",
            "W0712 07:02:20.801171 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-1.gamma\n",
            "W0712 07:02:20.802736 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-1.beta\n",
            "W0712 07:02:20.804390 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-2.kernel\n",
            "W0712 07:02:20.805710 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-2.bias\n",
            "W0712 07:02:20.807877 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-3.gamma\n",
            "W0712 07:02:20.809528 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-3.beta\n",
            "W0712 07:02:20.811012 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-4.kernel\n",
            "W0712 07:02:20.812676 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-4.bias\n",
            "W0712 07:02:20.814352 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-5.gamma\n",
            "W0712 07:02:20.816045 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-5.beta\n",
            "W0712 07:02:20.817673 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-6.kernel\n",
            "W0712 07:02:20.819013 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-6.bias\n",
            "W0712 07:02:20.820585 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-7.kernel\n",
            "W0712 07:02:20.822259 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-7.bias\n",
            "W0712 07:02:20.823754 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-8.gamma\n",
            "W0712 07:02:20.825424 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-8.beta\n",
            "W0712 07:02:20.827175 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-9.kernel\n",
            "W0712 07:02:20.828838 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-9.bias\n",
            "W0712 07:02:20.830238 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-10.gamma\n",
            "W0712 07:02:20.831815 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-10.beta\n",
            "W0712 07:02:20.833542 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-11.kernel\n",
            "W0712 07:02:20.835104 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-11.bias\n",
            "W0712 07:02:20.836377 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-12.gamma\n",
            "W0712 07:02:20.838460 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-12.beta\n",
            "W0712 07:02:20.840087 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-13.kernel\n",
            "W0712 07:02:20.841902 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-13.bias\n",
            "W0712 07:02:20.843581 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-14.gamma\n",
            "W0712 07:02:20.845128 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-14.beta\n",
            "W0712 07:02:20.846821 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-15.kernel\n",
            "W0712 07:02:20.848456 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-15.bias\n",
            "W0712 07:02:20.850294 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-16.gamma\n",
            "W0712 07:02:20.851993 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-16.beta\n",
            "W0712 07:02:20.853806 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-17.kernel\n",
            "W0712 07:02:20.855271 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-17.bias\n",
            "W0712 07:02:20.857206 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-18.gamma\n",
            "W0712 07:02:20.858937 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-18.beta\n",
            "W0712 07:02:20.860630 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-19.kernel\n",
            "W0712 07:02:20.862241 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-19.bias\n",
            "W0712 07:02:20.863886 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-20.gamma\n",
            "W0712 07:02:20.865302 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-20.beta\n",
            "W0712 07:02:20.867073 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-21.kernel\n",
            "W0712 07:02:20.868880 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-21.bias\n",
            "W0712 07:02:20.870469 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-22.gamma\n",
            "W0712 07:02:20.872077 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-22.beta\n",
            "W0712 07:02:20.873763 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-23.kernel\n",
            "W0712 07:02:20.875404 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-23.bias\n",
            "W0712 07:02:20.876723 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-24.gamma\n",
            "W0712 07:02:20.878341 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-24.beta\n",
            "W0712 07:02:20.879904 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-25.kernel\n",
            "W0712 07:02:20.881705 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-25.bias\n",
            "W0712 07:02:20.883280 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-26.kernel\n",
            "W0712 07:02:20.884573 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-26.bias\n",
            "W0712 07:02:20.886159 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-27.gamma\n",
            "W0712 07:02:20.887770 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-27.beta\n",
            "W0712 07:02:20.889336 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-28.kernel\n",
            "W0712 07:02:20.890866 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-28.bias\n",
            "W0712 07:02:20.892673 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-29.gamma\n",
            "W0712 07:02:20.894282 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-29.beta\n",
            "W0712 07:02:20.895917 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-30.kernel\n",
            "W0712 07:02:20.897423 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-30.bias\n",
            "W0712 07:02:20.899021 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-31.gamma\n",
            "W0712 07:02:20.900797 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-31.beta\n",
            "W0712 07:02:20.902430 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-32.kernel\n",
            "W0712 07:02:20.903959 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-32.bias\n",
            "W0712 07:02:20.905297 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-33.gamma\n",
            "W0712 07:02:20.906872 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-33.beta\n",
            "W0712 07:02:20.908385 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-34.kernel\n",
            "W0712 07:02:20.910066 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-34.bias\n",
            "W0712 07:02:20.911668 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-35.gamma\n",
            "W0712 07:02:20.913280 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-35.beta\n",
            "W0712 07:02:20.914581 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-36.kernel\n",
            "W0712 07:02:20.916137 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-36.bias\n",
            "W0712 07:02:20.917612 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-37.gamma\n",
            "W0712 07:02:20.919119 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-37.beta\n",
            "W0712 07:02:20.920851 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-38.kernel\n",
            "W0712 07:02:20.922478 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-38.bias\n",
            "W0712 07:02:20.924010 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-39.gamma\n",
            "W0712 07:02:20.925397 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-39.beta\n",
            "W0712 07:02:20.927051 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-40.kernel\n",
            "W0712 07:02:20.928975 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-40.bias\n",
            "W0712 07:02:20.930827 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-41.gamma\n",
            "W0712 07:02:20.933162 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-41.beta\n",
            "W0712 07:02:20.935292 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-42.kernel\n",
            "W0712 07:02:20.936891 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-42.bias\n",
            "W0712 07:02:20.938428 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-43.gamma\n",
            "W0712 07:02:20.940020 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-43.beta\n",
            "W0712 07:02:20.941455 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-44.kernel\n",
            "W0712 07:02:20.943728 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-44.bias\n",
            "W0712 07:02:20.945502 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-45.kernel\n",
            "W0712 07:02:20.947093 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-45.bias\n",
            "W0712 07:02:20.948701 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-46.gamma\n",
            "W0712 07:02:20.950254 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-46.beta\n",
            "W0712 07:02:20.951874 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-47.kernel\n",
            "W0712 07:02:20.954102 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-47.bias\n",
            "W0712 07:02:20.956485 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-48.gamma\n",
            "W0712 07:02:20.958360 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-48.beta\n",
            "W0712 07:02:20.959644 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-49.kernel\n",
            "W0712 07:02:20.961917 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-49.bias\n",
            "W0712 07:02:20.964086 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-50.gamma\n",
            "W0712 07:02:20.965679 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-50.beta\n",
            "W0712 07:02:20.967962 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-51.kernel\n",
            "W0712 07:02:20.969774 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-51.bias\n",
            "W0712 07:02:20.971471 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-52.gamma\n",
            "W0712 07:02:20.973426 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-52.beta\n",
            "W0712 07:02:20.975128 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-53.kernel\n",
            "W0712 07:02:20.976128 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-53.bias\n",
            "W0712 07:02:20.978483 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-54.gamma\n",
            "W0712 07:02:20.980255 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-54.beta\n",
            "W0712 07:02:20.981930 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-55.kernel\n",
            "W0712 07:02:20.983207 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-55.bias\n",
            "W0712 07:02:20.985474 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-56.gamma\n",
            "W0712 07:02:20.987745 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-56.beta\n",
            "W0712 07:02:20.989270 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-57.kernel\n",
            "W0712 07:02:20.991442 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-57.bias\n",
            "W0712 07:02:20.992979 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-58.gamma\n",
            "W0712 07:02:20.994581 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-58.beta\n",
            "W0712 07:02:20.996474 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-59.kernel\n",
            "W0712 07:02:20.998080 140019297666944 util.py:244] Unresolved object in checkpoint: (root).optimizer's state 'momentum' for (root).layer_with_weights-59.bias\n",
            "W0712 07:02:20.999433 140019297666944 util.py:252] A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/alpha/guide/checkpoints#loading_mechanics for details.\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}