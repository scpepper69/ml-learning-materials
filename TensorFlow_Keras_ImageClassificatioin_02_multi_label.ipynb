{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow_Keras_ImageClassificatioin_02_multi_label.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scpepper69/ml-learning-materials/blob/master/TensorFlow_Keras_ImageClassificatioin_02_multi_label.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYOnZhT30bGu",
        "colab_type": "text"
      },
      "source": [
        "# AI・機械学習 勉強会 #2\n",
        "## - オリジナル画像による画像分類モデルの構築 -\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRUBscIZP3DA",
        "colab_type": "text"
      },
      "source": [
        "## 目次\n",
        "\n",
        "  \n",
        "2.1.   概要\n",
        "\n",
        "2.2.   実装プロセス\n",
        "\n",
        "1.   画像データの収集\n",
        "2.   環境準備\n",
        "3.   学習に向けたデータの準備\n",
        "4.   モデル構築\n",
        "5.   モデルの学習\n",
        "6.   モデルによる予測\n",
        "7.    特徴の可視化\n",
        "8.   モデルのファイル出力\n",
        "9.   TensorBoardでの確認\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNaQV4x5qxii",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "##2.1 概要\n",
        "「TensorFlow_Keras_ImageClassification_01」をベースとして、自分で収集した画像ファイルを用いて画像分類モデルを構築します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2n0HLjd0bG3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## 2.2 実装プロセス"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVNVVU2o0bG5",
        "colab_type": "text"
      },
      "source": [
        "画像データの準備は本ノートブック上では行えませんので、各自のPCにて実施します。\n",
        "\n",
        "MNISTやCIFAR10などの公開されているデータセットではなく、自ら収集したオリジナルデータセットを用いて学習を行います。\n",
        "\n",
        "オリジナルのデータセットの作成およびそれを用いた学習の参考としてください。\n",
        "\n",
        "データ準備以降のステップについては、「TensorFlow_Keras_ImageClassification_01」と基本的には同じです。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juCkIfgekUSE",
        "colab_type": "text"
      },
      "source": [
        "###2.2.1 画像データの収集\n",
        "\n",
        "まずは、画像分類を行いたい画像を集めましょう。Webからスクレイピングするのも良し、自分で写真を撮って集めるのも良いです。\n",
        "\n",
        "Deep Learningにおいては、データの量が重要だと良く言われます。\n",
        "\n",
        "ただ、大量のデータといっても、品質が伴っていないと意味がありません。\n",
        "\n",
        "たとえば、判断不能なデータをラベリングし、学習させてしまっては、間違いを教えていることと同義になってしまいます。\n",
        "\n",
        "また、データは大量に用意できても、バリエーションに乏しければ、モデルの汎化性能は高くなりません。\n",
        "\n",
        "どのような画像が使えるのか、使えないのか、を知ることも精度の高いモデル構築に必要な知識です。\n",
        "\n",
        "かなり地味な作業となりますが、是非トライしてみてください。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDMe26ba_Bfs",
        "colab_type": "text"
      },
      "source": [
        "スクレイピングの方法はいくらでもありますが、Python使いであれば、下記が使いやすいので紹介しておきます。\n",
        "\n",
        "[Google Image Download](https://google-images-download.readthedocs.io/en/latest/index.html)\n",
        "\n",
        "こちらは[GitHub](https://github.com/hardikvasa/google-images-download)でソースも公開されています。\n",
        "\n",
        "Googleの画像検索から、指定したキーワードの結果を取得してくれます。\n",
        "\n",
        "本ノートブックで使うサンプルもこちらを用いて収集しました。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhfqR_vAk_GO",
        "colab_type": "text"
      },
      "source": [
        "本ノートブックでは、4クラス分類、64 x 64 のカラーの各クラス40枚(合計120枚)のサンプル画像をもとにソースコードを記載しています。\n",
        "\n",
        "各自準備したデータに応じて実装内容を調整してください。\n",
        "\n",
        "※サンプルデータでは、すべてのファイルを64 x 64に調整済みですが、コーディングにてreshapeする形でも問題ありません。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czcjbvDBl82U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 分類クラス数\n",
        "num_classes = 4\n",
        "\n",
        "# クラス毎の画像ファイル数\n",
        "num_images = 40\n",
        "\n",
        "# 画像のサイズ\n",
        "height, width, color = 64, 64, 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfinbX-8rfFr",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.2 環境準備\n",
        "\n",
        "モデル構造は、前回のノートブックと同様のシンプルなCNN、VGG16、RESNETv1/v2を用意しています。\n",
        "\n",
        "自力でデータ収集するとなると、それほど多くの画像ファイルは期待できないと思います。\n",
        "\n",
        "少量のデータの場合に、モデル構造によってどのような違いが出るのか比較するもの良いかと思います。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b5TKT0SrZVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# モデル構造を指定 (CNN, VGG16, RESNET1 or RESNET2)\n",
        "model_opt=\"VGG16\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZaJgfxvyLAy",
        "colab_type": "text"
      },
      "source": [
        "Google Colabratoryは、ランタイムが初期化されるとデータも失われます。\n",
        "\n",
        "学習した中のチェックポイントが学習済みモデルを再利用できるよう、Google Driveをマウントし、ここに出力できるようしておきましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf6gYTI1xt0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, shutil\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "gdrive_base='/content/drive/My Drive/Colab Notebooks/'\n",
        "\n",
        "# TensorBorad用ログ\n",
        "log_dir=gdrive_base+'ImageClassification/logs/'\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "# チェックポイントおよび学習済みモデルファイル\n",
        "model_dir=gdrive_base+'ImageClassification/model/'\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFKVp80t0bG8",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.3 学習に向けたデータの準備\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94oYsnmpjFNI",
        "colab_type": "text"
      },
      "source": [
        "収集した画像データをアップロードし、学習に使えるデータに変換していきます。\n",
        "\n",
        "全データを格納するための空のテンソルを準備します。\n",
        "\n",
        "テンソルは以下の５要素になります。\n",
        "\n",
        "- クラス番号(0～)\n",
        "- クラスごとのファイル番号(0～)\n",
        "- 画像のHeight\n",
        "- 画像のWidth\n",
        "- 画像のRBG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0t_rwbYO9kT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "from glob import glob\n",
        "\n",
        "# 空のテンソルを用意\n",
        "ary = np.zeros([num_classes, num_images, height, width, color], dtype=np.int)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zplsWRSw_xAV",
        "colab_type": "text"
      },
      "source": [
        "画像ファイルをGoogle Colabにアップロードし、１枚ずつ読み込み、テンソルに格納していきます。\n",
        "\n",
        "サンプルデータでは、ファイル名にてクラスを判別し、テンソルの１要素目(=クラス)を指定し、データを格納させています。\n",
        "\n",
        "Numpyには、テンソルデータを保存させておく機能があります。\n",
        "\n",
        "savez_compressed関数を使用し、作成したテンソルデータを再利用可能なようにファイル出力しておきましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gERS_nJSQVRR",
        "colab_type": "code",
        "outputId": "765f5064-c5d9-4476-a7b3-58a4b4019062",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# 学習データのアップロード\n",
        "# ここではサンプルデータをGitHubから取得していますが、適宜zip形式などでGoogle Colabにアップロードしてください。\n",
        "!wget -nc https://raw.githubusercontent.com/scpepper69/ml-learning-materials/master/sample/gface64x64.zip\n",
        "!unzip -oq gface64x64.zip\n",
        "\n",
        "dir_name='gface64x64'\n",
        "\n",
        "c0=0 # rx-178:mk2\n",
        "c1=0 # msz-006:Z\n",
        "c2=0 # rx-93:Nu\n",
        "c3=0 # ms-06:Zaku\n",
        "\n",
        "# 画像を順次読み込み、テンソルデータに変換\n",
        "for file in glob(dir_name + '/*.jpg'):\n",
        "    img = cv2.imread(file,cv2.IMREAD_COLOR)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    if 'rx-178' in file:\n",
        "        ary[0, c0] = img\n",
        "        c0 += 1\n",
        "    elif 'msz-006' in file:\n",
        "        ary[1, c1] = img\n",
        "        c1 += 1\n",
        "    elif 'rx-93' in file:\n",
        "        ary[2, c2] = img\n",
        "        c2 += 1\n",
        "    elif 'ms-06' in file:\n",
        "        ary[3, c3] = img\n",
        "        c3 += 1\n",
        "\n",
        "np.savez_compressed('gface_images.npz', ary)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-24 07:28:06--  https://raw.githubusercontent.com/scpepper69/ml-learning-materials/master/sample/gface64x64.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 504181 (492K) [application/zip]\n",
            "Saving to: ‘gface64x64.zip’\n",
            "\n",
            "\rgface64x64.zip        0%[                    ]       0  --.-KB/s               \rgface64x64.zip      100%[===================>] 492.36K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-07-24 07:28:07 (3.91 MB/s) - ‘gface64x64.zip’ saved [504181/504181]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySt5HvoHBVAn",
        "colab_type": "text"
      },
      "source": [
        "ここまでで、収集データを１つのテンソルに纏めることができました。\n",
        "\n",
        "次に、このテンソルをもとに、画像データ用テンソルと、ラベル用テンソルを作成します。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGkHthXETusA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#保存したnpzファイルはnp.loadにて読み込むことができます\n",
        "#ary = np.load(\"gface_images.npz\")['arr_0']\n",
        "\n",
        "# 画像データのテンソルをソートし、ラベル用テンソルを用意\n",
        "X_train = np.zeros([num_classes * num_images, height, width, color], dtype=np.int)\n",
        "for i in range(num_classes):\n",
        "    for j in range(num_images):\n",
        "        X_train[(i * num_images) + j] = ary[i][j]\n",
        "\n",
        "# X_trainはクラス番号でソートされて格納されているので、下記だけでラベルデータが生成できる\n",
        "Y_train = np.repeat(np.arange(num_classes), num_images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SnbE2e-DBZH",
        "colab_type": "text"
      },
      "source": [
        "Deep Learningには、学習データと検証データの２種類のデータが必要です。\n",
        "\n",
        "sklearnには、データを指定の割合で分割してくれる関数があります。これを利用して学習データと検証データに分割します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70olgK22Cw8g",
        "colab_type": "code",
        "outputId": "c70b2633-0e6d-46b7-8c91-26e675d2d968",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 検証データの割合を指定\n",
        "validate_rate=0.2\n",
        "\n",
        "# 学習データと検証データに分割\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=validate_rate)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 64, 64, 3)\n",
            "(32, 64, 64, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2-51a7GmiIm",
        "colab_type": "text"
      },
      "source": [
        "今回はもう一つ、ラベルを追加し、1つのインプット画像から、2つアウトプット(ラベル)を出力するネットワークとしてみます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nztymPcumerE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "\n",
        "# 全体のラベルから複製して構成\n",
        "def add_label(y):\n",
        "    y2 = copy.deepcopy(y)\n",
        "    for i in range(len(y)):\n",
        "        # Gundam\n",
        "        if y2[i] in [0,1,2]:\n",
        "            y2[i] = 0\n",
        "        # Zeon\n",
        "        else:\n",
        "            y2[i] = 1\n",
        "    return y2\n",
        "\n",
        "num_classes2 = 2\n",
        "y_train2 = add_label(y_train)\n",
        "y_test2 = add_label(y_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3pSTYqA0bHI",
        "colab_type": "text"
      },
      "source": [
        "学習用画像データと画像に対応したラベルを表示してみます。\n",
        "\n",
        "画像表示には、matplotlibライブラリを用います。これはPythonにてグラフ表示によく使われるライブラリなので、使用方法は覚えておくと良いです。\n",
        "\n",
        "参考：https://matplotlib.org/api/pyplot_api.html\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-XaTkOc0bHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#jupyter notebook用マジックコマンド\n",
        "%matplotlib inline \n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(9, 15))\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=0.5, hspace=0.05, wspace=0.05)\n",
        "\n",
        "# 各MNIST画像の上に（タイトルとして）対応するラベルを表示\n",
        "for i in range(10):\n",
        "    ax = fig.add_subplot(1, 10, i + 1, xticks=[], yticks=[])\n",
        "    ax.set_title(str(y_train[i]))\n",
        "    ax.imshow(x_train[i], cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZCxZxgl0bHX",
        "colab_type": "text"
      },
      "source": [
        "ラベルデータをone-hot表現に変換します。\n",
        "\n",
        "keras.utils.to_categorical関数を使いましょう。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBvnPZCU0bHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "# ラベルデータをone-hot表現へ変換\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n",
        "# 追加ラベルも同じく変換\n",
        "y_train2 = to_categorical(y_train2, num_classes2)\n",
        "y_test2 = to_categorical(y_test2, num_classes2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfE9xtc-0Mgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 画像データの型を変換\n",
        "x_train = x_train.reshape(-1, height, width, color).astype(np.float32)\n",
        "x_test = x_test.reshape(-1, height, width, color).astype(np.float32)\n",
        "input_shape = (height, width, color)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUUkN6qj0bHi",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.4 モデル構築\n",
        "\n",
        "これから、学習モデルを構築します。\n",
        "\n",
        "モデルの構築は、モデルの「容器」としてSquential()を実施したのち、その中に順にレイヤーを追加していく流れになります。\n",
        "\n",
        "* [Sequential](https://keras.io/ja/models/sequential/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rIGYTTK0bHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# モデル構築用ライブラリをインポート\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
        "\n",
        "# CNNモデル用ライブラリ\n",
        "from tensorflow.python.keras.layers import Conv2D, Convolution2D, MaxPooling2D, Dropout, BatchNormalization, GlobalAveragePooling2D,AveragePooling2D,Input\n",
        "from tensorflow.python.keras import initializers\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.regularizers import l2\n",
        "\n",
        "# モデルの「容器」を作成\n",
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWgvceiHbKnb",
        "colab_type": "text"
      },
      "source": [
        "今回は、シンプルなCNNモデルの他、VGG16、RESNETの関数を準備してあります。\n",
        "\n",
        "- VGG16\n",
        "\n",
        "    Oxford大学の研究グループが提案し2014年のILSVRで2位を獲得したモデルで、畳み込みが13層、全結合層が3層の合計16層からなるニューラルネットワークです。\n",
        "    Kerasでは関数として組み込まれており、容易に実装することが可能になっています。\n",
        "    \n",
        "- ResNet\n",
        "\n",
        "    ResNetは2015年にMicrosoftより発表された152層からなるニューラルネットワークです。今まで20層ほどで作られていたCNNを特別なユニットを挟むことで深くすることを可能にしています。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt0Xe1505Quo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# VGG16モデル\n",
        "def cnn_vgg16():\n",
        "\n",
        "    input_tensor = Input(shape=input_shape)\n",
        "    vgg16 = VGG16(include_top=False, weights='imagenet', input_tensor=input_tensor)    \n",
        "\n",
        "    x = vgg16.output\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # original labels, 0 to 9, as output1\n",
        "    output1 = Dense(num_classes, activation='softmax', name='output1')(x)\n",
        "    # second labels, 0 or 1, as output2\n",
        "    output2 = Dense(num_classes2, activation='softmax', name='output2')(x)\n",
        "    \n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=input_tensor, outputs=[output1, output2])\n",
        "\n",
        "    #fix weights VGG16 layers\n",
        "    for layer in vgg16.layers:\n",
        "        layer.trainable = False\n",
        "        \n",
        "    return model    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLLXeRM3f7Fi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CNNモデル w/Batch Normalization\n",
        "def cnn_w_batchnorm():\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = inputs\n",
        "\n",
        "    x = Conv2D(32, (3, 3), kernel_initializer=initializers.TruncatedNormal(stddev=0.1), bias_initializer=initializers.Zeros())(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(32, (3, 3), kernel_initializer=initializers.TruncatedNormal(stddev=0.1), bias_initializer=initializers.Zeros())(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "               \n",
        "    x = Conv2D(64, (3, 3), kernel_initializer=initializers.TruncatedNormal(stddev=0.1), bias_initializer=initializers.Zeros())(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Conv2D(64, (3, 3), kernel_initializer=initializers.TruncatedNormal(stddev=0.1), bias_initializer=initializers.Zeros())(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # original labels, 0 to 9, as output1\n",
        "    output1 = Dense(num_classes, activation='softmax', name='output1')(x)\n",
        "    # second labels, 0 or 1, as output2\n",
        "    output2 = Dense(num_classes2, activation='softmax', name='output2')(x)\n",
        "    \n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=[output1, output2])\n",
        "    return model    \n",
        "     "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGWk7WWk5Ya3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ResNetモデル from Keras Documentation\n",
        "\n",
        "# Model parameter\n",
        "# ----------------------------------------------------------------------------\n",
        "#           |      | 200-epoch | Orig Paper| 200-epoch | Orig Paper| sec/epoch\n",
        "# Model     |  n   | ResNet v1 | ResNet v1 | ResNet v2 | ResNet v2 | GTX1080Ti\n",
        "#           |v1(v2)| %Accuracy | %Accuracy | %Accuracy | %Accuracy | v1 (v2)\n",
        "# ----------------------------------------------------------------------------\n",
        "# ResNet20  | 3 (2)| 92.16     | 91.25     | -----     | -----     | 35 (---)\n",
        "# ResNet32  | 5(NA)| 92.46     | 92.49     | NA        | NA        | 50 ( NA)\n",
        "# ResNet44  | 7(NA)| 92.50     | 92.83     | NA        | NA        | 70 ( NA)\n",
        "# ResNet56  | 9 (6)| 92.71     | 93.03     | 93.01     | NA        | 90 (100)\n",
        "# ResNet110 |18(12)| 92.65     | 93.39+-.16| 93.15     | 93.63     | 165(180)\n",
        "# ResNet164 |27(18)| -----     | 94.07     | -----     | 94.54     | ---(---)\n",
        "# ResNet1001| (111)| -----     | 92.39     | -----     | 95.08+-.14| ---(---)\n",
        "# ---------------------------------------------------------------------------\n",
        "n = 3\n",
        "\n",
        "# Model version\n",
        "# Orig paper: version = 1 (ResNet v1), Improved ResNet: version = 2 (ResNet v2)\n",
        "version = 1\n",
        "\n",
        "# Computed depth from supplied model parameter n\n",
        "#if version == 1:\n",
        "#    depth = n * 6 + 2\n",
        "#elif version == 2:\n",
        "#    depth = n * 9 + 2\n",
        "\n",
        "# ResNetモデル\n",
        "def resnet_layer(inputs,\n",
        "                 num_filters=16,\n",
        "                 kernel_size=3,\n",
        "                 strides=1,\n",
        "                 activation='relu',\n",
        "                 batch_normalization=True,\n",
        "                 conv_first=True):\n",
        "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
        "\n",
        "    # Arguments\n",
        "        inputs (tensor): input tensor from input image or previous layer\n",
        "        num_filters (int): Conv2D number of filters\n",
        "        kernel_size (int): Conv2D square kernel dimensions\n",
        "        strides (int): Conv2D square stride dimensions\n",
        "        activation (string): activation name\n",
        "        batch_normalization (bool): whether to include batch normalization\n",
        "        conv_first (bool): conv-bn-activation (True) or\n",
        "            bn-activation-conv (False)\n",
        "\n",
        "    # Returns\n",
        "        x (tensor): tensor as input to the next layer\n",
        "    \"\"\"\n",
        "    conv = Conv2D(num_filters,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same',\n",
        "                  kernel_initializer='he_normal',\n",
        "                  kernel_regularizer=l2(1e-4))\n",
        "\n",
        "    x = inputs\n",
        "    if conv_first:\n",
        "        x = conv(x)\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "    else:\n",
        "        if batch_normalization:\n",
        "            x = BatchNormalization()(x)\n",
        "        if activation is not None:\n",
        "            x = Activation(activation)(x)\n",
        "        x = conv(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def resnet_v1(input_shape, depth=20, num_classes=num_classes):\n",
        "    \"\"\"ResNet Version 1 Model builder [a]\n",
        "\n",
        "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
        "    Last ReLU is after the shortcut connection.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filters is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same number of filters.\n",
        "    Features maps sizes:\n",
        "    stage 0: 32x32, 16\n",
        "    stage 1: 16x16, 32\n",
        "    stage 2:  8x8,  64\n",
        "    The Number of parameters is approx the same as Table 6 of [a]:\n",
        "    ResNet20 0.27M\n",
        "    ResNet32 0.46M\n",
        "    ResNet44 0.66M\n",
        "    ResNet56 0.85M\n",
        "    ResNet110 1.7M\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 6 != 0:\n",
        "        raise ValueError('depth should be 6n+2 (eg 20, 32, 44 in [a])')\n",
        "    # Start model definition.\n",
        "    num_filters = 16\n",
        "    num_res_blocks = int((depth - 2) / 6)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = resnet_layer(inputs=inputs)\n",
        "    # Instantiate the stack of residual units\n",
        "    for stack in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            strides = 1\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                strides = 2  # downsample\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters,\n",
        "                             strides=strides)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters,\n",
        "                             activation=None)\n",
        "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "            x = Activation('relu')(x)\n",
        "        num_filters *= 2\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v1 does not use BN after last shortcut connection-ReLU\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "#    y = Flatten()(x)\n",
        "#    outputs = Dense(num_classes,\n",
        "#                    activation='softmax',\n",
        "#                    kernel_initializer='he_normal')(y)\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "#    x = Dropout(0.5)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # original labels, 0 to 9, as output1\n",
        "    output1 = Dense(num_classes, activation='softmax', name='output1')(x)\n",
        "    # second labels, 0 or 1, as output2\n",
        "    output2 = Dense(num_classes2, activation='softmax', name='output2')(x)\n",
        "    \n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=[output1, output2])\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def resnet_v2(input_shape, depth=29, num_classes=num_classes):\n",
        "    \"\"\"ResNet Version 2 Model builder [b]\n",
        "\n",
        "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
        "    bottleneck layer\n",
        "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
        "    Second and onwards shortcut connection is identity.\n",
        "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
        "    by a convolutional layer with strides=2, while the number of filter maps is\n",
        "    doubled. Within each stage, the layers have the same number filters and the\n",
        "    same filter map sizes.\n",
        "    Features maps sizes:\n",
        "    conv1  : 32x32,  16\n",
        "    stage 0: 32x32,  64\n",
        "    stage 1: 16x16, 128\n",
        "    stage 2:  8x8,  256\n",
        "\n",
        "    # Arguments\n",
        "        input_shape (tensor): shape of input image tensor\n",
        "        depth (int): number of core convolutional layers\n",
        "        num_classes (int): number of classes (CIFAR10 has 10)\n",
        "\n",
        "    # Returns\n",
        "        model (Model): Keras model instance\n",
        "    \"\"\"\n",
        "    if (depth - 2) % 9 != 0:\n",
        "        raise ValueError('depth should be 9n+2 (eg 56 or 110 in [b])')\n",
        "    # Start model definition.\n",
        "    num_filters_in = 16\n",
        "    num_res_blocks = int((depth - 2) / 9)\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    # v2 performs Conv2D with BN-ReLU on input before splitting into 2 paths\n",
        "    x = resnet_layer(inputs=inputs,\n",
        "                     num_filters=num_filters_in,\n",
        "                     conv_first=True)\n",
        "\n",
        "    # Instantiate the stack of residual units\n",
        "    for stage in range(3):\n",
        "        for res_block in range(num_res_blocks):\n",
        "            activation = 'relu'\n",
        "            batch_normalization = True\n",
        "            strides = 1\n",
        "            if stage == 0:\n",
        "                num_filters_out = num_filters_in * 4\n",
        "                if res_block == 0:  # first layer and first stage\n",
        "                    activation = None\n",
        "                    batch_normalization = False\n",
        "            else:\n",
        "                num_filters_out = num_filters_in * 2\n",
        "                if res_block == 0:  # first layer but not first stage\n",
        "                    strides = 2    # downsample\n",
        "\n",
        "            # bottleneck residual unit\n",
        "            y = resnet_layer(inputs=x,\n",
        "                             num_filters=num_filters_in,\n",
        "                             kernel_size=1,\n",
        "                             strides=strides,\n",
        "                             activation=activation,\n",
        "                             batch_normalization=batch_normalization,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_in,\n",
        "                             conv_first=False)\n",
        "            y = resnet_layer(inputs=y,\n",
        "                             num_filters=num_filters_out,\n",
        "                             kernel_size=1,\n",
        "                             conv_first=False)\n",
        "            if res_block == 0:\n",
        "                # linear projection residual shortcut connection to match\n",
        "                # changed dims\n",
        "                x = resnet_layer(inputs=x,\n",
        "                                 num_filters=num_filters_out,\n",
        "                                 kernel_size=1,\n",
        "                                 strides=strides,\n",
        "                                 activation=None,\n",
        "                                 batch_normalization=False)\n",
        "            x = keras.layers.add([x, y])\n",
        "\n",
        "        num_filters_in = num_filters_out\n",
        "\n",
        "    # Add classifier on top.\n",
        "    # v2 has BN-ReLU before Pooling\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = AveragePooling2D(pool_size=8)(x)\n",
        "#    y = Flatten()(x)\n",
        "#    outputs = Dense(num_classes,\n",
        "#                    activation='softmax',\n",
        "#                    kernel_initializer='he_normal')(y)\n",
        "    \n",
        "    x = Flatten()(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "#    x = Dropout(0.5)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    # original labels, 0 to 9, as output1\n",
        "    output1 = Dense(num_classes, activation='softmax', name='output1')(x)\n",
        "    # second labels, 0 or 1, as output2\n",
        "    output2 = Dense(num_classes2, activation='softmax', name='output2')(x)\n",
        "    \n",
        "    # Instantiate model.\n",
        "    model = Model(inputs=inputs, outputs=[output1, output2])\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DNVsEupdMjn",
        "colab_type": "text"
      },
      "source": [
        "それでは、任意のモデルを選び、構築してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nq7BxPyz0bH3",
        "colab_type": "code",
        "outputId": "c231d9c0-64ac-4469-fd54-1823cc77f145",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# model構築\n",
        "if model_opt==\"VGG16\":\n",
        "    model = cnn_vgg16()\n",
        "elif model_opt==\"RESNET1\":\n",
        "    model = resnet_v1(input_shape=input_shape)\n",
        "elif model_opt==\"RESNET2\":\n",
        "    model = resnet_v2(input_shape=input_shape)\n",
        "else:\n",
        "    model=cnn_w_batchnorm()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0724 07:28:13.567831 140146061514624 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 6s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5Dz7x4Scrf8",
        "colab_type": "text"
      },
      "source": [
        "学習の開始にあたり、最後にcompile関数でコンパイルを行います。\n",
        "\n",
        "compile関数でも、学習にあたって以下のパラメータを指定する必要があります。\n",
        "\n",
        "* optimizer（最適化手法）\n",
        "* loss（損失関数）\n",
        "* metrics（評価関数（任意））\n",
        "\n",
        "以下サンプルコードでは、現在Kerasで利用できるoptimizerを並べてみました。こちらもそれぞれどのような結果になるか比較してみるといいでしょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI4UDcU60bH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import losses\n",
        "# モデルの学習方法について指定しておく\n",
        "model.compile(loss={'output1': 'categorical_crossentropy','output2': 'categorical_crossentropy'}, optimizer=optimizers.SGD(lr=0.001, momentum=0.9), metrics=['accuracy'])\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adam(lr=0.001), metrics=['accuracy'])\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=optimizers.RMSprop(lr=0.001), metrics=['accuracy'])\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adagrad(lr=0.001), metrics=['accuracy'])\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adadelta(lr=0.001), metrics=['accuracy'])\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=optimizers.Adamax(lr=0.001), metrics=['accuracy'])\n",
        "#model.compile(loss='categorical_crossentropy', optimizer=optimizers.Nadam(lr=0.001), metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZwzQE-FYxdR",
        "colab_type": "text"
      },
      "source": [
        "Optimizerアルゴリズムの動作イメージ(作者：Alec Radford)\n",
        "\n",
        "\n",
        "<img src=\"http://sebastianruder.com/content/images/2016/01/contours_evaluation_optimizers.gif\" width=\"400\">\n",
        "<img src=\"http://sebastianruder.com/content/images/2016/01/saddle_point_evaluation_optimizers.gif\" width=\"400\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LYqpAQPO04I",
        "colab_type": "text"
      },
      "source": [
        "早速学習開始と行きたいところですが、学習の途中結果を記録する(Checkpoint)ことと、TensorBoard向けのログ情報を取得できるようにしておきましょう。\n",
        "\n",
        "モデル構造や結果等の視覚化することで、何がどのように動いているのか、理解の手助けとなります。\n",
        "\n",
        "このjupyter notebook上でも可視化ロジックを組み込んでいますが、通常のモデル開発においては、TensorBoardを用いるほうがスマートでしょう。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weO_9YIVJ4dA",
        "colab_type": "text"
      },
      "source": [
        "TensorBoardでも参照できるよう、設定を組み込んでおきます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s80FRVRL0bH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TensorBoardでの可視化のため、出力先の設定\n",
        "from tensorflow.keras import callbacks\n",
        "\n",
        "tb_cb = callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1,write_images=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJVjFnA8h5LT",
        "colab_type": "text"
      },
      "source": [
        "チェックポイントを生成しておくことで、中断した学習の再開や、チェックポイントを利用した静的モデルの出力を行うことができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rk3ohoKJhd9P",
        "colab_type": "code",
        "outputId": "fa5e8f04-5640-431d-f7d5-81973ec438da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# チェックポイント出力先\n",
        "RUN = RUN + 1 if 'RUN' in locals() else 1\n",
        "checkpoint_path = model_dir + f'run{RUN}/' + model_opt + \"_cp-{epoch:04d}.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# チェックポイントコールバックを作る\n",
        "cp_cb = callbacks.ModelCheckpoint(checkpoint_path, save_weights_only=True, verbose=1, period=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0724 07:28:24.864158 140146061514624 callbacks.py:875] `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMWlqNgvPBvp",
        "colab_type": "text"
      },
      "source": [
        "学習の実行の前に、モデルのサマリ情報を確認してみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "135NNq6iO-b2",
        "colab_type": "code",
        "outputId": "b25a38e4-d0e6-4df7-90b7-26351c1866c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        }
      },
      "source": [
        "# モデルのサマリ情報の表示\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 64, 64, 64)   1792        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 64, 64, 64)   36928       block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_pool (MaxPooling2D)      (None, 32, 32, 64)   0           block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv1 (Conv2D)           (None, 32, 32, 128)  73856       block1_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv2 (Conv2D)           (None, 32, 32, 128)  147584      block2_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 16, 16, 128)  0           block2_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv1 (Conv2D)           (None, 16, 16, 256)  295168      block2_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv2 (Conv2D)           (None, 16, 16, 256)  590080      block3_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv3 (Conv2D)           (None, 16, 16, 256)  590080      block3_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 8, 8, 256)    0           block3_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv1 (Conv2D)           (None, 8, 8, 512)    1180160     block3_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv2 (Conv2D)           (None, 8, 8, 512)    2359808     block4_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv3 (Conv2D)           (None, 8, 8, 512)    2359808     block4_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 4, 4, 512)    0           block4_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv1 (Conv2D)           (None, 4, 4, 512)    2359808     block4_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv2 (Conv2D)           (None, 4, 4, 512)    2359808     block5_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv3 (Conv2D)           (None, 4, 4, 512)    2359808     block5_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_pool (MaxPooling2D)      (None, 2, 2, 512)    0           block5_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 2048)         0           block5_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          1049088     flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 512)          2048        dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "output1 (Dense)                 (None, 4)            2052        batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "output2 (Dense)                 (None, 2)            1026        batch_normalization[0][0]        \n",
            "==================================================================================================\n",
            "Total params: 15,768,902\n",
            "Trainable params: 1,053,190\n",
            "Non-trainable params: 14,715,712\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUmiS87vdonR",
        "colab_type": "text"
      },
      "source": [
        "学習途上のチェックポイントから再開させることもできます。\n",
        "\n",
        "load_weights関数にてチェックポイントファイルから重みをロードします。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dAQ_PWeidls-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# チェックポイントから学習済みパラメータを復元\n",
        "#model.load_weights(f'{model_dir}run1/{model_structure}_{data_set}_cp-0010.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_yBnQiN0bID",
        "colab_type": "text"
      },
      "source": [
        "### 1.2.4 モデルの学習\n",
        "\n",
        "ようやく学習に入ります。今回は少ない画像データを水増しして学習させる方法を使います。\n",
        "\n",
        "水増しとは聞こえが悪いですが、画像データに若干の加工を行い、データのバリエーションを増やすことが目的です。\n",
        "\n",
        "この場合、学習処理にはfit_generator関数を使います。\n",
        "\n",
        "Kerasに実装されているImageDataGenerator関数を用いる他、自分でgeneratorを実装することもできます。\n",
        "\n",
        "ImageDataGeneratorでは以下のようなパラメータを使用することができます。\n",
        "\n",
        "- rotation_range：画像を指定のレンジの幅で傾ける\n",
        "- zoom_range：画像の指定のレンジの幅で拡大する\n",
        "- height_shift_range：画像を指定のレンジの幅で縦にスライドする\n",
        "- width_shift_range：画像を指定のレンジの幅で横にスライドする\n",
        "\n",
        "今回は、自作のgenerator関数を用意しました。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI775x1593fK",
        "colab_type": "code",
        "outputId": "5f4aca83-9da9-40c1-9fd0-3a0528335196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.preprocessing import image\n",
        "from random import shuffle\n",
        "from scipy import ndimage\n",
        "\n",
        "def generator(x, y1, y2, train):\n",
        "    batch_size = 16\n",
        "\n",
        "    while True:\n",
        "        if train:\n",
        "            keys = list(range(len(x)))\n",
        "            shuffle(keys)\n",
        "        else:\n",
        "            keys = list(range(len(y1)))\n",
        "            shuffle(keys)\n",
        "        inputs = []\n",
        "        label1 = []\n",
        "        label2 = []\n",
        "\n",
        "        for key in keys:\n",
        "            img = x[key]\n",
        "            if train:\n",
        "                # 画像の回転\n",
        "                rotate_rate = np.random.normal(0,0.5)*10\n",
        "                img = ndimage.rotate(x[key], rotate_rate)\n",
        "                img = cv2.resize(img,(height,width))\n",
        "                # 画像のぼかし\n",
        "                if np.random.randint(0,2):\n",
        "                    filter_rate = np.random.randint(0,6)\n",
        "                    img = ndimage.gaussian_filter(img, sigma=filter_rate)\n",
        "\n",
        "            inputs.append(img)\n",
        "            label1.append(y1[key])\n",
        "            label2.append(y2[key])\n",
        "\n",
        "            if len(inputs) == batch_size:\n",
        "                tmp_inputs = np.array(inputs)\n",
        "                tmp_label1 = np.array(label1)\n",
        "                tmp_label2 = np.array(label2)\n",
        "                inputs = []\n",
        "                label1 = []\n",
        "                label2 = []\n",
        "                yield tmp_inputs, {'output1': tmp_label1, 'output2': tmp_label2}\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "_IQUEb790bIE",
        "colab_type": "code",
        "outputId": "cdac2d0f-afa7-4aa4-f955-51f2e06a4825",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "# epoch数を指定\n",
        "epochs=10\n",
        "\n",
        "# batchサイズを指定\n",
        "batch_size=500\n",
        "\n",
        "# 学習の実行(fit)\n",
        "#result = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1,callbacks=[tb_cb, cp_cb], validation_data=(x_test, y_test))\n",
        "\n",
        "    \n",
        "# 学習の実行 (fit_generator)\n",
        "result = model.fit_generator(generator(x_train, y_train, y_train2, True),\n",
        "                             steps_per_epoch=x_train.shape[0], \n",
        "                             epochs=epochs, \n",
        "                             validation_data=generator(x_test, y_test, y_test2, False), \n",
        "                             validation_steps=2, \n",
        "                             verbose=1,\n",
        "                             callbacks=[tb_cb])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "  1/128 [..............................] - ETA: 8:33 - loss: 4.0396 - output1_loss: 2.6773 - output2_loss: 1.3623 - output1_acc: 0.2500 - output2_acc: 0.3125"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0724 07:28:30.802888 140146061514624 callbacks.py:257] Method (on_train_batch_end) is slow compared to the batch update (0.460649). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  2/128 [..............................] - ETA: 5:00 - loss: 3.3163 - output1_loss: 2.2780 - output2_loss: 1.0383 - output1_acc: 0.2500 - output2_acc: 0.4688"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0724 07:28:30.854873 140146061514624 callbacks.py:257] Method (on_train_batch_end) is slow compared to the batch update (0.247199). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r  3/128 [..............................] - ETA: 3:20 - loss: 3.0883 - output1_loss: 2.1299 - output2_loss: 0.9584 - output1_acc: 0.2083 - output2_acc: 0.4583"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0724 07:28:30.901086 140146061514624 callbacks.py:257] Method (on_train_batch_end) is slow compared to the batch update (0.123623). Check your callbacks.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "128/128 [==============================] - 16s 123ms/step - loss: 0.7872 - output1_loss: 0.5474 - output2_loss: 0.2398 - output1_acc: 0.7861 - output2_acc: 0.8950 - val_loss: 0.5204 - val_output1_loss: 0.4567 - val_output2_loss: 0.0637 - val_output1_acc: 0.8750 - val_output2_acc: 0.9688\n",
            "Epoch 2/10\n",
            "128/128 [==============================] - 11s 84ms/step - loss: 0.4144 - output1_loss: 0.2820 - output2_loss: 0.1324 - output1_acc: 0.8984 - output2_acc: 0.9497 - val_loss: 0.7119 - val_output1_loss: 0.6238 - val_output2_loss: 0.0880 - val_output1_acc: 0.8750 - val_output2_acc: 1.0000\n",
            "Epoch 3/10\n",
            "128/128 [==============================] - 11s 84ms/step - loss: 0.2993 - output1_loss: 0.2034 - output2_loss: 0.0959 - output1_acc: 0.9351 - output2_acc: 0.9697 - val_loss: 0.5883 - val_output1_loss: 0.4708 - val_output2_loss: 0.1175 - val_output1_acc: 0.8750 - val_output2_acc: 0.9688\n",
            "Epoch 4/10\n",
            "128/128 [==============================] - 11s 83ms/step - loss: 0.2618 - output1_loss: 0.1901 - output2_loss: 0.0717 - output1_acc: 0.9390 - output2_acc: 0.9785 - val_loss: 0.7377 - val_output1_loss: 0.6915 - val_output2_loss: 0.0462 - val_output1_acc: 0.8750 - val_output2_acc: 0.9688\n",
            "Epoch 5/10\n",
            "128/128 [==============================] - 11s 84ms/step - loss: 0.2359 - output1_loss: 0.1670 - output2_loss: 0.0688 - output1_acc: 0.9409 - output2_acc: 0.9790 - val_loss: 0.4588 - val_output1_loss: 0.4150 - val_output2_loss: 0.0438 - val_output1_acc: 0.8750 - val_output2_acc: 1.0000\n",
            "Epoch 6/10\n",
            "128/128 [==============================] - 11s 84ms/step - loss: 0.1876 - output1_loss: 0.1374 - output2_loss: 0.0502 - output1_acc: 0.9546 - output2_acc: 0.9849 - val_loss: 0.6175 - val_output1_loss: 0.5088 - val_output2_loss: 0.1087 - val_output1_acc: 0.9062 - val_output2_acc: 0.9688\n",
            "Epoch 7/10\n",
            "128/128 [==============================] - 11s 84ms/step - loss: 0.2013 - output1_loss: 0.1439 - output2_loss: 0.0574 - output1_acc: 0.9473 - output2_acc: 0.9795 - val_loss: 0.8816 - val_output1_loss: 0.5344 - val_output2_loss: 0.3472 - val_output1_acc: 0.9375 - val_output2_acc: 0.9688\n",
            "Epoch 8/10\n",
            "128/128 [==============================] - 11s 83ms/step - loss: 0.1532 - output1_loss: 0.1095 - output2_loss: 0.0437 - output1_acc: 0.9639 - output2_acc: 0.9878 - val_loss: 0.7523 - val_output1_loss: 0.7068 - val_output2_loss: 0.0454 - val_output1_acc: 0.8438 - val_output2_acc: 1.0000\n",
            "Epoch 9/10\n",
            "128/128 [==============================] - 11s 83ms/step - loss: 0.1570 - output1_loss: 0.1200 - output2_loss: 0.0369 - output1_acc: 0.9604 - output2_acc: 0.9873 - val_loss: 0.5651 - val_output1_loss: 0.4890 - val_output2_loss: 0.0761 - val_output1_acc: 0.9062 - val_output2_acc: 0.9688\n",
            "Epoch 10/10\n",
            "128/128 [==============================] - 11s 83ms/step - loss: 0.1460 - output1_loss: 0.1035 - output2_loss: 0.0425 - output1_acc: 0.9663 - output2_acc: 0.9839 - val_loss: 0.4584 - val_output1_loss: 0.4043 - val_output2_loss: 0.0541 - val_output1_acc: 0.9062 - val_output2_acc: 1.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCLIozIF0bIJ",
        "colab_type": "text"
      },
      "source": [
        "学習結果の評価については、evaluate関数にて得ることができます。\n",
        "\n",
        "主な引数は次の通りです。\n",
        "\n",
        "* x：評価に使用する入力データ\n",
        "* y：評価に使用する出力データ\n",
        "* batch_size：1回の評価を行うにあたって用いるサンプル数\n",
        "* verbose：評価のログを出力するか（0:しない、1：する(デフォルト)）\n",
        "\n",
        "基本的には、損失(Loss)は低ければ低いほうが、評価(Accuracy)は高ければ高いほうが良いです。\n",
        "\n",
        "Accuracyはモデルの精度そのもの、Lossは学習が効率よく行われているかを示す指標で、Lossが高いまま収束していかない＝効率が良くなく、モデル構造やパラメータ改善の余地あり、という感覚でよいかと思います。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ys0GtCR60bIK",
        "colab_type": "code",
        "outputId": "456098eb-ed2c-4296-c73d-914df7e93f4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#score = model.evaluate(x_test, y_test, verbose=0)\n",
        "score = model.evaluate(x_test, {'output1': y_test,'output2': y_test2}, verbose=0)\n",
        "print('Label#1 Test loss:', score[1])\n",
        "print('Label#1 Test accuracy:', score[3])\n",
        "\n",
        "print('Label#2 Test loss:', score[2])\n",
        "print('Label#2 Test accuracy:', score[4])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label#1 Test loss: 0.5269705\n",
            "Label#1 Test accuracy: 0.90625\n",
            "Label#2 Test loss: 0.03419111\n",
            "Label#2 Test accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvQIQuhhWdOo",
        "colab_type": "text"
      },
      "source": [
        "epochごとのAccuracyおよびLossの遷移をグラフ化してみましょう。\n",
        "\n",
        "fit関数のreturnから、データを取得することができます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UB8isRg_hfGT",
        "colab_type": "code",
        "outputId": "5ed67804-f259-4565-f985-d1bcc266cd25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "source": [
        "result.history.keys() # ヒストリデータのラベルを見てみる\n",
        "\n",
        "plt.plot(range(1, epochs+1), result.history['output1_acc'], label=\"training#1\")\n",
        "plt.plot(range(1, epochs+1), result.history['output2_acc'], label=\"training#2\")\n",
        "plt.plot(range(1, epochs+1), result.history['val_output1_acc'], label=\"validation#1\")\n",
        "plt.plot(range(1, epochs+1), result.history['val_output2_acc'], label=\"validation#2\")\n",
        "plt.title('Accuracy History')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.xlim([1,epochs])\n",
        "plt.ylim([0,1])\n",
        "plt.show()\n",
        "plt.savefig(model_opt+'_'+'acc.png')\n",
        "\n",
        "plt.plot(range(1, epochs+1), result.history['output1_loss'], label=\"training#1\")\n",
        "plt.plot(range(1, epochs+1), result.history['output2_loss'], label=\"training#2\")\n",
        "plt.plot(range(1, epochs+1), result.history['val_output1_loss'], label=\"validation#1\")\n",
        "plt.plot(range(1, epochs+1), result.history['val_output2_loss'], label=\"validation#2\")\n",
        "plt.title('Loss History')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.xlim([1,epochs])\n",
        "plt.ylim([0,10])\n",
        "plt.show()\n",
        "plt.savefig(model_opt+'_'+'loss.png')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMUn9f8NOIadgx/T0AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wFchUhzaCWMhwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zq7fmArk0bIN",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.6 モデルによる予測\n",
        "\n",
        "精度だけ見ても面白くないので、検証用データの予測結果を見てみましょう。\n",
        "\n",
        "Sequential.predict関数によって予測を行うことができます。\n",
        "\n",
        "主な引数は次の通りです。\n",
        "\n",
        "* x_test：予測に使用する入力データ\n",
        "* batch_size：まとめて1度に予測を行うサンプル数\n",
        "* verbose：評価のログを出力するか（0:しない(デフォルト)、1：する）"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7CJSTL50bIN",
        "colab_type": "code",
        "outputId": "9ac14539-ac5b-4b54-dbe3-34263583978f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "classes = model.predict(x_test, batch_size=128, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r32/32 [==============================] - 0s 3ms/sample\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx85EWpfW7Nz",
        "colab_type": "text"
      },
      "source": [
        "上記コマンドで検証用データの予測を実施しています。\n",
        "\n",
        "どのような結果が得られているかイメージしやすくするために、画像を1枚抽出して、その結果を実際に見てみます。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sTY0plvOe-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# データセットの推論結果と元画像を確認\n",
        "# test_numを0～9999で指定してください\n",
        "%matplotlib inline\n",
        "test_num=1\n",
        "test_img = np.squeeze(x_test[test_num])\n",
        "\n",
        "#labels = np.array([0,1,2,3,4,5,6,7,8,9])\n",
        "labels1 = np.array([\n",
        "        'rx-178',\n",
        "        'msz-006',\n",
        "        'rx-93',\n",
        "        'ms-06'])\n",
        "\n",
        "labels2 = np.array([\n",
        "        'EFF',\n",
        "        'Zeon'])\n",
        "\n",
        "print(\"ラベル#1の確からしさ(%)：\"+str(np.round(classes[0][test_num],decimals=2)*100))\n",
        "print(\"推論結果：\"+str(labels1[classes[0][test_num].argmax()]))\n",
        "\n",
        "print(\"ラベル#2の確からしさ(%)：\"+str(np.round(classes[1][test_num],decimals=2)*100))\n",
        "print(\"推論結果：\"+str(labels2[classes[1][test_num].argmax()]))\n",
        "\n",
        "plt.imshow(test_img.astype(np.int),'gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQ4aUVfFVYfE",
        "colab_type": "text"
      },
      "source": [
        "予測精度の最終確認として、Confusion Matrixを表示してみます。\n",
        "\n",
        "X軸とY軸が一致する箇所に集中していれば(要するに左上から右下に斜めに赤くなっている)、精度の良いモデルと言えるでしょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIOawSjjUl0y",
        "colab_type": "code",
        "outputId": "2d42f8fc-6fb1-4c87-d279-f2be38075ea3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 847
        }
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# ラベル#1のマトリックス\n",
        "cmatrix = confusion_matrix(np.argmax(y_test, 1), np.argmax(classes[0], 1))\n",
        "cmatrix_plt = pd.DataFrame(cmatrix, index=labels1, columns=labels1)\n",
        "\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(cmatrix_plt, annot=True, cmap=\"Reds\", fmt=\"d\")\n",
        "plt.show()\n",
        "\n",
        "# ラベル#2のマトリックス\n",
        "cmatrix = confusion_matrix(np.argmax(y_test2, 1), np.argmax(classes[1], 1))\n",
        "cmatrix_plt = pd.DataFrame(cmatrix, index=labels2, columns=labels2)\n",
        "\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(cmatrix_plt, annot=True, cmap=\"Blues\", fmt=\"d\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiAAAAGfCAYAAABm/WkhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiAAAAGfCAYAAABm/WkhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 720x504 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cs86zSLrtyBv",
        "colab_type": "text"
      },
      "source": [
        "###2.2.7 特徴の可視化\n",
        "\n",
        "[GradCam](http://gradcam.cloudcv.org/)を用いて、特徴部位のヒートマップによる可視化を行ってみます。\n",
        "\n",
        "具体的には、CNNモデルが画像のどの場所を見て答えを導いているのか、ヒートマップを用いて可視化します。\n",
        "\n",
        "ソースコードは[ココ](https://github.com/jacobgil/keras-grad-cam)をベースとして、今回のモデルに適合させています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITmUwk2mtzIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.applications.vgg16 import preprocess_input, decode_predictions\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.python.keras.layers.core import Lambda\n",
        "from tensorflow.python.framework import ops\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "\n",
        "def target_category_loss(x, category_index, nb_classes):\n",
        "    return tf.multiply(x, K.one_hot([category_index], nb_classes))\n",
        "\n",
        "def target_category_loss_output_shape(input_shape):\n",
        "    return input_shape\n",
        "\n",
        "def normalize(x):\n",
        "    # utility function to normalize a tensor by its L2 norm\n",
        "    return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
        "\n",
        "def load_image(path):\n",
        "    #img_path = sys.argv[1]\n",
        "    img_path = path\n",
        "    img = image.load_img(img_path, target_size=(height, width))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    x = preprocess_input(x)\n",
        "    return x\n",
        "\n",
        "def register_gradient():\n",
        "    if \"GuidedBackProp\" not in ops._gradient_registry._registry:\n",
        "        @ops.RegisterGradient(\"GuidedBackProp\")\n",
        "        def _GuidedBackProp(op, grad):\n",
        "            dtype = op.inputs[0].dtype\n",
        "            return grad * tf.cast(grad > 0., dtype) * \\\n",
        "                tf.cast(op.inputs[0] > 0., dtype)\n",
        "\n",
        "def compile_saliency_function(model, activation_layer='block5_conv3'):\n",
        "    input_img = model.input\n",
        "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
        "    layer_output = layer_dict[activation_layer].output\n",
        "    max_output = K.max(layer_output, axis=3)\n",
        "    saliency = K.gradients(K.sum(max_output), input_img)[0]\n",
        "    return K.function([input_img, K.learning_phase()], [saliency])\n",
        "\n",
        "def modify_backprop(model, name):\n",
        "    g = tf.get_default_graph()\n",
        "    with g.gradient_override_map({'Relu': name}):\n",
        "\n",
        "        # get layers that have an activation\n",
        "        layer_dict = [layer for layer in model.layers[1:]\n",
        "                      if hasattr(layer, 'activation')]\n",
        "\n",
        "        # replace relu activation\n",
        "        for layer in layer_dict:\n",
        "            if layer.activation == keras.activations.relu:\n",
        "                layer.activation = tf.nn.relu\n",
        "\n",
        "        # re-instanciate a new model\n",
        "        new_model = VGG16(weights='imagenet')\n",
        "    return new_model\n",
        "\n",
        "def deprocess_image(x):\n",
        "    '''\n",
        "    Same normalization as in:\n",
        "    https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n",
        "    '''\n",
        "    if np.ndim(x) > 3:\n",
        "        x = np.squeeze(x)\n",
        "    # normalize tensor: center on 0., ensure std is 0.1\n",
        "    x -= x.mean()\n",
        "    x /= (x.std() + 1e-5)\n",
        "    x *= 0.1\n",
        "\n",
        "    # clip to [0, 1]\n",
        "    x += 0.5\n",
        "    x = np.clip(x, 0, 1)\n",
        "\n",
        "    # convert to RGB array\n",
        "    x *= 255\n",
        "    if K.image_data_format() == 'th':\n",
        "        x = x.transpose((1, 2, 0))\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x\n",
        "\n",
        "def grad_cam(input_model, image, category_index, layer_name):\n",
        "    nb_classes = 4\n",
        "    target_layer = lambda x: target_category_loss(x, category_index, nb_classes)\n",
        "\n",
        "    x = input_model.layers[-2].output\n",
        "    x = Lambda(target_layer, output_shape=target_category_loss_output_shape)(x)\n",
        "    model = Model(input_model.layers[0].input, x)\n",
        "\n",
        "    loss = K.sum(model.layers[-1].output)\n",
        "\n",
        "    print(layer_name)\n",
        "    for l in model.layers:\n",
        "        if l.name == layer_name:\n",
        "            print(l.name)\n",
        "            tmp_layer = l\n",
        "    \n",
        "    print(tmp_layer)\n",
        "            \n",
        "            \n",
        "    #conv_output = [l for l in model.layers[0].layers if l.name is layer_name][0].output\n",
        "#    conv_output = [l for l in model.layers if l.name is layer_name][0].output\n",
        "    conv_output = tmp_layer.output\n",
        "#    print(conv_output)\n",
        "\n",
        "    grads = normalize(K.gradients(loss, conv_output)[0])\n",
        "    gradient_function = K.function([model.layers[0].input], [conv_output, grads])\n",
        "\n",
        "    output, grads_val = gradient_function([image])\n",
        "    output, grads_val = output[0, :], grads_val[0, :, :, :]\n",
        "\n",
        "    weights = np.mean(grads_val, axis = (0, 1))\n",
        "    cam = np.ones(output.shape[0 : 2], dtype = np.float32)\n",
        "\n",
        "    for i, w in enumerate(weights):\n",
        "        cam += w * output[:, :, i]\n",
        "\n",
        "    cam = cv2.resize(cam, (height, width))\n",
        "    cam = np.maximum(cam, 0)\n",
        "    heatmap = cam / np.max(cam)\n",
        "\n",
        "    #Return to BGR [0..255] from the preprocessed image\n",
        "    image = image[0, :]\n",
        "    image -= np.min(image)\n",
        "    image = np.minimum(image, 255)\n",
        "\n",
        "    cam = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n",
        "    cam = np.float32(cam) + np.float32(image)\n",
        "    cam = 255 * cam / np.max(cam)\n",
        "    return np.uint8(cam), heatmap\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2_tP5divPMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# テスト対象イメージを準備\n",
        "preprocessed_input = np.zeros([1, height, width, color], dtype=np.int)\n",
        "preprocessed_input[0] = test_img\n",
        "\n",
        "predictions = model.predict(preprocessed_input)[0]\n",
        "predicted_class = np.argmax(predictions)\n",
        "\n",
        "# 畳み込み最終層の指定\n",
        "if model_opt==\"VGG16\":\n",
        "    cnn_out_layer = \"block5_conv3\"\n",
        "elif model_opt==\"RESNET1\":\n",
        "    cnn_out_layer = \"add_8\"\n",
        "elif model_opt==\"RESNET2\":\n",
        "    cnn_out_layer = \"add_8\"\n",
        "else:\n",
        "    cnn_out_layer = \"conv2d_3\"\n",
        "\n",
        "# Grad Cam\n",
        "cam, heatmap = grad_cam(model, preprocessed_input, predicted_class, cnn_out_layer)\n",
        "plt.imshow(cam.astype(np.int),'gray')\n",
        "plt.show()\n",
        "\n",
        "# Guided Grad Cam\n",
        "register_gradient()\n",
        "guided_model = modify_backprop(model, 'GuidedBackProp')\n",
        "saliency_fn = compile_saliency_function(guided_model)\n",
        "saliency = saliency_fn([preprocessed_input, 0])\n",
        "gradcam = saliency[0] * heatmap[..., np.newaxis]\n",
        "plt.imshow(deprocess_image(gradcam).astype(np.int),'gray')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqbPWg1mfbA3",
        "colab_type": "text"
      },
      "source": [
        "### 2.2.8 モデルのファイル出力\n",
        "\n",
        "学習させたモデルを出力し、静的学習済みモデルとして外部で活用することもできます。\n",
        "\n",
        "ここでは、Keras形式に加えて、TensorFlowのSaved Model形式も試してみます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tcWm1vmlVvA",
        "colab_type": "code",
        "outputId": "484dd498-c1c5-4f59-c23f-94e2b89a789d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        }
      },
      "source": [
        "# Keras形式でモデルを出力\n",
        "output_keras_name = f\"{model_dir}02_{model_opt}_{epochs}_frozen_graph.h5\"\n",
        "model.save(output_keras_name, include_optimizer=False)\n",
        "\n",
        "# TensorFlow Saved Model形式でモデルを出力\n",
        "from tensorflow.contrib import saved_model\n",
        "\n",
        "out_tf_saved_model = f\"{model_dir}02_{model_opt}_{epochs}_saved_models\"\n",
        "\n",
        "if os.path.exists(out_tf_saved_model):\n",
        "    shutil.rmtree(out_tf_saved_model)\n",
        "#saved_model_path = saved_model.save_keras_model(model, out_tf_saved_model)\n",
        "saved_model_path = saved_model.save_keras_model(model, \"./saved_model\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0724 07:31:07.249290 140146061514624 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0724 07:31:07.261420 140146061514624 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0724 07:31:07.750354 140146061514624 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0724 07:31:09.184810 140146061514624 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:253: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "W0724 07:31:09.187309 140146061514624 export_utils.py:182] Export includes no default signature!\n",
            "W0724 07:31:10.435590 140146061514624 export_utils.py:182] Export includes no default signature!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AU493_lgMncX",
        "colab_type": "text"
      },
      "source": [
        "###2.2.9 TensorBoardでの確認\n",
        "\n",
        "TensorBoardを用いてモデル構造を確認してみましょう。\n",
        "\n",
        "Google Colabでは、直接HTTPアクセスはできないので、ngrokを用いて参照させます。\n",
        "\n",
        "下記コード実行後し、出力されるURLにアクセスしてみましょう。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cehjp0h3Mwot",
        "colab_type": "code",
        "outputId": "708d53f6-4b5e-4eab-b36e-8506cb286632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# Web参照のため、ngrokを利用\n",
        "if not os.path.exists('./ngrok'):\n",
        "    !wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "    !unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "# TensorBoardおよびngrokの起動\n",
        "import subprocess\n",
        "cmd = f'tensorboard --logdir=\"{log_dir}\" --host 0.0.0.0 --port 6006 &'\n",
        "proc_tb = subprocess.call(cmd, shell=True)\n",
        "\n",
        "cmd = \"./ngrok http 6006 &\"\n",
        "proc_ng = subprocess.call(cmd, shell=True)\n",
        "\n",
        "# TensorBoard URL\n",
        "!curl -s http://localhost:4040/api/tunnels | python -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-07-24 07:31:13--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.204.136.9, 34.232.40.183, 52.201.75.180, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.204.136.9|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13607069 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  12.98M  6.53MB/s    in 2.0s    \n",
            "\n",
            "2019-07-24 07:31:16 (6.53 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13607069/13607069]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 1, in <module>\n",
            "IndexError: list index out of range\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nwohZfUM6Mx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TensorBoardプロセスの停止\n",
        "#!ps -ef | grep tensorboard | grep -v grep | awk '{print \"kill -9\",$2}'| sh\n",
        "\n",
        "# ngrokプロセスの停止\n",
        "#!ps -ef | grep ngrok | grep -v grep | awk '{print \"kill -9\",$2}'| sh"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMRzOAUDWSob",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}