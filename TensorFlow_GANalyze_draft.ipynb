{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scpepper69/ml-learning-materials/blob/master/TensorFlow_GANalyze_draft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q332UgulrWhj",
        "colab_type": "text"
      },
      "source": [
        "下記GANalyzeのコードをJupyter Notebook上で動作するようしたものです。\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "###GANalyze\n",
        "\n",
        "GitHub：\n",
        "https://github.com/LoreGoetschalckx/GANalyze\n",
        "\n",
        "L. Goetschalckx, A. Andonian, A. Oliva, and P. Isola. GANalyze: Toward Visiual Definitions of Cognitive Image Properties. arXiv:1906.10112, 2019.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUrkMwmRROk9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "ab48bf07-ed71-479c-e657-26fc64662cda"
      },
      "source": [
        "# Tensorflow 1.12 with GPU support (highly recommended)\n",
        "# Ver1.12はcuda9.0なのでそのままだと動かない。1.13.1で動かす。\n",
        "!pip install tensorflow-gpu==1.13.1\n",
        "\n",
        "# Tensorflow hub (for pretrained BigGAN modules)\n",
        "!pip install tensorflow_hub==0.1.1\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/b1/0ad4ae02e17ddd62109cd54c291e311c4b5fd09b4d0678d3d6ce4159b0f0/tensorflow_gpu-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (345.2MB)\n",
            "\u001b[K     |████████████████████████████████| 345.2MB 80kB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (3.7.1)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0 (from tensorflow-gpu==1.13.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 40.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.33.4)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.8.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.16.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0 (from tensorflow-gpu==1.13.1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 40.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (0.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.13.1) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==1.13.1) (41.0.1)\n",
            "Collecting mock>=2.0.0 (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow-gpu==1.13.1)\n",
            "  Downloading https://files.pythonhosted.org/packages/05/d2/f94e68be6b17f46d2c353564da56e6fb89ef09faeeff3313a046cb810ca9/mock-3.0.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (0.15.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow-gpu==1.13.1) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.13.1) (2.8.0)\n",
            "\u001b[31mERROR: tensorflow 1.14.0 has requirement tensorboard<1.15.0,>=1.14.0, but you'll have tensorboard 1.13.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.14.0 has requirement tensorflow-estimator<1.15.0rc0,>=1.14.0rc0, but you'll have tensorflow-estimator 1.13.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: mock, tensorflow-estimator, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorflow-estimator 1.14.0\n",
            "    Uninstalling tensorflow-estimator-1.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
            "  Found existing installation: tensorboard 1.14.0\n",
            "    Uninstalling tensorboard-1.14.0:\n",
            "      Successfully uninstalled tensorboard-1.14.0\n",
            "Successfully installed mock-3.0.5 tensorboard-1.13.1 tensorflow-estimator-1.13.0 tensorflow-gpu-1.13.1\n",
            "Collecting tensorflow_hub==0.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/22/64f246ef80e64b1a13b2f463cefa44f397a51c49a303294f5f3d04ac39ac/tensorflow_hub-0.1.1-py2.py3-none-any.whl (52kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.1.1) (1.16.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.1.1) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub==0.1.1) (3.7.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow_hub==0.1.1) (41.0.1)\n",
            "Installing collected packages: tensorflow-hub\n",
            "  Found existing installation: tensorflow-hub 0.4.0\n",
            "    Uninstalling tensorflow-hub-0.4.0:\n",
            "      Successfully uninstalled tensorflow-hub-0.4.0\n",
            "Successfully installed tensorflow-hub-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-NiD7KeRTt-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "d1300274-5f85-419b-94f4-3c157b79749c"
      },
      "source": [
        "!git clone https://github.com/LoreGoetschalckx/GANalyze.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'GANalyze'...\n",
            "remote: Enumerating objects: 50, done.\u001b[K\n",
            "remote: Counting objects:   2% (1/50)   \u001b[K\rremote: Counting objects:   4% (2/50)   \u001b[K\rremote: Counting objects:   6% (3/50)   \u001b[K\rremote: Counting objects:   8% (4/50)   \u001b[K\rremote: Counting objects:  10% (5/50)   \u001b[K\rremote: Counting objects:  12% (6/50)   \u001b[K\rremote: Counting objects:  14% (7/50)   \u001b[K\rremote: Counting objects:  16% (8/50)   \u001b[K\rremote: Counting objects:  18% (9/50)   \u001b[K\rremote: Counting objects:  20% (10/50)   \u001b[K\rremote: Counting objects:  22% (11/50)   \u001b[K\rremote: Counting objects:  24% (12/50)   \u001b[K\rremote: Counting objects:  26% (13/50)   \u001b[K\rremote: Counting objects:  28% (14/50)   \u001b[K\rremote: Counting objects:  30% (15/50)   \u001b[K\rremote: Counting objects:  32% (16/50)   \u001b[K\rremote: Counting objects:  34% (17/50)   \u001b[K\rremote: Counting objects:  36% (18/50)   \u001b[K\rremote: Counting objects:  38% (19/50)   \u001b[K\rremote: Counting objects:  40% (20/50)   \u001b[K\rremote: Counting objects:  42% (21/50)   \u001b[K\rremote: Counting objects:  44% (22/50)   \u001b[K\rremote: Counting objects:  46% (23/50)   \u001b[K\rremote: Counting objects:  48% (24/50)   \u001b[K\rremote: Counting objects:  50% (25/50)   \u001b[K\rremote: Counting objects:  52% (26/50)   \u001b[K\rremote: Counting objects:  54% (27/50)   \u001b[K\rremote: Counting objects:  56% (28/50)   \u001b[K\rremote: Counting objects:  58% (29/50)   \u001b[K\rremote: Counting objects:  60% (30/50)   \u001b[K\rremote: Counting objects:  62% (31/50)   \u001b[K\rremote: Counting objects:  64% (32/50)   \u001b[K\rremote: Counting objects:  66% (33/50)   \u001b[K\rremote: Counting objects:  68% (34/50)   \u001b[K\rremote: Counting objects:  70% (35/50)   \u001b[K\rremote: Counting objects:  72% (36/50)   \u001b[K\rremote: Counting objects:  74% (37/50)   \u001b[K\rremote: Counting objects:  76% (38/50)   \u001b[K\rremote: Counting objects:  78% (39/50)   \u001b[K\rremote: Counting objects:  80% (40/50)   \u001b[K\rremote: Counting objects:  82% (41/50)   \u001b[K\rremote: Counting objects:  84% (42/50)   \u001b[K\rremote: Counting objects:  86% (43/50)   \u001b[K\rremote: Counting objects:  88% (44/50)   \u001b[K\rremote: Counting objects:  90% (45/50)   \u001b[K\rremote: Counting objects:  92% (46/50)   \u001b[K\rremote: Counting objects:  94% (47/50)   \u001b[K\rremote: Counting objects:  96% (48/50)   \u001b[K\rremote: Counting objects:  98% (49/50)   \u001b[K\rremote: Counting objects: 100% (50/50)   \u001b[K\rremote: Counting objects: 100% (50/50), done.\u001b[K\n",
            "remote: Compressing objects: 100% (40/40), done.\u001b[K\n",
            "remote: Total 50 (delta 7), reused 45 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (50/50), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApgahSZLRkhh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        },
        "outputId": "e6388d6a-ed27-4a4f-f9dd-d6c92d3f284b"
      },
      "source": [
        "# マジックコマンド：%cd\n",
        "%cd GANalyze/tensorflow/\n",
        "\n",
        "# 「memnet_mean.mat」と「mean_AADB_regression_warp256_lore.npy」が404だけど。。。\n",
        "!sh download_pretrained.sh"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/GANalyze/tensorflow\n",
            "/content/GANalyze/tensorflow\n",
            "Downloading AestheticsNet weights\n",
            "--2019-07-01 02:25:37--  http://ganalyze.csail.mit.edu/models/aestheticsnet_state_dict.p\n",
            "Resolving ganalyze.csail.mit.edu (ganalyze.csail.mit.edu)... 128.30.100.223\n",
            "Connecting to ganalyze.csail.mit.edu (ganalyze.csail.mit.edu)|128.30.100.223|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 638536993 (609M) [text/x-pascal]\n",
            "Saving to: ‘assessors/aestheticsnet_state_dict.p’\n",
            "\n",
            "aestheticsnet_state 100%[===================>] 608.96M  86.9MB/s    in 7.2s    \n",
            "\n",
            "2019-07-01 02:25:44 (84.5 MB/s) - ‘assessors/aestheticsnet_state_dict.p’ saved [638536993/638536993]\n",
            "\n",
            "Downloading MemNet weights\n",
            "--2019-07-01 02:25:44--  http://ganalyze.csail.mit.edu/models/memnet_state_dict.p\n",
            "Resolving ganalyze.csail.mit.edu (ganalyze.csail.mit.edu)... 128.30.100.223\n",
            "Connecting to ganalyze.csail.mit.edu (ganalyze.csail.mit.edu)|128.30.100.223|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 511965917 (488M) [text/x-pascal]\n",
            "Saving to: ‘assessors/memnet_state_dict.p’\n",
            "\n",
            "memnet_state_dict.p 100%[===================>] 488.25M  86.8MB/s    in 5.8s    \n",
            "\n",
            "2019-07-01 02:25:50 (83.8 MB/s) - ‘assessors/memnet_state_dict.p’ saved [511965917/511965917]\n",
            "\n",
            "--2019-07-01 02:25:50--  http://ganalyze.csail.mit.edu/models/memnet_mean.mat\n",
            "Resolving ganalyze.csail.mit.edu (ganalyze.csail.mit.edu)... 128.30.100.223\n",
            "Connecting to ganalyze.csail.mit.edu (ganalyze.csail.mit.edu)|128.30.100.223|:80... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2019-07-01 02:25:50 ERROR 404: Not Found.\n",
            "\n",
            "--2019-07-01 02:25:50--  http://ganalyze.csail.mit.edu/models/mean_AADB_regression_warp256.binaryproto\n",
            "Resolving ganalyze.csail.mit.edu (ganalyze.csail.mit.edu)... 128.30.100.223\n",
            "Connecting to ganalyze.csail.mit.edu (ganalyze.csail.mit.edu)|128.30.100.223|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 786446 (768K)\n",
            "Saving to: ‘assessors/mean_AADB_regression_warp256.binaryproto’\n",
            "\n",
            "mean_AADB_regressio 100%[===================>] 768.01K  4.66MB/s    in 0.2s    \n",
            "\n",
            "2019-07-01 02:25:51 (4.66 MB/s) - ‘assessors/mean_AADB_regression_warp256.binaryproto’ saved [786446/786446]\n",
            "\n",
            "--2019-07-01 02:25:51--  http://ganalyze.csail.mit.edu/models/mean_AADB_regression_warp256_lore.npy\n",
            "Resolving ganalyze.csail.mit.edu (ganalyze.csail.mit.edu)... 128.30.100.223\n",
            "Connecting to ganalyze.csail.mit.edu (ganalyze.csail.mit.edu)|128.30.100.223|:80... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2019-07-01 02:25:51 ERROR 404: Not Found.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKGP2LoFRrNz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67c697c1-d817-451d-f96d-4791621126f3"
      },
      "source": [
        "# 1秒 2Step程度の速度。元は40万での指定だけど、時間がかかるので1/100 (--num_samples)\n",
        "!python train_tf.py --generator_arch biggan --generator_model biggan256 --assessor memnet --transformer OneDirection None --train_alpha_a -0.5 --train_alpha_b 0.5 --gpu_id 0 --num_samples 4000 --checkpoint_resume 0"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-07-01 02:45:50.778301: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2019-07-01 02:45:50.977781: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-07-01 02:45:50.978405: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2ed9b80 executing computations on platform CUDA. Devices:\n",
            "2019-07-01 02:45:50.978476: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2019-07-01 02:45:50.980850: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-07-01 02:45:50.981256: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2ed9a20 executing computations on platform Host. Devices:\n",
            "2019-07-01 02:45:50.981287: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-07-01 02:45:50.981632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.60GiB\n",
            "2019-07-01 02:45:50.981659: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-07-01 02:45:50.982243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-07-01 02:45:50.982270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-07-01 02:45:50.982277: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-07-01 02:45:50.982531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14202 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "\n",
            "approach:  one_direction\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /content/GANalyze/tensorflow/assessors/memnet.py:115: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.conv2d instead.\n",
            "WARNING:tensorflow:From /content/GANalyze/tensorflow/assessors/memnet.py:120: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.max_pooling2d instead.\n",
            "WARNING:tensorflow:From /content/GANalyze/tensorflow/assessors/memnet.py:248: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "WARNING:tensorflow:From /content/GANalyze/tensorflow/assessors/memnet.py:270: average_pooling1d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.average_pooling1d instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "initializing tensors:\n",
            "['module/linear/w:0', 'module/Generator/G_Z/G_linear/w:0', 'module/Generator/G_Z/G_linear/u0:0', 'module/Generator/G_Z/G_linear/u1:0', 'module/Generator/G_Z/G_linear/u2:0', 'module/Generator/G_Z/G_linear/b:0', 'module/Generator/GBlock/HyperBN/gamma/w:0', 'module/Generator/GBlock/HyperBN/gamma/u0:0', 'module/Generator/GBlock/HyperBN/gamma/u1:0', 'module/Generator/GBlock/HyperBN/gamma/u2:0', 'module/Generator/GBlock/HyperBN/beta/w:0', 'module/Generator/GBlock/HyperBN/beta/u0:0', 'module/Generator/GBlock/HyperBN/beta/u1:0', 'module/Generator/GBlock/HyperBN/beta/u2:0', 'module/Generator/GBlock/CrossReplicaBN/accumulated_mean:0', 'module/Generator/GBlock/CrossReplicaBN/accumulated_var:0', 'module/Generator/GBlock/CrossReplicaBN/accumulation_counter:0', 'module/Generator/GBlock/conv0/w:0', 'module/Generator/GBlock/conv0/u0:0', 'module/Generator/GBlock/conv0/u1:0', 'module/Generator/GBlock/conv0/u2:0', 'module/Generator/GBlock/conv0/b:0', 'module/Generator/GBlock/HyperBN_1/gamma/w:0', 'module/Generator/GBlock/HyperBN_1/gamma/u0:0', 'module/Generator/GBlock/HyperBN_1/gamma/u1:0', 'module/Generator/GBlock/HyperBN_1/gamma/u2:0', 'module/Generator/GBlock/HyperBN_1/beta/w:0', 'module/Generator/GBlock/HyperBN_1/beta/u0:0', 'module/Generator/GBlock/HyperBN_1/beta/u1:0', 'module/Generator/GBlock/HyperBN_1/beta/u2:0', 'module/Generator/GBlock/CrossReplicaBN_1/accumulated_mean:0', 'module/Generator/GBlock/CrossReplicaBN_1/accumulated_var:0', 'module/Generator/GBlock/CrossReplicaBN_1/accumulation_counter:0', 'module/Generator/GBlock/conv1/w:0', 'module/Generator/GBlock/conv1/u0:0', 'module/Generator/GBlock/conv1/u1:0', 'module/Generator/GBlock/conv1/u2:0', 'module/Generator/GBlock/conv1/b:0', 'module/Generator/GBlock/conv_sc/w:0', 'module/Generator/GBlock/conv_sc/u0:0', 'module/Generator/GBlock/conv_sc/u1:0', 'module/Generator/GBlock/conv_sc/u2:0', 'module/Generator/GBlock/conv_sc/b:0', 'module/Generator/GBlock_1/HyperBN/gamma/w:0', 'module/Generator/GBlock_1/HyperBN/gamma/u0:0', 'module/Generator/GBlock_1/HyperBN/gamma/u1:0', 'module/Generator/GBlock_1/HyperBN/gamma/u2:0', 'module/Generator/GBlock_1/HyperBN/beta/w:0', 'module/Generator/GBlock_1/HyperBN/beta/u0:0', 'module/Generator/GBlock_1/HyperBN/beta/u1:0', 'module/Generator/GBlock_1/HyperBN/beta/u2:0', 'module/Generator/GBlock_1/CrossReplicaBN/accumulated_mean:0', 'module/Generator/GBlock_1/CrossReplicaBN/accumulated_var:0', 'module/Generator/GBlock_1/CrossReplicaBN/accumulation_counter:0', 'module/Generator/GBlock_1/conv0/w:0', 'module/Generator/GBlock_1/conv0/u0:0', 'module/Generator/GBlock_1/conv0/u1:0', 'module/Generator/GBlock_1/conv0/u2:0', 'module/Generator/GBlock_1/conv0/b:0', 'module/Generator/GBlock_1/HyperBN_1/gamma/w:0', 'module/Generator/GBlock_1/HyperBN_1/gamma/u0:0', 'module/Generator/GBlock_1/HyperBN_1/gamma/u1:0', 'module/Generator/GBlock_1/HyperBN_1/gamma/u2:0', 'module/Generator/GBlock_1/HyperBN_1/beta/w:0', 'module/Generator/GBlock_1/HyperBN_1/beta/u0:0', 'module/Generator/GBlock_1/HyperBN_1/beta/u1:0', 'module/Generator/GBlock_1/HyperBN_1/beta/u2:0', 'module/Generator/GBlock_1/CrossReplicaBN_1/accumulated_mean:0', 'module/Generator/GBlock_1/CrossReplicaBN_1/accumulated_var:0', 'module/Generator/GBlock_1/CrossReplicaBN_1/accumulation_counter:0', 'module/Generator/GBlock_1/conv1/w:0', 'module/Generator/GBlock_1/conv1/u0:0', 'module/Generator/GBlock_1/conv1/u1:0', 'module/Generator/GBlock_1/conv1/u2:0', 'module/Generator/GBlock_1/conv1/b:0', 'module/Generator/GBlock_1/conv_sc/w:0', 'module/Generator/GBlock_1/conv_sc/u0:0', 'module/Generator/GBlock_1/conv_sc/u1:0', 'module/Generator/GBlock_1/conv_sc/u2:0', 'module/Generator/GBlock_1/conv_sc/b:0', 'module/Generator/GBlock_2/HyperBN/gamma/w:0', 'module/Generator/GBlock_2/HyperBN/gamma/u0:0', 'module/Generator/GBlock_2/HyperBN/gamma/u1:0', 'module/Generator/GBlock_2/HyperBN/gamma/u2:0', 'module/Generator/GBlock_2/HyperBN/beta/w:0', 'module/Generator/GBlock_2/HyperBN/beta/u0:0', 'module/Generator/GBlock_2/HyperBN/beta/u1:0', 'module/Generator/GBlock_2/HyperBN/beta/u2:0', 'module/Generator/GBlock_2/CrossReplicaBN/accumulated_mean:0', 'module/Generator/GBlock_2/CrossReplicaBN/accumulated_var:0', 'module/Generator/GBlock_2/CrossReplicaBN/accumulation_counter:0', 'module/Generator/GBlock_2/conv0/w:0', 'module/Generator/GBlock_2/conv0/u0:0', 'module/Generator/GBlock_2/conv0/u1:0', 'module/Generator/GBlock_2/conv0/u2:0', 'module/Generator/GBlock_2/conv0/b:0', 'module/Generator/GBlock_2/HyperBN_1/gamma/w:0', 'module/Generator/GBlock_2/HyperBN_1/gamma/u0:0', 'module/Generator/GBlock_2/HyperBN_1/gamma/u1:0', 'module/Generator/GBlock_2/HyperBN_1/gamma/u2:0', 'module/Generator/GBlock_2/HyperBN_1/beta/w:0', 'module/Generator/GBlock_2/HyperBN_1/beta/u0:0', 'module/Generator/GBlock_2/HyperBN_1/beta/u1:0', 'module/Generator/GBlock_2/HyperBN_1/beta/u2:0', 'module/Generator/GBlock_2/CrossReplicaBN_1/accumulated_mean:0', 'module/Generator/GBlock_2/CrossReplicaBN_1/accumulated_var:0', 'module/Generator/GBlock_2/CrossReplicaBN_1/accumulation_counter:0', 'module/Generator/GBlock_2/conv1/w:0', 'module/Generator/GBlock_2/conv1/u0:0', 'module/Generator/GBlock_2/conv1/u1:0', 'module/Generator/GBlock_2/conv1/u2:0', 'module/Generator/GBlock_2/conv1/b:0', 'module/Generator/GBlock_2/conv_sc/w:0', 'module/Generator/GBlock_2/conv_sc/u0:0', 'module/Generator/GBlock_2/conv_sc/u1:0', 'module/Generator/GBlock_2/conv_sc/u2:0', 'module/Generator/GBlock_2/conv_sc/b:0', 'module/Generator/GBlock_3/HyperBN/gamma/w:0', 'module/Generator/GBlock_3/HyperBN/gamma/u0:0', 'module/Generator/GBlock_3/HyperBN/gamma/u1:0', 'module/Generator/GBlock_3/HyperBN/gamma/u2:0', 'module/Generator/GBlock_3/HyperBN/beta/w:0', 'module/Generator/GBlock_3/HyperBN/beta/u0:0', 'module/Generator/GBlock_3/HyperBN/beta/u1:0', 'module/Generator/GBlock_3/HyperBN/beta/u2:0', 'module/Generator/GBlock_3/CrossReplicaBN/accumulated_mean:0', 'module/Generator/GBlock_3/CrossReplicaBN/accumulated_var:0', 'module/Generator/GBlock_3/CrossReplicaBN/accumulation_counter:0', 'module/Generator/GBlock_3/conv0/w:0', 'module/Generator/GBlock_3/conv0/u0:0', 'module/Generator/GBlock_3/conv0/u1:0', 'module/Generator/GBlock_3/conv0/u2:0', 'module/Generator/GBlock_3/conv0/b:0', 'module/Generator/GBlock_3/HyperBN_1/gamma/w:0', 'module/Generator/GBlock_3/HyperBN_1/gamma/u0:0', 'module/Generator/GBlock_3/HyperBN_1/gamma/u1:0', 'module/Generator/GBlock_3/HyperBN_1/gamma/u2:0', 'module/Generator/GBlock_3/HyperBN_1/beta/w:0', 'module/Generator/GBlock_3/HyperBN_1/beta/u0:0', 'module/Generator/GBlock_3/HyperBN_1/beta/u1:0', 'module/Generator/GBlock_3/HyperBN_1/beta/u2:0', 'module/Generator/GBlock_3/CrossReplicaBN_1/accumulated_mean:0', 'module/Generator/GBlock_3/CrossReplicaBN_1/accumulated_var:0', 'module/Generator/GBlock_3/CrossReplicaBN_1/accumulation_counter:0', 'module/Generator/GBlock_3/conv1/w:0', 'module/Generator/GBlock_3/conv1/u0:0', 'module/Generator/GBlock_3/conv1/u1:0', 'module/Generator/GBlock_3/conv1/u2:0', 'module/Generator/GBlock_3/conv1/b:0', 'module/Generator/GBlock_3/conv_sc/w:0', 'module/Generator/GBlock_3/conv_sc/u0:0', 'module/Generator/GBlock_3/conv_sc/u1:0', 'module/Generator/GBlock_3/conv_sc/u2:0', 'module/Generator/GBlock_3/conv_sc/b:0', 'module/Generator/GBlock_4/HyperBN/gamma/w:0', 'module/Generator/GBlock_4/HyperBN/gamma/u0:0', 'module/Generator/GBlock_4/HyperBN/gamma/u1:0', 'module/Generator/GBlock_4/HyperBN/gamma/u2:0', 'module/Generator/GBlock_4/HyperBN/beta/w:0', 'module/Generator/GBlock_4/HyperBN/beta/u0:0', 'module/Generator/GBlock_4/HyperBN/beta/u1:0', 'module/Generator/GBlock_4/HyperBN/beta/u2:0', 'module/Generator/GBlock_4/CrossReplicaBN/accumulated_mean:0', 'module/Generator/GBlock_4/CrossReplicaBN/accumulated_var:0', 'module/Generator/GBlock_4/CrossReplicaBN/accumulation_counter:0', 'module/Generator/GBlock_4/conv0/w:0', 'module/Generator/GBlock_4/conv0/u0:0', 'module/Generator/GBlock_4/conv0/u1:0', 'module/Generator/GBlock_4/conv0/u2:0', 'module/Generator/GBlock_4/conv0/b:0', 'module/Generator/GBlock_4/HyperBN_1/gamma/w:0', 'module/Generator/GBlock_4/HyperBN_1/gamma/u0:0', 'module/Generator/GBlock_4/HyperBN_1/gamma/u1:0', 'module/Generator/GBlock_4/HyperBN_1/gamma/u2:0', 'module/Generator/GBlock_4/HyperBN_1/beta/w:0', 'module/Generator/GBlock_4/HyperBN_1/beta/u0:0', 'module/Generator/GBlock_4/HyperBN_1/beta/u1:0', 'module/Generator/GBlock_4/HyperBN_1/beta/u2:0', 'module/Generator/GBlock_4/CrossReplicaBN_1/accumulated_mean:0', 'module/Generator/GBlock_4/CrossReplicaBN_1/accumulated_var:0', 'module/Generator/GBlock_4/CrossReplicaBN_1/accumulation_counter:0', 'module/Generator/GBlock_4/conv1/w:0', 'module/Generator/GBlock_4/conv1/u0:0', 'module/Generator/GBlock_4/conv1/u1:0', 'module/Generator/GBlock_4/conv1/u2:0', 'module/Generator/GBlock_4/conv1/b:0', 'module/Generator/GBlock_4/conv_sc/w:0', 'module/Generator/GBlock_4/conv_sc/u0:0', 'module/Generator/GBlock_4/conv_sc/u1:0', 'module/Generator/GBlock_4/conv_sc/u2:0', 'module/Generator/GBlock_4/conv_sc/b:0', 'module/Generator/attention/theta/w:0', 'module/Generator/attention/theta/u0:0', 'module/Generator/attention/theta/u1:0', 'module/Generator/attention/theta/u2:0', 'module/Generator/attention/phi/w:0', 'module/Generator/attention/phi/u0:0', 'module/Generator/attention/phi/u1:0', 'module/Generator/attention/phi/u2:0', 'module/Generator/attention/g/w:0', 'module/Generator/attention/g/u0:0', 'module/Generator/attention/g/u1:0', 'module/Generator/attention/g/u2:0', 'module/Generator/attention/o_conv/w:0', 'module/Generator/attention/o_conv/u0:0', 'module/Generator/attention/o_conv/u1:0', 'module/Generator/attention/o_conv/u2:0', 'module/Generator/attention/gamma:0', 'module/Generator/GBlock_5/HyperBN/gamma/w:0', 'module/Generator/GBlock_5/HyperBN/gamma/u0:0', 'module/Generator/GBlock_5/HyperBN/gamma/u1:0', 'module/Generator/GBlock_5/HyperBN/gamma/u2:0', 'module/Generator/GBlock_5/HyperBN/beta/w:0', 'module/Generator/GBlock_5/HyperBN/beta/u0:0', 'module/Generator/GBlock_5/HyperBN/beta/u1:0', 'module/Generator/GBlock_5/HyperBN/beta/u2:0', 'module/Generator/GBlock_5/CrossReplicaBN/accumulated_mean:0', 'module/Generator/GBlock_5/CrossReplicaBN/accumulated_var:0', 'module/Generator/GBlock_5/CrossReplicaBN/accumulation_counter:0', 'module/Generator/GBlock_5/conv0/w:0', 'module/Generator/GBlock_5/conv0/u0:0', 'module/Generator/GBlock_5/conv0/u1:0', 'module/Generator/GBlock_5/conv0/u2:0', 'module/Generator/GBlock_5/conv0/b:0', 'module/Generator/GBlock_5/HyperBN_1/gamma/w:0', 'module/Generator/GBlock_5/HyperBN_1/gamma/u0:0', 'module/Generator/GBlock_5/HyperBN_1/gamma/u1:0', 'module/Generator/GBlock_5/HyperBN_1/gamma/u2:0', 'module/Generator/GBlock_5/HyperBN_1/beta/w:0', 'module/Generator/GBlock_5/HyperBN_1/beta/u0:0', 'module/Generator/GBlock_5/HyperBN_1/beta/u1:0', 'module/Generator/GBlock_5/HyperBN_1/beta/u2:0', 'module/Generator/GBlock_5/CrossReplicaBN_1/accumulated_mean:0', 'module/Generator/GBlock_5/CrossReplicaBN_1/accumulated_var:0', 'module/Generator/GBlock_5/CrossReplicaBN_1/accumulation_counter:0', 'module/Generator/GBlock_5/conv1/w:0', 'module/Generator/GBlock_5/conv1/u0:0', 'module/Generator/GBlock_5/conv1/u1:0', 'module/Generator/GBlock_5/conv1/u2:0', 'module/Generator/GBlock_5/conv1/b:0', 'module/Generator/GBlock_5/conv_sc/w:0', 'module/Generator/GBlock_5/conv_sc/u0:0', 'module/Generator/GBlock_5/conv_sc/u1:0', 'module/Generator/GBlock_5/conv_sc/u2:0', 'module/Generator/GBlock_5/conv_sc/b:0', 'module/Generator/ScaledCrossReplicaBN/beta:0', 'module/Generator/ScaledCrossReplicaBN/gamma:0', 'module/Generator/ScaledCrossReplicaBNbn/accumulated_mean:0', 'module/Generator/ScaledCrossReplicaBNbn/accumulated_var:0', 'module/Generator/ScaledCrossReplicaBNbn/accumulation_counter:0', 'module/Generator/conv_2d/w:0', 'module/Generator/conv_2d/u0:0', 'module/Generator/conv_2d/u1:0', 'module/Generator/conv_2d/u2:0', 'module/Generator/conv_2d/b:0', 'module/Generator/G_Z/G_linear/w/ema_b999900:0', 'module/Generator/G_Z/G_linear/b/ema_b999900:0', 'module/Generator/GBlock/HyperBN/gamma/w/ema_b999900:0', 'module/Generator/GBlock/HyperBN/beta/w/ema_b999900:0', 'module/Generator/GBlock/conv0/w/ema_b999900:0', 'module/Generator/GBlock/conv0/b/ema_b999900:0', 'module/Generator/GBlock/HyperBN_1/gamma/w/ema_b999900:0', 'module/Generator/GBlock/HyperBN_1/beta/w/ema_b999900:0', 'module/Generator/GBlock/conv1/w/ema_b999900:0', 'module/Generator/GBlock/conv1/b/ema_b999900:0', 'module/Generator/GBlock/conv_sc/w/ema_b999900:0', 'module/Generator/GBlock/conv_sc/b/ema_b999900:0', 'module/Generator/GBlock_1/HyperBN/gamma/w/ema_b999900:0', 'module/Generator/GBlock_1/HyperBN/beta/w/ema_b999900:0', 'module/Generator/GBlock_1/conv0/w/ema_b999900:0', 'module/Generator/GBlock_1/conv0/b/ema_b999900:0', 'module/Generator/GBlock_1/HyperBN_1/gamma/w/ema_b999900:0', 'module/Generator/GBlock_1/HyperBN_1/beta/w/ema_b999900:0', 'module/Generator/GBlock_1/conv1/w/ema_b999900:0', 'module/Generator/GBlock_1/conv1/b/ema_b999900:0', 'module/Generator/GBlock_1/conv_sc/w/ema_b999900:0', 'module/Generator/GBlock_1/conv_sc/b/ema_b999900:0', 'module/Generator/GBlock_2/HyperBN/gamma/w/ema_b999900:0', 'module/Generator/GBlock_2/HyperBN/beta/w/ema_b999900:0', 'module/Generator/GBlock_2/conv0/w/ema_b999900:0', 'module/Generator/GBlock_2/conv0/b/ema_b999900:0', 'module/Generator/GBlock_2/HyperBN_1/gamma/w/ema_b999900:0', 'module/Generator/GBlock_2/HyperBN_1/beta/w/ema_b999900:0', 'module/Generator/GBlock_2/conv1/w/ema_b999900:0', 'module/Generator/GBlock_2/conv1/b/ema_b999900:0', 'module/Generator/GBlock_2/conv_sc/w/ema_b999900:0', 'module/Generator/GBlock_2/conv_sc/b/ema_b999900:0', 'module/Generator/GBlock_3/HyperBN/gamma/w/ema_b999900:0', 'module/Generator/GBlock_3/HyperBN/beta/w/ema_b999900:0', 'module/Generator/GBlock_3/conv0/w/ema_b999900:0', 'module/Generator/GBlock_3/conv0/b/ema_b999900:0', 'module/Generator/GBlock_3/HyperBN_1/gamma/w/ema_b999900:0', 'module/Generator/GBlock_3/HyperBN_1/beta/w/ema_b999900:0', 'module/Generator/GBlock_3/conv1/w/ema_b999900:0', 'module/Generator/GBlock_3/conv1/b/ema_b999900:0', 'module/Generator/GBlock_3/conv_sc/w/ema_b999900:0', 'module/Generator/GBlock_3/conv_sc/b/ema_b999900:0', 'module/Generator/GBlock_4/HyperBN/gamma/w/ema_b999900:0', 'module/Generator/GBlock_4/HyperBN/beta/w/ema_b999900:0', 'module/Generator/GBlock_4/conv0/w/ema_b999900:0', 'module/Generator/GBlock_4/conv0/b/ema_b999900:0', 'module/Generator/GBlock_4/HyperBN_1/gamma/w/ema_b999900:0', 'module/Generator/GBlock_4/HyperBN_1/beta/w/ema_b999900:0', 'module/Generator/GBlock_4/conv1/w/ema_b999900:0', 'module/Generator/GBlock_4/conv1/b/ema_b999900:0', 'module/Generator/GBlock_4/conv_sc/w/ema_b999900:0', 'module/Generator/GBlock_4/conv_sc/b/ema_b999900:0', 'module/Generator/attention/theta/w/ema_b999900:0', 'module/Generator/attention/phi/w/ema_b999900:0', 'module/Generator/attention/g/w/ema_b999900:0', 'module/Generator/attention/o_conv/w/ema_b999900:0', 'module/Generator/attention/gamma/ema_b999900:0', 'module/Generator/GBlock_5/HyperBN/gamma/w/ema_b999900:0', 'module/Generator/GBlock_5/HyperBN/beta/w/ema_b999900:0', 'module/Generator/GBlock_5/conv0/w/ema_b999900:0', 'module/Generator/GBlock_5/conv0/b/ema_b999900:0', 'module/Generator/GBlock_5/HyperBN_1/gamma/w/ema_b999900:0', 'module/Generator/GBlock_5/HyperBN_1/beta/w/ema_b999900:0', 'module/Generator/GBlock_5/conv1/w/ema_b999900:0', 'module/Generator/GBlock_5/conv1/b/ema_b999900:0', 'module/Generator/GBlock_5/conv_sc/w/ema_b999900:0', 'module/Generator/GBlock_5/conv_sc/b/ema_b999900:0', 'module/Generator/ScaledCrossReplicaBN/beta/ema_b999900:0', 'module/Generator/ScaledCrossReplicaBN/gamma/ema_b999900:0', 'module/Generator/conv_2d/w/ema_b999900:0', 'module/Generator/conv_2d/b/ema_b999900:0', 'module/linear/w/ema_b999900:0', 'module/prev_truncation:0', 'parameters_to_train/w_ganalyze:0', 'conv2d/kernel:0', 'conv2d/bias:0', 'conv2d_1/kernel:0', 'conv2d_1/bias:0', 'conv2d_2/kernel:0', 'conv2d_2/bias:0', 'conv2d_3/kernel:0', 'conv2d_3/bias:0', 'conv2d_4/kernel:0', 'conv2d_4/bias:0', 'conv2d_5/kernel:0', 'conv2d_5/bias:0', 'conv2d_6/kernel:0', 'conv2d_6/bias:0', 'conv2d_7/kernel:0', 'conv2d_7/bias:0', 'dense/kernel:0', 'dense/bias:0', 'dense_1/kernel:0', 'dense_1/bias:0', 'dense_2/kernel:0', 'dense_2/bias:0', 'conv2d_8/kernel:0', 'conv2d_8/bias:0', 'conv2d_9/kernel:0', 'conv2d_9/bias:0', 'conv2d_10/kernel:0', 'conv2d_10/bias:0', 'conv2d_11/kernel:0', 'conv2d_11/bias:0', 'conv2d_12/kernel:0', 'conv2d_12/bias:0', 'conv2d_13/kernel:0', 'conv2d_13/bias:0', 'conv2d_14/kernel:0', 'conv2d_14/bias:0', 'conv2d_15/kernel:0', 'conv2d_15/bias:0', 'dense_3/kernel:0', 'dense_3/bias:0', 'dense_4/kernel:0', 'dense_4/bias:0', 'dense_5/kernel:0', 'dense_5/bias:0', 'beta1_power:0', 'beta2_power:0', 'parameters_to_train/w_ganalyze/Adam:0', 'parameters_to_train/w_ganalyze/Adam_1:0']\n",
            "2019-07-01 02:46:31.332020: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "[0/4000] Loss 0.093064 (0.093064)\n",
            "saving checkpoint\n",
            "[4/4000] Loss 0.108221 (0.100642)\n",
            "[8/4000] Loss 0.021669 (0.074318)\n",
            "[12/4000] Loss 0.112784 (0.083935)\n",
            "[16/4000] Loss 0.066261 (0.080400)\n",
            "[20/4000] Loss 0.030158 (0.072026)\n",
            "[24/4000] Loss 0.122860 (0.079288)\n",
            "[28/4000] Loss 0.130649 (0.085708)\n",
            "[32/4000] Loss 0.091498 (0.086352)\n",
            "[36/4000] Loss 0.160010 (0.093718)\n",
            "[40/4000] Loss 0.080085 (0.092478)\n",
            "[44/4000] Loss 0.109268 (0.093877)\n",
            "[48/4000] Loss 0.049232 (0.090443)\n",
            "[52/4000] Loss 0.089107 (0.090348)\n",
            "[56/4000] Loss 0.066434 (0.088753)\n",
            "[60/4000] Loss 0.145867 (0.092323)\n",
            "[64/4000] Loss 0.046003 (0.089598)\n",
            "[68/4000] Loss 0.106126 (0.090517)\n",
            "[72/4000] Loss 0.112189 (0.091657)\n",
            "[76/4000] Loss 0.122584 (0.093204)\n",
            "[80/4000] Loss 0.074900 (0.092332)\n",
            "[84/4000] Loss 0.111248 (0.093192)\n",
            "[88/4000] Loss 0.123737 (0.094520)\n",
            "[92/4000] Loss 0.106291 (0.095010)\n",
            "[96/4000] Loss 0.107128 (0.095495)\n",
            "[100/4000] Loss 0.043977 (0.093514)\n",
            "[104/4000] Loss 0.098203 (0.093687)\n",
            "[108/4000] Loss 0.052358 (0.092211)\n",
            "[112/4000] Loss 0.070490 (0.091462)\n",
            "[116/4000] Loss 0.075195 (0.090920)\n",
            "[120/4000] Loss 0.025299 (0.088803)\n",
            "[124/4000] Loss 0.037393 (0.087197)\n",
            "[128/4000] Loss 0.134525 (0.088631)\n",
            "[132/4000] Loss 0.044529 (0.087334)\n",
            "[136/4000] Loss 0.068802 (0.086804)\n",
            "[140/4000] Loss 0.108787 (0.087415)\n",
            "[144/4000] Loss 0.127058 (0.088486)\n",
            "[148/4000] Loss 0.188413 (0.091116)\n",
            "[152/4000] Loss 0.085134 (0.090963)\n",
            "[156/4000] Loss 0.130052 (0.091940)\n",
            "[160/4000] Loss 0.089210 (0.091873)\n",
            "[164/4000] Loss 0.034349 (0.090504)\n",
            "[168/4000] Loss 0.023448 (0.088944)\n",
            "[172/4000] Loss 0.030099 (0.087607)\n",
            "[176/4000] Loss 0.115179 (0.088219)\n",
            "[180/4000] Loss 0.143635 (0.089424)\n",
            "[184/4000] Loss 0.139762 (0.090495)\n",
            "[188/4000] Loss 0.114291 (0.090991)\n",
            "[192/4000] Loss 0.032669 (0.089801)\n",
            "[196/4000] Loss 0.121376 (0.090432)\n",
            "[200/4000] Loss 0.144549 (0.091493)\n",
            "[204/4000] Loss 0.056318 (0.090817)\n",
            "[208/4000] Loss 0.057338 (0.090185)\n",
            "[212/4000] Loss 0.119562 (0.090729)\n",
            "[216/4000] Loss 0.156313 (0.091922)\n",
            "[220/4000] Loss 0.145378 (0.092876)\n",
            "[224/4000] Loss 0.122001 (0.093387)\n",
            "[228/4000] Loss 0.171972 (0.094742)\n",
            "[232/4000] Loss 0.064109 (0.094223)\n",
            "[236/4000] Loss 0.095850 (0.094250)\n",
            "[240/4000] Loss 0.086322 (0.094120)\n",
            "[244/4000] Loss 0.069332 (0.093720)\n",
            "[248/4000] Loss 0.032065 (0.092742)\n",
            "[252/4000] Loss 0.085594 (0.092630)\n",
            "[256/4000] Loss 0.077855 (0.092403)\n",
            "[260/4000] Loss 0.048086 (0.091731)\n",
            "[264/4000] Loss 0.053967 (0.091167)\n",
            "[268/4000] Loss 0.024979 (0.090194)\n",
            "[272/4000] Loss 0.050795 (0.089623)\n",
            "[276/4000] Loss 0.050229 (0.089060)\n",
            "[280/4000] Loss 0.077808 (0.088902)\n",
            "[284/4000] Loss 0.081399 (0.088798)\n",
            "[288/4000] Loss 0.039257 (0.088119)\n",
            "[292/4000] Loss 0.120177 (0.088552)\n",
            "[296/4000] Loss 0.041355 (0.087923)\n",
            "[300/4000] Loss 0.097329 (0.088047)\n",
            "[304/4000] Loss 0.135685 (0.088665)\n",
            "[308/4000] Loss 0.121352 (0.089084)\n",
            "[312/4000] Loss 0.044927 (0.088525)\n",
            "[316/4000] Loss 0.141559 (0.089188)\n",
            "[320/4000] Loss 0.069006 (0.088939)\n",
            "[324/4000] Loss 0.069384 (0.088701)\n",
            "[328/4000] Loss 0.017565 (0.087844)\n",
            "[332/4000] Loss 0.100141 (0.087990)\n",
            "[336/4000] Loss 0.057994 (0.087637)\n",
            "[340/4000] Loss 0.103749 (0.087825)\n",
            "[344/4000] Loss 0.068801 (0.087606)\n",
            "[348/4000] Loss 0.095303 (0.087693)\n",
            "[352/4000] Loss 0.013680 (0.086862)\n",
            "[356/4000] Loss 0.063024 (0.086597)\n",
            "[360/4000] Loss 0.041603 (0.086102)\n",
            "[364/4000] Loss 0.110172 (0.086364)\n",
            "[368/4000] Loss 0.128820 (0.086821)\n",
            "[372/4000] Loss 0.108882 (0.087055)\n",
            "[376/4000] Loss 0.103752 (0.087231)\n",
            "[380/4000] Loss 0.038691 (0.086725)\n",
            "[384/4000] Loss 0.150325 (0.087381)\n",
            "[388/4000] Loss 0.067766 (0.087181)\n",
            "[392/4000] Loss 0.162234 (0.087939)\n",
            "[396/4000] Loss 0.071615 (0.087776)\n",
            "[400/4000] Loss 0.092111 (0.087819)\n",
            "[404/4000] Loss 0.113556 (0.088071)\n",
            "[408/4000] Loss 0.051785 (0.087719)\n",
            "[412/4000] Loss 0.078967 (0.087635)\n",
            "[416/4000] Loss 0.070947 (0.087476)\n",
            "[420/4000] Loss 0.103804 (0.087630)\n",
            "[424/4000] Loss 0.029087 (0.087083)\n",
            "[428/4000] Loss 0.122176 (0.087407)\n",
            "[432/4000] Loss 0.117301 (0.087682)\n",
            "[436/4000] Loss 0.094043 (0.087740)\n",
            "[440/4000] Loss 0.056631 (0.087459)\n",
            "[444/4000] Loss 0.043232 (0.087064)\n",
            "[448/4000] Loss 0.042288 (0.086668)\n",
            "[452/4000] Loss 0.099163 (0.086778)\n",
            "[456/4000] Loss 0.088785 (0.086795)\n",
            "[460/4000] Loss 0.092855 (0.086847)\n",
            "[464/4000] Loss 0.084131 (0.086824)\n",
            "[468/4000] Loss 0.100189 (0.086938)\n",
            "[472/4000] Loss 0.099479 (0.087043)\n",
            "[476/4000] Loss 0.064110 (0.086852)\n",
            "[480/4000] Loss 0.095811 (0.086926)\n",
            "[484/4000] Loss 0.144486 (0.087398)\n",
            "[488/4000] Loss 0.077376 (0.087316)\n",
            "[492/4000] Loss 0.063839 (0.087127)\n",
            "[496/4000] Loss 0.096027 (0.087198)\n",
            "[500/4000] Loss 0.028366 (0.086731)\n",
            "[504/4000] Loss 0.158261 (0.087294)\n",
            "[508/4000] Loss 0.040532 (0.086929)\n",
            "[512/4000] Loss 0.051171 (0.086652)\n",
            "[516/4000] Loss 0.065928 (0.086492)\n",
            "[520/4000] Loss 0.107292 (0.086651)\n",
            "[524/4000] Loss 0.046354 (0.086346)\n",
            "[528/4000] Loss 0.070606 (0.086228)\n",
            "[532/4000] Loss 0.119529 (0.086476)\n",
            "[536/4000] Loss 0.150237 (0.086948)\n",
            "[540/4000] Loss 0.070371 (0.086826)\n",
            "[544/4000] Loss 0.065641 (0.086672)\n",
            "[548/4000] Loss 0.061206 (0.086487)\n",
            "[552/4000] Loss 0.114139 (0.086686)\n",
            "[556/4000] Loss 0.122330 (0.086941)\n",
            "[560/4000] Loss 0.055389 (0.086717)\n",
            "[564/4000] Loss 0.078137 (0.086657)\n",
            "[568/4000] Loss 0.072032 (0.086554)\n",
            "[572/4000] Loss 0.085343 (0.086546)\n",
            "[576/4000] Loss 0.114333 (0.086738)\n",
            "[580/4000] Loss 0.031723 (0.086361)\n",
            "[584/4000] Loss 0.131886 (0.086670)\n",
            "[588/4000] Loss 0.069697 (0.086556)\n",
            "[592/4000] Loss 0.128563 (0.086838)\n",
            "[596/4000] Loss 0.055304 (0.086628)\n",
            "[600/4000] Loss 0.093176 (0.086671)\n",
            "[604/4000] Loss 0.101030 (0.086765)\n",
            "[608/4000] Loss 0.136956 (0.087093)\n",
            "[612/4000] Loss 0.134855 (0.087404)\n",
            "[616/4000] Loss 0.057950 (0.087213)\n",
            "[620/4000] Loss 0.076544 (0.087145)\n",
            "[624/4000] Loss 0.053234 (0.086929)\n",
            "[628/4000] Loss 0.086730 (0.086928)\n",
            "[632/4000] Loss 0.096635 (0.086989)\n",
            "[636/4000] Loss 0.128116 (0.087246)\n",
            "[640/4000] Loss 0.091545 (0.087273)\n",
            "[644/4000] Loss 0.068445 (0.087156)\n",
            "[648/4000] Loss 0.041880 (0.086879)\n",
            "[652/4000] Loss 0.107621 (0.087005)\n",
            "[656/4000] Loss 0.031265 (0.086667)\n",
            "[660/4000] Loss 0.094483 (0.086714)\n",
            "[664/4000] Loss 0.065260 (0.086586)\n",
            "[668/4000] Loss 0.048940 (0.086362)\n",
            "[672/4000] Loss 0.109198 (0.086497)\n",
            "[676/4000] Loss 0.128974 (0.086747)\n",
            "[680/4000] Loss 0.039488 (0.086470)\n",
            "[684/4000] Loss 0.088789 (0.086484)\n",
            "[688/4000] Loss 0.001134 (0.085991)\n",
            "[692/4000] Loss 0.082417 (0.085970)\n",
            "[696/4000] Loss 0.021069 (0.085599)\n",
            "[700/4000] Loss 0.089036 (0.085619)\n",
            "[704/4000] Loss 0.059209 (0.085470)\n",
            "[708/4000] Loss 0.101062 (0.085557)\n",
            "[712/4000] Loss 0.139807 (0.085860)\n",
            "[716/4000] Loss 0.047142 (0.085645)\n",
            "[720/4000] Loss 0.060218 (0.085505)\n",
            "[724/4000] Loss 0.160313 (0.085916)\n",
            "[728/4000] Loss 0.089125 (0.085933)\n",
            "[732/4000] Loss 0.126322 (0.086153)\n",
            "[736/4000] Loss 0.086416 (0.086154)\n",
            "[740/4000] Loss 0.093230 (0.086192)\n",
            "[744/4000] Loss 0.139398 (0.086477)\n",
            "[748/4000] Loss 0.052249 (0.086295)\n",
            "[752/4000] Loss 0.093952 (0.086335)\n",
            "[756/4000] Loss 0.151326 (0.086677)\n",
            "[760/4000] Loss 0.113551 (0.086818)\n",
            "[764/4000] Loss 0.062797 (0.086693)\n",
            "[768/4000] Loss 0.139535 (0.086967)\n",
            "[772/4000] Loss 0.058898 (0.086822)\n",
            "[776/4000] Loss 0.107143 (0.086926)\n",
            "[780/4000] Loss 0.152085 (0.087259)\n",
            "[784/4000] Loss 0.027798 (0.086957)\n",
            "[788/4000] Loss 0.129744 (0.087173)\n",
            "[792/4000] Loss 0.092914 (0.087202)\n",
            "[796/4000] Loss 0.122929 (0.087380)\n",
            "[800/4000] Loss 0.083597 (0.087361)\n",
            "[804/4000] Loss 0.101230 (0.087430)\n",
            "[808/4000] Loss 0.063440 (0.087312)\n",
            "[812/4000] Loss 0.084135 (0.087296)\n",
            "[816/4000] Loss 0.100913 (0.087363)\n",
            "[820/4000] Loss 0.042453 (0.087145)\n",
            "[824/4000] Loss 0.122482 (0.087316)\n",
            "[828/4000] Loss 0.094284 (0.087349)\n",
            "[832/4000] Loss 0.072392 (0.087277)\n",
            "[836/4000] Loss 0.042531 (0.087064)\n",
            "[840/4000] Loss 0.073441 (0.087000)\n",
            "[844/4000] Loss 0.061214 (0.086878)\n",
            "[848/4000] Loss 0.078568 (0.086839)\n",
            "[852/4000] Loss 0.146223 (0.087117)\n",
            "[856/4000] Loss 0.069604 (0.087035)\n",
            "[860/4000] Loss 0.046520 (0.086848)\n",
            "[864/4000] Loss 0.105623 (0.086934)\n",
            "[868/4000] Loss 0.094718 (0.086970)\n",
            "[872/4000] Loss 0.080365 (0.086940)\n",
            "[876/4000] Loss 0.160100 (0.087272)\n",
            "[880/4000] Loss 0.107908 (0.087366)\n",
            "[884/4000] Loss 0.067760 (0.087277)\n",
            "[888/4000] Loss 0.053848 (0.087127)\n",
            "[892/4000] Loss 0.041009 (0.086922)\n",
            "[896/4000] Loss 0.086442 (0.086919)\n",
            "[900/4000] Loss 0.084318 (0.086908)\n",
            "[904/4000] Loss 0.147519 (0.087175)\n",
            "[908/4000] Loss 0.057544 (0.087045)\n",
            "[912/4000] Loss 0.076419 (0.086999)\n",
            "[916/4000] Loss 0.081548 (0.086975)\n",
            "[920/4000] Loss 0.087420 (0.086977)\n",
            "[924/4000] Loss 0.047029 (0.086805)\n",
            "[928/4000] Loss 0.083684 (0.086791)\n",
            "[932/4000] Loss 0.081051 (0.086767)\n",
            "[936/4000] Loss 0.104120 (0.086840)\n",
            "[940/4000] Loss 0.048868 (0.086680)\n",
            "[944/4000] Loss 0.092594 (0.086705)\n",
            "[948/4000] Loss 0.031416 (0.086472)\n",
            "[952/4000] Loss 0.073466 (0.086418)\n",
            "[956/4000] Loss 0.069723 (0.086348)\n",
            "[960/4000] Loss 0.035658 (0.086138)\n",
            "[964/4000] Loss 0.083830 (0.086128)\n",
            "[968/4000] Loss 0.152011 (0.086399)\n",
            "[972/4000] Loss 0.128219 (0.086571)\n",
            "[976/4000] Loss 0.048835 (0.086417)\n",
            "[980/4000] Loss 0.106914 (0.086500)\n",
            "[984/4000] Loss 0.142198 (0.086726)\n",
            "[988/4000] Loss 0.161454 (0.087027)\n",
            "[992/4000] Loss 0.146813 (0.087267)\n",
            "[996/4000] Loss 0.015570 (0.086980)\n",
            "[1000/4000] Loss 0.060450 (0.086875)\n",
            "[1004/4000] Loss 0.112695 (0.086977)\n",
            "[1008/4000] Loss 0.021949 (0.086720)\n",
            "[1012/4000] Loss 0.052909 (0.086587)\n",
            "[1016/4000] Loss 0.047381 (0.086433)\n",
            "[1020/4000] Loss 0.121827 (0.086571)\n",
            "[1024/4000] Loss 0.115844 (0.086685)\n",
            "[1028/4000] Loss 0.061201 (0.086587)\n",
            "[1032/4000] Loss 0.084556 (0.086579)\n",
            "[1036/4000] Loss 0.083584 (0.086567)\n",
            "[1040/4000] Loss 0.141585 (0.086778)\n",
            "[1044/4000] Loss 0.096099 (0.086814)\n",
            "[1048/4000] Loss 0.087732 (0.086817)\n",
            "[1052/4000] Loss 0.090910 (0.086833)\n",
            "[1056/4000] Loss 0.088596 (0.086839)\n",
            "[1060/4000] Loss 0.078423 (0.086808)\n",
            "[1064/4000] Loss 0.130216 (0.086970)\n",
            "[1068/4000] Loss 0.095712 (0.087003)\n",
            "[1072/4000] Loss 0.100444 (0.087053)\n",
            "[1076/4000] Loss 0.068124 (0.086983)\n",
            "[1080/4000] Loss 0.102028 (0.087038)\n",
            "[1084/4000] Loss 0.108022 (0.087115)\n",
            "[1088/4000] Loss 0.131082 (0.087276)\n",
            "[1092/4000] Loss 0.107047 (0.087349)\n",
            "[1096/4000] Loss 0.136644 (0.087528)\n",
            "[1100/4000] Loss 0.098039 (0.087566)\n",
            "[1104/4000] Loss 0.119540 (0.087681)\n",
            "[1108/4000] Loss 0.150460 (0.087907)\n",
            "[1112/4000] Loss 0.059941 (0.087807)\n",
            "[1116/4000] Loss 0.108552 (0.087881)\n",
            "[1120/4000] Loss 0.042653 (0.087720)\n",
            "[1124/4000] Loss 0.064665 (0.087638)\n",
            "[1128/4000] Loss 0.067268 (0.087566)\n",
            "[1132/4000] Loss 0.070273 (0.087505)\n",
            "[1136/4000] Loss 0.046429 (0.087361)\n",
            "[1140/4000] Loss 0.131704 (0.087516)\n",
            "[1144/4000] Loss 0.120607 (0.087632)\n",
            "[1148/4000] Loss 0.035894 (0.087452)\n",
            "[1152/4000] Loss 0.067321 (0.087382)\n",
            "[1156/4000] Loss 0.067834 (0.087315)\n",
            "[1160/4000] Loss 0.059636 (0.087220)\n",
            "[1164/4000] Loss 0.113444 (0.087310)\n",
            "[1168/4000] Loss 0.116875 (0.087410)\n",
            "[1172/4000] Loss 0.079547 (0.087384)\n",
            "[1176/4000] Loss 0.109646 (0.087459)\n",
            "[1180/4000] Loss 0.072643 (0.087409)\n",
            "[1184/4000] Loss 0.065473 (0.087335)\n",
            "[1188/4000] Loss 0.067842 (0.087270)\n",
            "[1192/4000] Loss 0.102022 (0.087319)\n",
            "[1196/4000] Loss 0.165669 (0.087580)\n",
            "[1200/4000] Loss 0.054950 (0.087472)\n",
            "[1204/4000] Loss 0.147771 (0.087672)\n",
            "[1208/4000] Loss 0.072406 (0.087621)\n",
            "[1212/4000] Loss 0.098924 (0.087658)\n",
            "[1216/4000] Loss 0.075436 (0.087618)\n",
            "[1220/4000] Loss 0.074966 (0.087577)\n",
            "[1224/4000] Loss 0.118014 (0.087676)\n",
            "[1228/4000] Loss 0.149969 (0.087878)\n",
            "[1232/4000] Loss 0.102057 (0.087924)\n",
            "[1236/4000] Loss 0.068147 (0.087861)\n",
            "[1240/4000] Loss 0.057387 (0.087763)\n",
            "[1244/4000] Loss 0.130757 (0.087900)\n",
            "[1248/4000] Loss 0.080206 (0.087876)\n",
            "[1252/4000] Loss 0.109223 (0.087944)\n",
            "[1256/4000] Loss 0.065487 (0.087872)\n",
            "[1260/4000] Loss 0.096525 (0.087900)\n",
            "[1264/4000] Loss 0.071527 (0.087848)\n",
            "[1268/4000] Loss 0.059746 (0.087760)\n",
            "[1272/4000] Loss 0.049160 (0.087639)\n",
            "[1276/4000] Loss 0.028096 (0.087453)\n",
            "[1280/4000] Loss 0.081820 (0.087435)\n",
            "[1284/4000] Loss 0.096351 (0.087463)\n",
            "[1288/4000] Loss 0.066671 (0.087398)\n",
            "[1292/4000] Loss 0.033204 (0.087231)\n",
            "[1296/4000] Loss 0.081834 (0.087215)\n",
            "[1300/4000] Loss 0.049563 (0.087099)\n",
            "[1304/4000] Loss 0.047153 (0.086977)\n",
            "[1308/4000] Loss 0.042085 (0.086840)\n",
            "[1312/4000] Loss 0.066285 (0.086778)\n",
            "[1316/4000] Loss 0.064158 (0.086709)\n",
            "[1320/4000] Loss 0.137500 (0.086863)\n",
            "[1324/4000] Loss 0.032674 (0.086699)\n",
            "[1328/4000] Loss 0.106140 (0.086758)\n",
            "[1332/4000] Loss 0.057377 (0.086670)\n",
            "[1336/4000] Loss 0.040651 (0.086532)\n",
            "[1340/4000] Loss 0.126593 (0.086652)\n",
            "[1344/4000] Loss 0.099388 (0.086689)\n",
            "[1348/4000] Loss 0.110695 (0.086760)\n",
            "[1352/4000] Loss 0.106405 (0.086818)\n",
            "[1356/4000] Loss 0.038822 (0.086677)\n",
            "[1360/4000] Loss 0.078749 (0.086654)\n",
            "[1364/4000] Loss 0.050584 (0.086548)\n",
            "[1368/4000] Loss 0.093444 (0.086569)\n",
            "[1372/4000] Loss 0.066255 (0.086510)\n",
            "[1376/4000] Loss 0.018085 (0.086311)\n",
            "[1380/4000] Loss 0.060440 (0.086236)\n",
            "[1384/4000] Loss 0.058381 (0.086156)\n",
            "[1388/4000] Loss 0.040497 (0.086025)\n",
            "[1392/4000] Loss 0.051098 (0.085925)\n",
            "[1396/4000] Loss 0.075374 (0.085895)\n",
            "[1400/4000] Loss 0.046828 (0.085783)\n",
            "[1404/4000] Loss 0.010097 (0.085568)\n",
            "[1408/4000] Loss 0.040186 (0.085440)\n",
            "[1412/4000] Loss 0.019848 (0.085255)\n",
            "[1416/4000] Loss 0.036654 (0.085118)\n",
            "[1420/4000] Loss 0.114694 (0.085201)\n",
            "[1424/4000] Loss 0.091280 (0.085218)\n",
            "[1428/4000] Loss 0.095610 (0.085247)\n",
            "[1432/4000] Loss 0.098474 (0.085284)\n",
            "[1436/4000] Loss 0.130094 (0.085408)\n",
            "[1440/4000] Loss 0.056066 (0.085327)\n",
            "[1444/4000] Loss 0.072278 (0.085291)\n",
            "[1448/4000] Loss 0.087998 (0.085298)\n",
            "[1452/4000] Loss 0.036559 (0.085164)\n",
            "[1456/4000] Loss 0.164457 (0.085382)\n",
            "[1460/4000] Loss 0.051420 (0.085289)\n",
            "[1464/4000] Loss 0.132763 (0.085418)\n",
            "[1468/4000] Loss 0.111043 (0.085488)\n",
            "[1472/4000] Loss 0.145932 (0.085652)\n",
            "[1476/4000] Loss 0.121894 (0.085750)\n",
            "[1480/4000] Loss 0.076005 (0.085723)\n",
            "[1484/4000] Loss 0.158149 (0.085918)\n",
            "[1488/4000] Loss 0.099367 (0.085954)\n",
            "[1492/4000] Loss 0.050878 (0.085860)\n",
            "[1496/4000] Loss 0.007305 (0.085651)\n",
            "[1500/4000] Loss 0.078763 (0.085632)\n",
            "[1504/4000] Loss 0.095334 (0.085658)\n",
            "[1508/4000] Loss 0.103667 (0.085706)\n",
            "[1512/4000] Loss 0.088665 (0.085714)\n",
            "[1516/4000] Loss 0.065142 (0.085659)\n",
            "[1520/4000] Loss 0.120266 (0.085750)\n",
            "[1524/4000] Loss 0.062861 (0.085690)\n",
            "[1528/4000] Loss 0.140090 (0.085832)\n",
            "[1532/4000] Loss 0.087474 (0.085837)\n",
            "[1536/4000] Loss 0.138364 (0.085973)\n",
            "[1540/4000] Loss 0.078813 (0.085955)\n",
            "[1544/4000] Loss 0.073164 (0.085922)\n",
            "[1548/4000] Loss 0.019447 (0.085750)\n",
            "[1552/4000] Loss 0.083707 (0.085745)\n",
            "[1556/4000] Loss 0.081366 (0.085734)\n",
            "[1560/4000] Loss 0.069948 (0.085693)\n",
            "[1564/4000] Loss 0.016121 (0.085516)\n",
            "[1568/4000] Loss 0.084812 (0.085514)\n",
            "[1572/4000] Loss 0.082169 (0.085506)\n",
            "[1576/4000] Loss 0.094090 (0.085527)\n",
            "[1580/4000] Loss 0.077850 (0.085508)\n",
            "[1584/4000] Loss 0.180113 (0.085746)\n",
            "[1588/4000] Loss 0.016107 (0.085571)\n",
            "[1592/4000] Loss 0.097928 (0.085602)\n",
            "[1596/4000] Loss 0.101892 (0.085643)\n",
            "[1600/4000] Loss 0.085636 (0.085643)\n",
            "[1604/4000] Loss 0.013378 (0.085463)\n",
            "[1608/4000] Loss 0.096837 (0.085491)\n",
            "[1612/4000] Loss 0.083868 (0.085487)\n",
            "[1616/4000] Loss 0.077995 (0.085469)\n",
            "[1620/4000] Loss 0.097334 (0.085498)\n",
            "[1624/4000] Loss 0.037603 (0.085380)\n",
            "[1628/4000] Loss 0.052431 (0.085300)\n",
            "[1632/4000] Loss 0.172578 (0.085513)\n",
            "[1636/4000] Loss 0.076858 (0.085492)\n",
            "[1640/4000] Loss 0.066446 (0.085446)\n",
            "[1644/4000] Loss 0.087439 (0.085450)\n",
            "[1648/4000] Loss 0.063283 (0.085397)\n",
            "[1652/4000] Loss 0.111438 (0.085460)\n",
            "[1656/4000] Loss 0.045762 (0.085364)\n",
            "[1660/4000] Loss 0.120488 (0.085448)\n",
            "[1664/4000] Loss 0.098129 (0.085479)\n",
            "[1668/4000] Loss 0.027207 (0.085339)\n",
            "[1672/4000] Loss 0.105617 (0.085388)\n",
            "[1676/4000] Loss 0.105866 (0.085437)\n",
            "[1680/4000] Loss 0.015731 (0.085271)\n",
            "[1684/4000] Loss 0.063433 (0.085219)\n",
            "[1688/4000] Loss 0.088915 (0.085228)\n",
            "[1692/4000] Loss 0.110712 (0.085288)\n",
            "[1696/4000] Loss 0.042243 (0.085187)\n",
            "[1700/4000] Loss 0.112931 (0.085252)\n",
            "[1704/4000] Loss 0.083632 (0.085248)\n",
            "[1708/4000] Loss 0.106634 (0.085298)\n",
            "[1712/4000] Loss 0.112814 (0.085362)\n",
            "[1716/4000] Loss 0.032394 (0.085239)\n",
            "[1720/4000] Loss 0.042647 (0.085140)\n",
            "[1724/4000] Loss 0.018887 (0.084987)\n",
            "[1728/4000] Loss 0.130059 (0.085091)\n",
            "[1732/4000] Loss 0.062580 (0.085039)\n",
            "[1736/4000] Loss 0.053106 (0.084966)\n",
            "[1740/4000] Loss 0.043152 (0.084870)\n",
            "[1744/4000] Loss 0.027960 (0.084740)\n",
            "[1748/4000] Loss 0.036083 (0.084629)\n",
            "[1752/4000] Loss 0.107860 (0.084681)\n",
            "[1756/4000] Loss 0.108111 (0.084735)\n",
            "[1760/4000] Loss 0.076323 (0.084716)\n",
            "[1764/4000] Loss 0.032596 (0.084598)\n",
            "[1768/4000] Loss 0.116911 (0.084671)\n",
            "[1772/4000] Loss 0.151737 (0.084822)\n",
            "[1776/4000] Loss 0.086189 (0.084825)\n",
            "[1780/4000] Loss 0.048381 (0.084743)\n",
            "[1784/4000] Loss 0.126505 (0.084836)\n",
            "[1788/4000] Loss 0.101665 (0.084874)\n",
            "[1792/4000] Loss 0.070214 (0.084841)\n",
            "[1796/4000] Loss 0.042281 (0.084747)\n",
            "[1800/4000] Loss 0.056211 (0.084684)\n",
            "[1804/4000] Loss 0.071051 (0.084653)\n",
            "[1808/4000] Loss 0.089162 (0.084663)\n",
            "[1812/4000] Loss 0.027169 (0.084537)\n",
            "[1816/4000] Loss 0.110391 (0.084594)\n",
            "[1820/4000] Loss 0.113494 (0.084657)\n",
            "[1824/4000] Loss 0.059648 (0.084602)\n",
            "[1828/4000] Loss 0.122636 (0.084685)\n",
            "[1832/4000] Loss 0.091397 (0.084700)\n",
            "[1836/4000] Loss 0.035523 (0.084593)\n",
            "[1840/4000] Loss 0.130748 (0.084693)\n",
            "[1844/4000] Loss 0.079437 (0.084682)\n",
            "[1848/4000] Loss 0.056381 (0.084621)\n",
            "[1852/4000] Loss 0.090033 (0.084632)\n",
            "[1856/4000] Loss 0.068374 (0.084597)\n",
            "[1860/4000] Loss 0.116513 (0.084666)\n",
            "[1864/4000] Loss 0.079559 (0.084655)\n",
            "[1868/4000] Loss 0.137911 (0.084769)\n",
            "[1872/4000] Loss 0.070310 (0.084738)\n",
            "[1876/4000] Loss 0.072337 (0.084711)\n",
            "[1880/4000] Loss 0.039517 (0.084615)\n",
            "[1884/4000] Loss 0.130702 (0.084713)\n",
            "[1888/4000] Loss 0.077209 (0.084697)\n",
            "[1892/4000] Loss 0.103850 (0.084738)\n",
            "[1896/4000] Loss 0.078424 (0.084724)\n",
            "[1900/4000] Loss 0.121093 (0.084801)\n",
            "[1904/4000] Loss 0.050679 (0.084729)\n",
            "[1908/4000] Loss 0.119534 (0.084802)\n",
            "[1912/4000] Loss 0.033581 (0.084695)\n",
            "[1916/4000] Loss 0.071108 (0.084667)\n",
            "[1920/4000] Loss 0.066690 (0.084629)\n",
            "[1924/4000] Loss 0.038227 (0.084533)\n",
            "[1928/4000] Loss 0.121512 (0.084610)\n",
            "[1932/4000] Loss 0.098716 (0.084639)\n",
            "[1936/4000] Loss 0.082987 (0.084635)\n",
            "[1940/4000] Loss 0.075595 (0.084617)\n",
            "[1944/4000] Loss 0.073220 (0.084593)\n",
            "[1948/4000] Loss 0.129670 (0.084686)\n",
            "[1952/4000] Loss 0.040809 (0.084596)\n",
            "[1956/4000] Loss 0.091210 (0.084610)\n",
            "[1960/4000] Loss 0.086417 (0.084613)\n",
            "[1964/4000] Loss 0.085465 (0.084615)\n",
            "[1968/4000] Loss 0.098879 (0.084644)\n",
            "[1972/4000] Loss 0.039027 (0.084552)\n",
            "[1976/4000] Loss 0.054327 (0.084490)\n",
            "[1980/4000] Loss 0.092693 (0.084507)\n",
            "[1984/4000] Loss 0.136987 (0.084613)\n",
            "[1988/4000] Loss 0.053586 (0.084550)\n",
            "[1992/4000] Loss 0.140476 (0.084662)\n",
            "[1996/4000] Loss 0.052093 (0.084597)\n",
            "[2000/4000] Loss 0.068372 (0.084565)\n",
            "[2004/4000] Loss 0.091462 (0.084579)\n",
            "[2008/4000] Loss 0.073330 (0.084556)\n",
            "[2012/4000] Loss 0.026463 (0.084441)\n",
            "[2016/4000] Loss 0.060832 (0.084394)\n",
            "[2020/4000] Loss 0.150032 (0.084524)\n",
            "[2024/4000] Loss 0.126590 (0.084607)\n",
            "[2028/4000] Loss 0.059702 (0.084558)\n",
            "[2032/4000] Loss 0.116786 (0.084621)\n",
            "[2036/4000] Loss 0.127344 (0.084705)\n",
            "[2040/4000] Loss 0.080641 (0.084697)\n",
            "[2044/4000] Loss 0.113612 (0.084754)\n",
            "[2048/4000] Loss 0.047698 (0.084681)\n",
            "[2052/4000] Loss 0.116326 (0.084743)\n",
            "[2056/4000] Loss 0.122618 (0.084816)\n",
            "[2060/4000] Loss 0.092811 (0.084832)\n",
            "[2064/4000] Loss 0.081803 (0.084826)\n",
            "[2068/4000] Loss 0.056408 (0.084771)\n",
            "[2072/4000] Loss 0.021603 (0.084649)\n",
            "[2076/4000] Loss 0.025470 (0.084536)\n",
            "[2080/4000] Loss 0.068773 (0.084505)\n",
            "[2084/4000] Loss 0.043388 (0.084427)\n",
            "[2088/4000] Loss 0.043744 (0.084349)\n",
            "[2092/4000] Loss 0.120709 (0.084418)\n",
            "[2096/4000] Loss 0.044442 (0.084342)\n",
            "[2100/4000] Loss 0.153817 (0.084474)\n",
            "[2104/4000] Loss 0.124599 (0.084550)\n",
            "[2108/4000] Loss 0.066606 (0.084516)\n",
            "[2112/4000] Loss 0.091086 (0.084529)\n",
            "[2116/4000] Loss 0.062402 (0.084487)\n",
            "[2120/4000] Loss 0.098571 (0.084513)\n",
            "[2124/4000] Loss 0.099770 (0.084542)\n",
            "[2128/4000] Loss 0.036054 (0.084451)\n",
            "[2132/4000] Loss 0.107230 (0.084494)\n",
            "[2136/4000] Loss 0.058267 (0.084445)\n",
            "[2140/4000] Loss 0.109011 (0.084491)\n",
            "[2144/4000] Loss 0.135235 (0.084585)\n",
            "[2148/4000] Loss 0.104471 (0.084622)\n",
            "[2152/4000] Loss 0.040603 (0.084540)\n",
            "[2156/4000] Loss 0.086400 (0.084544)\n",
            "[2160/4000] Loss 0.032440 (0.084448)\n",
            "[2164/4000] Loss 0.157645 (0.084583)\n",
            "[2168/4000] Loss 0.037156 (0.084495)\n",
            "[2172/4000] Loss 0.093350 (0.084512)\n",
            "[2176/4000] Loss 0.160386 (0.084651)\n",
            "[2180/4000] Loss 0.038039 (0.084565)\n",
            "[2184/4000] Loss 0.154975 (0.084694)\n",
            "[2188/4000] Loss 0.037623 (0.084608)\n",
            "[2192/4000] Loss 0.063203 (0.084569)\n",
            "[2196/4000] Loss 0.076708 (0.084555)\n",
            "[2200/4000] Loss 0.115247 (0.084611)\n",
            "[2204/4000] Loss 0.106500 (0.084650)\n",
            "[2208/4000] Loss 0.131691 (0.084735)\n",
            "[2212/4000] Loss 0.101741 (0.084766)\n",
            "[2216/4000] Loss 0.040933 (0.084687)\n",
            "[2220/4000] Loss 0.146466 (0.084798)\n",
            "[2224/4000] Loss 0.089915 (0.084807)\n",
            "[2228/4000] Loss 0.047047 (0.084740)\n",
            "[2232/4000] Loss 0.055866 (0.084688)\n",
            "[2236/4000] Loss 0.135189 (0.084778)\n",
            "[2240/4000] Loss 0.042481 (0.084703)\n",
            "[2244/4000] Loss 0.102680 (0.084735)\n",
            "[2248/4000] Loss 0.026035 (0.084631)\n",
            "[2252/4000] Loss 0.028549 (0.084531)\n",
            "[2256/4000] Loss 0.138210 (0.084626)\n",
            "[2260/4000] Loss 0.068497 (0.084598)\n",
            "[2264/4000] Loss 0.063381 (0.084560)\n",
            "[2268/4000] Loss 0.092060 (0.084573)\n",
            "[2272/4000] Loss 0.081826 (0.084569)\n",
            "[2276/4000] Loss 0.051520 (0.084511)\n",
            "[2280/4000] Loss 0.042733 (0.084437)\n",
            "[2284/4000] Loss 0.093192 (0.084453)\n",
            "[2288/4000] Loss 0.072696 (0.084432)\n",
            "[2292/4000] Loss 0.126834 (0.084506)\n",
            "[2296/4000] Loss 0.075535 (0.084491)\n",
            "[2300/4000] Loss 0.120550 (0.084553)\n",
            "[2304/4000] Loss 0.023082 (0.084447)\n",
            "[2308/4000] Loss 0.078284 (0.084436)\n",
            "[2312/4000] Loss 0.079233 (0.084427)\n",
            "[2316/4000] Loss 0.046771 (0.084362)\n",
            "[2320/4000] Loss 0.108757 (0.084404)\n",
            "[2324/4000] Loss 0.137572 (0.084495)\n",
            "[2328/4000] Loss 0.041474 (0.084422)\n",
            "[2332/4000] Loss 0.047220 (0.084358)\n",
            "[2336/4000] Loss 0.058424 (0.084314)\n",
            "[2340/4000] Loss 0.142607 (0.084413)\n",
            "[2344/4000] Loss 0.085481 (0.084415)\n",
            "[2348/4000] Loss 0.088649 (0.084422)\n",
            "[2352/4000] Loss 0.088686 (0.084429)\n",
            "[2356/4000] Loss 0.099423 (0.084455)\n",
            "[2360/4000] Loss 0.074332 (0.084438)\n",
            "[2364/4000] Loss 0.045044 (0.084371)\n",
            "[2368/4000] Loss 0.088087 (0.084377)\n",
            "[2372/4000] Loss 0.099762 (0.084403)\n",
            "[2376/4000] Loss 0.080577 (0.084397)\n",
            "[2380/4000] Loss 0.097416 (0.084419)\n",
            "[2384/4000] Loss 0.081800 (0.084414)\n",
            "[2388/4000] Loss 0.082509 (0.084411)\n",
            "[2392/4000] Loss 0.059714 (0.084370)\n",
            "[2396/4000] Loss 0.075704 (0.084355)\n",
            "[2400/4000] Loss 0.113465 (0.084404)\n",
            "[2404/4000] Loss 0.109510 (0.084445)\n",
            "[2408/4000] Loss 0.098869 (0.084469)\n",
            "[2412/4000] Loss 0.130872 (0.084546)\n",
            "[2416/4000] Loss 0.117238 (0.084600)\n",
            "[2420/4000] Loss 0.036818 (0.084521)\n",
            "[2424/4000] Loss 0.056517 (0.084475)\n",
            "[2428/4000] Loss 0.120593 (0.084535)\n",
            "[2432/4000] Loss 0.054031 (0.084485)\n",
            "[2436/4000] Loss 0.063834 (0.084451)\n",
            "[2440/4000] Loss 0.106050 (0.084486)\n",
            "[2444/4000] Loss 0.104602 (0.084519)\n",
            "[2448/4000] Loss 0.093718 (0.084534)\n",
            "[2452/4000] Loss 0.107548 (0.084571)\n",
            "[2456/4000] Loss 0.103488 (0.084602)\n",
            "[2460/4000] Loss 0.104732 (0.084635)\n",
            "[2464/4000] Loss 0.073221 (0.084616)\n",
            "[2468/4000] Loss 0.124437 (0.084681)\n",
            "[2472/4000] Loss 0.045565 (0.084618)\n",
            "[2476/4000] Loss 0.037992 (0.084542)\n",
            "[2480/4000] Loss 0.065894 (0.084512)\n",
            "[2484/4000] Loss 0.064549 (0.084480)\n",
            "[2488/4000] Loss 0.110912 (0.084523)\n",
            "[2492/4000] Loss 0.095340 (0.084540)\n",
            "[2496/4000] Loss 0.066124 (0.084511)\n",
            "[2500/4000] Loss 0.088346 (0.084517)\n",
            "[2504/4000] Loss 0.074068 (0.084500)\n",
            "[2508/4000] Loss 0.104677 (0.084532)\n",
            "[2512/4000] Loss 0.067892 (0.084506)\n",
            "[2516/4000] Loss 0.065093 (0.084475)\n",
            "[2520/4000] Loss 0.106304 (0.084510)\n",
            "[2524/4000] Loss 0.110099 (0.084550)\n",
            "[2528/4000] Loss 0.061866 (0.084514)\n",
            "[2532/4000] Loss 0.048862 (0.084458)\n",
            "[2536/4000] Loss 0.047486 (0.084400)\n",
            "[2540/4000] Loss 0.084697 (0.084400)\n",
            "[2544/4000] Loss 0.060827 (0.084363)\n",
            "[2548/4000] Loss 0.154628 (0.084473)\n",
            "[2552/4000] Loss 0.072677 (0.084455)\n",
            "[2556/4000] Loss 0.025409 (0.084363)\n",
            "[2560/4000] Loss 0.116906 (0.084413)\n",
            "[2564/4000] Loss 0.180797 (0.084564)\n",
            "[2568/4000] Loss 0.088000 (0.084569)\n",
            "[2572/4000] Loss 0.061688 (0.084533)\n",
            "[2576/4000] Loss 0.086437 (0.084536)\n",
            "[2580/4000] Loss 0.053325 (0.084488)\n",
            "[2584/4000] Loss 0.082978 (0.084486)\n",
            "[2588/4000] Loss 0.074339 (0.084470)\n",
            "[2592/4000] Loss 0.153343 (0.084576)\n",
            "[2596/4000] Loss 0.057466 (0.084534)\n",
            "[2600/4000] Loss 0.056635 (0.084492)\n",
            "[2604/4000] Loss 0.076184 (0.084479)\n",
            "[2608/4000] Loss 0.091507 (0.084490)\n",
            "[2612/4000] Loss 0.091171 (0.084500)\n",
            "[2616/4000] Loss 0.032266 (0.084420)\n",
            "[2620/4000] Loss 0.103956 (0.084450)\n",
            "[2624/4000] Loss 0.019752 (0.084351)\n",
            "[2628/4000] Loss 0.110599 (0.084391)\n",
            "[2632/4000] Loss 0.070675 (0.084370)\n",
            "[2636/4000] Loss 0.144681 (0.084462)\n",
            "[2640/4000] Loss 0.067811 (0.084437)\n",
            "[2644/4000] Loss 0.084973 (0.084437)\n",
            "[2648/4000] Loss 0.064855 (0.084408)\n",
            "[2652/4000] Loss 0.106434 (0.084441)\n",
            "[2656/4000] Loss 0.081666 (0.084437)\n",
            "[2660/4000] Loss 0.030318 (0.084356)\n",
            "[2664/4000] Loss 0.053975 (0.084310)\n",
            "[2668/4000] Loss 0.044642 (0.084251)\n",
            "[2672/4000] Loss 0.097142 (0.084270)\n",
            "[2676/4000] Loss 0.109060 (0.084307)\n",
            "[2680/4000] Loss 0.127468 (0.084371)\n",
            "[2684/4000] Loss 0.081429 (0.084367)\n",
            "[2688/4000] Loss 0.097282 (0.084386)\n",
            "[2692/4000] Loss 0.062188 (0.084353)\n",
            "[2696/4000] Loss 0.083035 (0.084351)\n",
            "[2700/4000] Loss 0.090758 (0.084361)\n",
            "[2704/4000] Loss 0.143713 (0.084448)\n",
            "[2708/4000] Loss 0.159204 (0.084559)\n",
            "[2712/4000] Loss 0.085240 (0.084560)\n",
            "[2716/4000] Loss 0.090350 (0.084568)\n",
            "[2720/4000] Loss 0.123637 (0.084625)\n",
            "[2724/4000] Loss 0.035268 (0.084553)\n",
            "[2728/4000] Loss 0.040319 (0.084488)\n",
            "[2732/4000] Loss 0.077672 (0.084478)\n",
            "[2736/4000] Loss 0.135519 (0.084553)\n",
            "[2740/4000] Loss 0.016479 (0.084454)\n",
            "[2744/4000] Loss 0.116853 (0.084501)\n",
            "[2748/4000] Loss 0.113997 (0.084544)\n",
            "[2752/4000] Loss 0.111536 (0.084583)\n",
            "[2756/4000] Loss 0.049094 (0.084531)\n",
            "[2760/4000] Loss 0.118278 (0.084580)\n",
            "[2764/4000] Loss 0.088094 (0.084585)\n",
            "[2768/4000] Loss 0.088978 (0.084592)\n",
            "[2772/4000] Loss 0.091557 (0.084602)\n",
            "[2776/4000] Loss 0.084681 (0.084602)\n",
            "[2780/4000] Loss 0.057486 (0.084563)\n",
            "[2784/4000] Loss 0.115958 (0.084608)\n",
            "[2788/4000] Loss 0.028573 (0.084528)\n",
            "[2792/4000] Loss 0.083119 (0.084526)\n",
            "[2796/4000] Loss 0.109150 (0.084561)\n",
            "[2800/4000] Loss 0.026796 (0.084478)\n",
            "[2804/4000] Loss 0.073909 (0.084463)\n",
            "[2808/4000] Loss 0.102473 (0.084489)\n",
            "[2812/4000] Loss 0.075019 (0.084476)\n",
            "[2816/4000] Loss 0.014820 (0.084377)\n",
            "[2820/4000] Loss 0.077877 (0.084368)\n",
            "[2824/4000] Loss 0.175461 (0.084496)\n",
            "[2828/4000] Loss 0.081402 (0.084492)\n",
            "[2832/4000] Loss 0.075154 (0.084479)\n",
            "[2836/4000] Loss 0.058228 (0.084442)\n",
            "[2840/4000] Loss 0.124120 (0.084498)\n",
            "[2844/4000] Loss 0.082248 (0.084494)\n",
            "[2848/4000] Loss 0.036927 (0.084428)\n",
            "[2852/4000] Loss 0.068591 (0.084406)\n",
            "[2856/4000] Loss 0.126748 (0.084465)\n",
            "[2860/4000] Loss 0.118231 (0.084512)\n",
            "[2864/4000] Loss 0.068073 (0.084489)\n",
            "[2868/4000] Loss 0.101857 (0.084513)\n",
            "[2872/4000] Loss 0.080266 (0.084507)\n",
            "[2876/4000] Loss 0.071348 (0.084489)\n",
            "[2880/4000] Loss 0.099866 (0.084510)\n",
            "[2884/4000] Loss 0.044745 (0.084455)\n",
            "[2888/4000] Loss 0.120820 (0.084506)\n",
            "[2892/4000] Loss 0.106749 (0.084536)\n",
            "[2896/4000] Loss 0.109741 (0.084571)\n",
            "[2900/4000] Loss 0.061048 (0.084539)\n",
            "[2904/4000] Loss 0.037653 (0.084474)\n",
            "[2908/4000] Loss 0.091827 (0.084484)\n",
            "[2912/4000] Loss 0.047788 (0.084434)\n",
            "[2916/4000] Loss 0.045701 (0.084381)\n",
            "[2920/4000] Loss 0.062832 (0.084351)\n",
            "[2924/4000] Loss 0.088187 (0.084357)\n",
            "[2928/4000] Loss 0.077404 (0.084347)\n",
            "[2932/4000] Loss 0.087136 (0.084351)\n",
            "[2936/4000] Loss 0.094261 (0.084364)\n",
            "[2940/4000] Loss 0.032296 (0.084294)\n",
            "[2944/4000] Loss 0.086632 (0.084297)\n",
            "[2948/4000] Loss 0.068989 (0.084276)\n",
            "[2952/4000] Loss 0.046031 (0.084224)\n",
            "[2956/4000] Loss 0.119137 (0.084272)\n",
            "[2960/4000] Loss 0.173951 (0.084393)\n",
            "[2964/4000] Loss 0.122544 (0.084444)\n",
            "[2968/4000] Loss 0.103426 (0.084470)\n",
            "[2972/4000] Loss 0.052135 (0.084426)\n",
            "[2976/4000] Loss 0.076985 (0.084416)\n",
            "[2980/4000] Loss 0.025282 (0.084337)\n",
            "[2984/4000] Loss 0.117613 (0.084381)\n",
            "[2988/4000] Loss 0.052916 (0.084339)\n",
            "[2992/4000] Loss 0.124067 (0.084392)\n",
            "[2996/4000] Loss 0.117555 (0.084437)\n",
            "[3000/4000] Loss 0.006612 (0.084333)\n",
            "[3004/4000] Loss 0.092052 (0.084343)\n",
            "[3008/4000] Loss 0.090912 (0.084352)\n",
            "[3012/4000] Loss 0.041689 (0.084295)\n",
            "[3016/4000] Loss 0.026690 (0.084219)\n",
            "[3020/4000] Loss 0.144047 (0.084298)\n",
            "[3024/4000] Loss 0.098678 (0.084317)\n",
            "[3028/4000] Loss 0.137848 (0.084388)\n",
            "[3032/4000] Loss 0.098990 (0.084407)\n",
            "[3036/4000] Loss 0.021613 (0.084324)\n",
            "[3040/4000] Loss 0.011782 (0.084229)\n",
            "[3044/4000] Loss 0.205079 (0.084388)\n",
            "[3048/4000] Loss 0.031632 (0.084319)\n",
            "[3052/4000] Loss 0.103846 (0.084344)\n",
            "[3056/4000] Loss 0.098427 (0.084363)\n",
            "[3060/4000] Loss 0.130598 (0.084423)\n",
            "[3064/4000] Loss 0.105077 (0.084450)\n",
            "[3068/4000] Loss 0.100661 (0.084471)\n",
            "[3072/4000] Loss 0.128787 (0.084529)\n",
            "[3076/4000] Loss 0.071654 (0.084512)\n",
            "[3080/4000] Loss 0.139671 (0.084583)\n",
            "[3084/4000] Loss 0.071723 (0.084567)\n",
            "[3088/4000] Loss 0.102014 (0.084589)\n",
            "[3092/4000] Loss 0.065129 (0.084564)\n",
            "[3096/4000] Loss 0.059695 (0.084532)\n",
            "[3100/4000] Loss 0.043716 (0.084479)\n",
            "[3104/4000] Loss 0.100604 (0.084500)\n",
            "[3108/4000] Loss 0.144145 (0.084577)\n",
            "[3112/4000] Loss 0.090162 (0.084584)\n",
            "[3116/4000] Loss 0.110629 (0.084617)\n",
            "[3120/4000] Loss 0.121332 (0.084664)\n",
            "[3124/4000] Loss 0.174270 (0.084779)\n",
            "[3128/4000] Loss 0.128949 (0.084835)\n",
            "[3132/4000] Loss 0.139997 (0.084906)\n",
            "[3136/4000] Loss 0.109658 (0.084937)\n",
            "[3140/4000] Loss 0.062791 (0.084909)\n",
            "[3144/4000] Loss 0.059798 (0.084877)\n",
            "[3148/4000] Loss 0.060662 (0.084847)\n",
            "[3152/4000] Loss 0.060306 (0.084815)\n",
            "[3156/4000] Loss 0.072384 (0.084800)\n",
            "[3160/4000] Loss 0.118629 (0.084842)\n",
            "[3164/4000] Loss 0.119808 (0.084887)\n",
            "[3168/4000] Loss 0.123514 (0.084935)\n",
            "[3172/4000] Loss 0.105179 (0.084961)\n",
            "[3176/4000] Loss 0.082245 (0.084957)\n",
            "[3180/4000] Loss 0.065926 (0.084933)\n",
            "[3184/4000] Loss 0.131657 (0.084992)\n",
            "[3188/4000] Loss 0.152510 (0.085077)\n",
            "[3192/4000] Loss 0.135138 (0.085139)\n",
            "[3196/4000] Loss 0.056063 (0.085103)\n",
            "[3200/4000] Loss 0.127964 (0.085157)\n",
            "[3204/4000] Loss 0.071272 (0.085139)\n",
            "[3208/4000] Loss 0.086772 (0.085141)\n",
            "[3212/4000] Loss 0.045249 (0.085092)\n",
            "[3216/4000] Loss 0.028976 (0.085022)\n",
            "[3220/4000] Loss 0.081891 (0.085018)\n",
            "[3224/4000] Loss 0.075238 (0.085006)\n",
            "[3228/4000] Loss 0.094370 (0.085018)\n",
            "[3232/4000] Loss 0.017066 (0.084934)\n",
            "[3236/4000] Loss 0.066668 (0.084911)\n",
            "[3240/4000] Loss 0.072309 (0.084895)\n",
            "[3244/4000] Loss 0.074351 (0.084882)\n",
            "[3248/4000] Loss 0.166383 (0.084983)\n",
            "[3252/4000] Loss 0.084509 (0.084982)\n",
            "[3256/4000] Loss 0.053742 (0.084944)\n",
            "[3260/4000] Loss 0.153478 (0.085028)\n",
            "[3264/4000] Loss 0.090045 (0.085034)\n",
            "[3268/4000] Loss 0.103479 (0.085056)\n",
            "[3272/4000] Loss 0.131188 (0.085113)\n",
            "[3276/4000] Loss 0.104976 (0.085137)\n",
            "[3280/4000] Loss 0.081084 (0.085132)\n",
            "[3284/4000] Loss 0.029223 (0.085064)\n",
            "[3288/4000] Loss 0.081534 (0.085060)\n",
            "[3292/4000] Loss 0.086972 (0.085062)\n",
            "[3296/4000] Loss 0.086569 (0.085064)\n",
            "[3300/4000] Loss 0.057943 (0.085031)\n",
            "[3304/4000] Loss 0.129944 (0.085085)\n",
            "[3308/4000] Loss 0.118028 (0.085125)\n",
            "[3312/4000] Loss 0.077611 (0.085116)\n",
            "[3316/4000] Loss 0.129111 (0.085169)\n",
            "[3320/4000] Loss 0.105854 (0.085194)\n",
            "[3324/4000] Loss 0.107748 (0.085221)\n",
            "[3328/4000] Loss 0.110727 (0.085252)\n",
            "[3332/4000] Loss 0.030674 (0.085186)\n",
            "[3336/4000] Loss 0.058643 (0.085154)\n",
            "[3340/4000] Loss 0.120610 (0.085197)\n",
            "[3344/4000] Loss 0.047226 (0.085152)\n",
            "[3348/4000] Loss 0.051573 (0.085111)\n",
            "[3352/4000] Loss 0.038087 (0.085055)\n",
            "[3356/4000] Loss 0.050980 (0.085015)\n",
            "[3360/4000] Loss 0.093946 (0.085025)\n",
            "[3364/4000] Loss 0.085457 (0.085026)\n",
            "[3368/4000] Loss 0.052357 (0.084987)\n",
            "[3372/4000] Loss 0.113329 (0.085021)\n",
            "[3376/4000] Loss 0.052057 (0.084982)\n",
            "[3380/4000] Loss 0.102082 (0.085002)\n",
            "[3384/4000] Loss 0.094025 (0.085013)\n",
            "[3388/4000] Loss 0.100468 (0.085031)\n",
            "[3392/4000] Loss 0.128761 (0.085082)\n",
            "[3396/4000] Loss 0.059205 (0.085052)\n",
            "[3400/4000] Loss 0.086274 (0.085053)\n",
            "[3404/4000] Loss 0.050556 (0.085013)\n",
            "[3408/4000] Loss 0.040563 (0.084961)\n",
            "[3412/4000] Loss 0.038762 (0.084907)\n",
            "[3416/4000] Loss 0.062954 (0.084881)\n",
            "[3420/4000] Loss 0.121803 (0.084924)\n",
            "[3424/4000] Loss 0.063928 (0.084900)\n",
            "[3428/4000] Loss 0.031904 (0.084838)\n",
            "[3432/4000] Loss 0.029215 (0.084773)\n",
            "[3436/4000] Loss 0.076425 (0.084763)\n",
            "[3440/4000] Loss 0.042926 (0.084715)\n",
            "[3444/4000] Loss 0.052624 (0.084678)\n",
            "[3448/4000] Loss 0.054249 (0.084642)\n",
            "[3452/4000] Loss 0.063380 (0.084618)\n",
            "[3456/4000] Loss 0.101173 (0.084637)\n",
            "[3460/4000] Loss 0.034048 (0.084578)\n",
            "[3464/4000] Loss 0.063514 (0.084554)\n",
            "[3468/4000] Loss 0.093685 (0.084565)\n",
            "[3472/4000] Loss 0.118633 (0.084604)\n",
            "[3476/4000] Loss 0.060412 (0.084576)\n",
            "[3480/4000] Loss 0.028685 (0.084512)\n",
            "[3484/4000] Loss 0.037960 (0.084459)\n",
            "[3488/4000] Loss 0.109071 (0.084487)\n",
            "[3492/4000] Loss 0.032421 (0.084427)\n",
            "[3496/4000] Loss 0.105140 (0.084451)\n",
            "[3500/4000] Loss 0.110367 (0.084480)\n",
            "[3504/4000] Loss 0.029388 (0.084418)\n",
            "[3508/4000] Loss 0.023535 (0.084348)\n",
            "[3512/4000] Loss 0.083886 (0.084348)\n",
            "[3516/4000] Loss 0.078244 (0.084341)\n",
            "[3520/4000] Loss 0.019568 (0.084267)\n",
            "[3524/4000] Loss 0.111189 (0.084298)\n",
            "[3528/4000] Loss 0.092335 (0.084307)\n",
            "[3532/4000] Loss 0.113863 (0.084340)\n",
            "[3536/4000] Loss 0.090507 (0.084347)\n",
            "[3540/4000] Loss 0.122904 (0.084391)\n",
            "[3544/4000] Loss 0.114970 (0.084425)\n",
            "[3548/4000] Loss 0.103397 (0.084447)\n",
            "[3552/4000] Loss 0.170545 (0.084543)\n",
            "[3556/4000] Loss 0.063559 (0.084520)\n",
            "[3560/4000] Loss 0.050319 (0.084482)\n",
            "[3564/4000] Loss 0.114277 (0.084515)\n",
            "[3568/4000] Loss 0.114468 (0.084548)\n",
            "[3572/4000] Loss 0.049125 (0.084509)\n",
            "[3576/4000] Loss 0.105742 (0.084533)\n",
            "[3580/4000] Loss 0.056638 (0.084501)\n",
            "[3584/4000] Loss 0.165822 (0.084592)\n",
            "[3588/4000] Loss 0.069917 (0.084576)\n",
            "[3592/4000] Loss 0.069580 (0.084559)\n",
            "[3596/4000] Loss 0.097712 (0.084574)\n",
            "[3600/4000] Loss 0.079386 (0.084568)\n",
            "[3604/4000] Loss 0.111923 (0.084598)\n",
            "[3608/4000] Loss 0.053414 (0.084564)\n",
            "[3612/4000] Loss 0.071782 (0.084550)\n",
            "[3616/4000] Loss 0.094941 (0.084561)\n",
            "[3620/4000] Loss 0.072204 (0.084547)\n",
            "[3624/4000] Loss 0.091598 (0.084555)\n",
            "[3628/4000] Loss 0.143073 (0.084620)\n",
            "[3632/4000] Loss 0.071463 (0.084605)\n",
            "[3636/4000] Loss 0.068359 (0.084587)\n",
            "[3640/4000] Loss 0.106182 (0.084611)\n",
            "[3644/4000] Loss 0.087515 (0.084614)\n",
            "[3648/4000] Loss 0.088514 (0.084618)\n",
            "[3652/4000] Loss 0.097584 (0.084633)\n",
            "[3656/4000] Loss 0.012835 (0.084554)\n",
            "[3660/4000] Loss 0.101543 (0.084573)\n",
            "[3664/4000] Loss 0.117333 (0.084608)\n",
            "[3668/4000] Loss 0.080437 (0.084604)\n",
            "[3672/4000] Loss 0.111308 (0.084633)\n",
            "[3676/4000] Loss 0.061944 (0.084608)\n",
            "[3680/4000] Loss 0.067206 (0.084589)\n",
            "[3684/4000] Loss 0.068722 (0.084572)\n",
            "[3688/4000] Loss 0.084217 (0.084572)\n",
            "[3692/4000] Loss 0.049594 (0.084534)\n",
            "[3696/4000] Loss 0.089113 (0.084539)\n",
            "[3700/4000] Loss 0.112337 (0.084569)\n",
            "[3704/4000] Loss 0.049318 (0.084531)\n",
            "[3708/4000] Loss 0.070572 (0.084516)\n",
            "[3712/4000] Loss 0.085023 (0.084516)\n",
            "[3716/4000] Loss 0.101540 (0.084535)\n",
            "[3720/4000] Loss 0.080953 (0.084531)\n",
            "[3724/4000] Loss 0.050802 (0.084495)\n",
            "[3728/4000] Loss 0.135638 (0.084550)\n",
            "[3732/4000] Loss 0.112925 (0.084580)\n",
            "[3736/4000] Loss 0.048025 (0.084541)\n",
            "[3740/4000] Loss 0.059636 (0.084514)\n",
            "[3744/4000] Loss 0.052988 (0.084481)\n",
            "[3748/4000] Loss 0.128392 (0.084527)\n",
            "[3752/4000] Loss 0.168783 (0.084617)\n",
            "[3756/4000] Loss 0.088694 (0.084621)\n",
            "[3760/4000] Loss 0.111189 (0.084650)\n",
            "[3764/4000] Loss 0.030488 (0.084592)\n",
            "[3768/4000] Loss 0.019168 (0.084523)\n",
            "[3772/4000] Loss 0.043502 (0.084479)\n",
            "[3776/4000] Loss 0.044733 (0.084437)\n",
            "[3780/4000] Loss 0.033720 (0.084384)\n",
            "[3784/4000] Loss 0.137039 (0.084439)\n",
            "[3788/4000] Loss 0.101928 (0.084458)\n",
            "[3792/4000] Loss 0.072215 (0.084445)\n",
            "[3796/4000] Loss 0.057782 (0.084417)\n",
            "[3800/4000] Loss 0.125022 (0.084459)\n",
            "[3804/4000] Loss 0.120591 (0.084497)\n",
            "[3808/4000] Loss 0.107559 (0.084522)\n",
            "[3812/4000] Loss 0.144984 (0.084585)\n",
            "[3816/4000] Loss 0.022550 (0.084520)\n",
            "[3820/4000] Loss 0.058754 (0.084493)\n",
            "[3824/4000] Loss 0.137187 (0.084548)\n",
            "[3828/4000] Loss 0.087609 (0.084551)\n",
            "[3832/4000] Loss 0.191233 (0.084663)\n",
            "[3836/4000] Loss 0.059643 (0.084637)\n",
            "[3840/4000] Loss 0.063437 (0.084614)\n",
            "[3844/4000] Loss 0.085070 (0.084615)\n",
            "[3848/4000] Loss 0.159922 (0.084693)\n",
            "[3852/4000] Loss 0.135117 (0.084745)\n",
            "[3856/4000] Loss 0.139316 (0.084802)\n",
            "[3860/4000] Loss 0.095241 (0.084813)\n",
            "[3864/4000] Loss 0.042701 (0.084769)\n",
            "[3868/4000] Loss 0.097278 (0.084782)\n",
            "[3872/4000] Loss 0.017182 (0.084712)\n",
            "[3876/4000] Loss 0.083559 (0.084711)\n",
            "[3880/4000] Loss 0.072922 (0.084699)\n",
            "[3884/4000] Loss 0.112275 (0.084727)\n",
            "[3888/4000] Loss 0.129948 (0.084774)\n",
            "[3892/4000] Loss 0.053746 (0.084742)\n",
            "[3896/4000] Loss 0.084571 (0.084742)\n",
            "[3900/4000] Loss 0.061548 (0.084718)\n",
            "[3904/4000] Loss 0.078575 (0.084712)\n",
            "[3908/4000] Loss 0.108654 (0.084736)\n",
            "[3912/4000] Loss 0.065312 (0.084716)\n",
            "[3916/4000] Loss 0.119134 (0.084752)\n",
            "[3920/4000] Loss 0.094972 (0.084762)\n",
            "[3924/4000] Loss 0.065001 (0.084742)\n",
            "[3928/4000] Loss 0.086062 (0.084743)\n",
            "[3932/4000] Loss 0.050822 (0.084709)\n",
            "[3936/4000] Loss 0.066265 (0.084690)\n",
            "[3940/4000] Loss 0.029330 (0.084634)\n",
            "[3944/4000] Loss 0.051268 (0.084600)\n",
            "[3948/4000] Loss 0.041769 (0.084557)\n",
            "[3952/4000] Loss 0.023671 (0.084495)\n",
            "[3956/4000] Loss 0.075518 (0.084486)\n",
            "[3960/4000] Loss 0.139722 (0.084542)\n",
            "[3964/4000] Loss 0.058125 (0.084515)\n",
            "[3968/4000] Loss 0.048072 (0.084478)\n",
            "[3972/4000] Loss 0.110789 (0.084505)\n",
            "[3976/4000] Loss 0.018322 (0.084438)\n",
            "[3980/4000] Loss 0.053002 (0.084407)\n",
            "[3984/4000] Loss 0.080045 (0.084403)\n",
            "[3988/4000] Loss 0.096892 (0.084415)\n",
            "[3992/4000] Loss 0.183240 (0.084514)\n",
            "[3996/4000] Loss 0.069780 (0.084499)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6Im9bJhVvtV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "2dbbfa38-45ab-4d69-c042-f3238b853d1e"
      },
      "source": [
        "# チェックポイントの確認\n",
        "!ls -ltr checkpoints/biggan__biggan256/memnet/OneDirection_None/\n",
        "!ls -ltr checkpoints/biggan__biggan256/memnet/OneDirection_None/*\n",
        "#!rm -rf checkpoints/biggan__biggan256"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4\n",
            "drwxr-xr-x 2 root root 4096 Jul  1 03:12 fd76206\n",
            "total 3116492\n",
            "-rw-r--r-- 1 root root        250 Jul  1 02:45 opts.json\n",
            "-rw-r--r-- 1 root root       8850 Jul  1 02:46 example0.jpg\n",
            "-rw-r--r-- 1 root root 1112481372 Jul  1 02:46 model.ckpt-0.data-00000-of-00001\n",
            "-rw-r--r-- 1 root root      14808 Jul  1 02:46 model.ckpt-0.index\n",
            "-rw-r--r-- 1 root root  483113596 Jul  1 02:46 model.ckpt-0.meta\n",
            "-rw-r--r-- 1 root root      28954 Jul  1 03:12 losses.txt\n",
            "-rw-r--r-- 1 root root 1112481372 Jul  1 03:12 model.ckpt-4000.data-00000-of-00001\n",
            "-rw-r--r-- 1 root root      14808 Jul  1 03:12 model.ckpt-4000.index\n",
            "-rw-r--r-- 1 root root        130 Jul  1 03:12 checkpoint\n",
            "-rw-r--r-- 1 root root  483113596 Jul  1 03:12 model.ckpt-4000.meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MslTHOya9Mc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python test_tf.py --alpha 0.1 --test_truncation 1 --checkpoint_dir checkpoints/biggan__biggan256/memnet/OneDirection_None/fd76206 --checkpoint 4000 --gpu_id 0\n",
        "# GANalyze/tensorflow/outputに画像ファイルが出力されていきます。"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHDPmhVfjcYu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}